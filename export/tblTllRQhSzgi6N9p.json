[
    {
        "# Top Paper votes": 0,
        "Category": [
            "Scalable oversight"
        ],
        "Internal source": [
            "Twitter"
        ],
        "Link": "https://www.lesswrong.com/posts/kguLeJTt6LnGuYX4E/the-limits-of-ai-safety-via-debate",
        "Reviewed by": [
            "AG",
            "VK"
        ],
        "Safety Category": [
            "Scalable oversight"
        ],
        "Safety Topic": [
            "Debate"
        ],
        "Title": "The limits of AI safety via debate (Hobbhahn, 2022)",
        "Type": [
            "Blog post"
        ],
        "airtable_createdTime": "2024-02-10T00:27:32.000Z",
        "airtable_id": "rec00H41EvEzuJWDx"
    },
    {
        "# Top Paper votes": 3,
        "AD: Top Paper": true,
        "AG comments": "Well written and important problem for people to be aware of, but may be a bit abstract for most of the audience\n",
        "AG: Top Paper": true,
        "Abstract": "The field of AI alignment is concerned with AI systems that pursue unintended goals. One commonly studied mechanism by which an unintended goal might arise is specification gaming, in which the designer-provided specification is flawed in a way that the designers did not foresee. However, an AI system may pursue an undesired goal even when the specification is correct, in the case of goal misgeneralization. Goal misgeneralization is a specific form of robustness failure for learning algorithms in which the learned program competently pursues an undesired goal that leads to good performance in training situations but bad performance in novel test situations. We demonstrate that goal misgeneralization can occur in practical systems by providing several examples in deep learning systems across a variety of domains. Extrapolating forward to more capable systems, we provide hypotheticals that illustrate how goal misgeneralization could lead to catastrophic risk. We suggest several research directions that could reduce the risk of goal misgeneralization for future systems. \n",
        "Blog or Video": "https://deepmindsafetyresearch.medium.com/goal-misgeneralisation-why-correct-specifications-arent-enough-for-correct-goals-cf96ebc60924",
        "Category": [
            "Reward misspecification and goal misgeneralization",
            "Reinforcement Learning"
        ],
        "Citations": 48,
        "Description from MAIA / AISF / Elsewhere": "Shah et al. argue that even an agent trained on the \u201cright\u201d reward function might learn goals which generalize in undesirable ways, and provide both concrete and hypothetical illustrations of the phenomenon.",
        "Goes online in Expanded Papers": true,
        "Internal source": [
            "Elsewhere"
        ],
        "Link": "https://arxiv.org/abs/2210.01790",
        "ML Subfield": [
            "Reinforcement Learning",
            "Theory"
        ],
        "ML Subfield (Is Starting Point)": [
            "Reinforcement Learning"
        ],
        "ML Subtopic": [
            "RL: Games",
            "Applied ML: Cognitive Tasks",
            "NLP: LLMs"
        ],
        "Reviewed by": [
            "AG",
            "VK",
            "PC",
            "RS",
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Reward misspecification and goal misgeneralization",
            "Alignment <-> RLHF"
        ],
        "Supplemental Material": "https://sites.google.com/view/goal-misgeneralization",
        "Time from MAIA / AISF / Elsewhere (min)": "30",
        "Title": "Goal Misgeneralization: Why Correct Specifications Aren\u2019t Enough For Correct Goals (Shah et al., 2022)",
        "Twitter": "https://twitter.com/rohinmshah/status/1578391329461133315",
        "Type": [
            "Paper"
        ],
        "VK: Top Paper": true,
        "What sections to read from MAIA": "(only sections 1-4)",
        "Who tagged for ML": [
            "Max"
        ],
        "Year": "2022",
        "airtable_createdTime": "2023-09-05T22:50:05.000Z",
        "airtable_id": "rec0LnJiCsCpSAmSl"
    },
    {
        "# Top Paper votes": 0,
        "Category": [
            "Overviews and agendas"
        ],
        "Internal source": [
            "Alignment Workshop 2023"
        ],
        "Link": "https://www.alignment-workshop.com/sf-talks/ilya-sutskever-opening-remarks-confronting-the-possibility-of-agi",
        "ML Subfield": [
            "Domain General"
        ],
        "Reviewed by": [
            "AG",
            "VK",
            "PC",
            "RS",
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Major problems (misalignment, misuse, threat models)"
        ],
        "Title": "Opening Remarks: Confronting the Possibility of AGI (Ilya Sutskever, 19-minute video)",
        "Type": [
            "Video"
        ],
        "Vael's Notes": "Talks about how AI winter made people pessimistic, but now we should be thinking about AGI happening because that era is no longer true\n",
        "Who tagged for ML": [
            "Viktoria"
        ],
        "airtable_createdTime": "2023-09-06T18:51:17.000Z",
        "airtable_id": "rec0TXV1hAdEuPmwl"
    },
    {
        "# Top Paper votes": 0,
        "Abstract": "We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have near-random accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model's academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings.\n",
        "Category": [
            "Model evaluations and benchmarks"
        ],
        "Internal source": [
            "Elsewhere"
        ],
        "Link": "https://arxiv.org/abs/2009.03300",
        "Safety Category": [
            "Model evaluations / monitoring / detection"
        ],
        "Title": "Measuring Massive Multitask Language Understanding (Hendrycks et al., 2020)",
        "Type": [
            "Paper"
        ],
        "airtable_createdTime": "2024-03-31T18:06:47.000Z",
        "airtable_id": "rec0cHFqFYoOfGu3g"
    },
    {
        "# Top Paper votes": 0,
        "Category": [
            "Overviews and agendas"
        ],
        "Internal source": [
            "Elsewhere"
        ],
        "Link": "https://bounded-regret.ghost.io/gpt-2030-and-catastrophic-drives-four-vignettes/",
        "ML Subfield": [
            "Human Model Interaction"
        ],
        "ML Subtopic": [
            "NLP: LLMs"
        ],
        "Reviewed by": [
            "PC",
            "RS",
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Major problems (misalignment, misuse, threat models)"
        ],
        "Title": "GPT-2030 and Catastrophic Drives: Four Vignettes (Steinhart, 2023)",
        "Type": [
            "Blog post"
        ],
        "Who tagged for ML": [
            "Max"
        ],
        "Year": "2023",
        "airtable_createdTime": "2023-12-12T20:00:03.000Z",
        "airtable_id": "rec0lnt7d1DPYJI30"
    },
    {
        "# Top Paper votes": 0,
        "Abstract": "For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals defined in terms of (non-expert) human preferences between pairs of trajectory segments. We show that this approach can effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on less than one percent of our agent's interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art RL systems. To demonstrate the flexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any that have been previously learned from human feedback.\n",
        "Category": [
            "Scalable oversight"
        ],
        "Internal source": [
            "Elsewhere"
        ],
        "Link": "https://arxiv.org/abs/1706.03741",
        "Safety Category": [
            "Alignment <-> RLHF"
        ],
        "Title": "Deep reinforcement learning from human preferences (Christiano et al., 2017)",
        "Type": [
            "Paper"
        ],
        "airtable_createdTime": "2024-03-31T18:06:47.000Z",
        "airtable_id": "rec0rJxOnZ6kyyQpd"
    },
    {
        "# Top Paper votes": 0,
        "Category": [
            "AI Governance"
        ],
        "Internal source": [
            "NOLA Workshop 2023"
        ],
        "Link": "https://www.alignment-workshop.com/nola-talks/irina-rish-complex-systems-view-of-large-scale-ai-systems",
        "Safety Category": [
            "AI Governance"
        ],
        "Title": "Complex systems view of large-scale AI systems (Irina Rish, 5-min video)",
        "Type": [
            "Video"
        ],
        "Vael's Notes": "Not including the lightning talks for now, since pretty EA probably\n",
        "airtable_createdTime": "2024-02-10T00:27:32.000Z",
        "airtable_id": "rec12d443wAq9QJ1H"
    },
    {
        "# Top Paper votes": 0,
        "Supplemental Material": "Some context: https://twitter.com/andy_l_jones/status/1666884026895073280",
        "Title": "Attached to the ITT (Inference-Time Intervention) post ",
        "airtable_createdTime": "2024-02-10T00:27:32.000Z",
        "airtable_id": "rec15nThd7SBqSscm"
    },
    {
        "# Top Paper votes": 1,
        "AD: Top Paper": true,
        "Category": [
            "Overviews and agendas"
        ],
        "Goes online in Expanded Papers": true,
        "Internal source": [
            "Alignment Workshop 2023"
        ],
        "Link": "https://www.alignment-workshop.com/sf-talks/dan-hendrycks-surveying-safety-research-directions",
        "ML Subfield": [
            "Domain General"
        ],
        "ML Subfield (Is Starting Point)": [
            "Domain General"
        ],
        "Reviewed by": [
            "AG",
            "VK",
            "PC",
            "RS",
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Major problems (misalignment, misuse, threat models)",
            "Adversaries / Robustness / Generalization",
            "Transparency / Interpretability / Model internals / Latent knowledge",
            "Deception",
            "AI Governance",
            "Overview"
        ],
        "Title": "Surveying Safety Research Directions (Dan Hendrycks, 40-min video)",
        "Type": [
            "Video"
        ],
        "Vael's Notes": "Dan Hendrycks - Surveying Safety Research Directions\n- didn\u2019t actually list xrisk via misalignment as the main safety worry, unlike rest of people. motivating examples were job loss, power inequality, persuasion, automated weapons, misuse, job loss\n- swiss cheese model: pursuing multiple safety research avenues creates multiple layers of protection which mitigates hazards and makes ML systems safer \u2014 systemic safety + monitoring + robustness + alignment (Vael: whereas most talks kind of smash alignment + robustness together, and talk about monitoring separately maybe?) \u201ccomplicated socio-technical problem\u201d\n- \u201cResearch Areas in Safety: Robustness (withstand hazards), Monitoring (identify hazards), Alignment (reduce inherent model hazards)\u201d Vael: robustness is like not changing much with distributional shift though (adversarial attacks aren\u2019t like all just attacks) and my impression is that this is less distinct\n- Overall goal of the talk is to flag problems that can be worked on today, and is an invitation for people to do further work \u2190-\\*\\*\n- Robustness (\u201dbuzzwords within that you might search\u201d):\n    - Proxy gaming / reward hacking (Vael: no one calls this outer misalignment but imo it\u2019s also that). Dan says this is related to robustness in that if we can make the proxy reward more robust this is safer. (Vael: \u2026uh) \u2026 Dan talks about a proxy gaming benchmark?\n    - Adversarial attacks on language models\n    - Unforeseen adversaries and less restricted models\n- Monitoring\n    - is grouping mechanistic interpretability and transparency together as terms\n    - transparency could make it easier to detect deception or other hazards (that\u2019s the direction Cas was pushing. Also Chris, Chris is also like: we want to be able to guarantee what features we have and a system\u2019s safety which is more comprehensive). Dan: maybe some problems with tractability but worth having lots of people trying to make progress on that\n    - Anomaly detection as a means for detecting proxy gaming\n- Trojans / backdoors. \u201cAIs with situational awareness could hide their true intentions while being monitored and execute a \u2018treacherous turn.\u2019 Trojans provide a microcosm for studying this in current systems.\u201d Sweet spot for xrisk research and not as severe but still important problems today. (\u2190 Vael: I like this, and also seems like robustness. probably seems like a different thing than scalable oversight. seems possibly quite related to interpretability, seems related to evals)\n    - Trojan detection research\n- Alignment\n    - AI alignment = How to get systems to try to satisfy our preferences, i.e. intent alignment\n    - AI alignment = A rebranding of \u201cAI safety,\u201d where AI safety is about reducing catastrophic and existential risks from AI\n- Approaches\n- Honesty and Eliciting Latent Knowledge (citing Collin\u2019s work). \u201cInternal beliefs about truth is one concept to extract \u2014 extracting other internal concepts such as \u201cutility\u201d and \u201cpower\u201d also is feasible.\u201d\n    - Truthful = \u201cmodel avoids asserting false statements\u201d, not the same as honesty (can be wrong with honesty, also is just saying stuff)\n    - Aside on safety-capability tradeoff: \u201cSo I think in people trying to come up with safety techniques, it's useful to shoot for some type of goal that can be somewhat decoupled from some general capabilities. And by general capabilities, I mean like general classification, data compression, instruction following, **helpfulness**, state estimation, efficiency, scalability, reasoning, optimization, self-supervised learning, that sort of stuff, those instrumentally useful capabilities for pretty much everything that have very strong downstream effects on lots of tasks. I think we shouldn't be in the business of doing that. But instead, I think safety is best if it's what's the sort of thing that you can do outside of scaling? What are desirable properties that we want the model to have outside of scaling? That way we're actually making some difference instead of kind of sloshing about, just making a lot of noise while we wait for the models to get larger, and then it solves the problem, and then we'll just make some more noise. But instead, let's differentially move on safety.\u201d An example of safety research he includes is adversarial robustness (isn\u2019t aimed at improving accuracy, has vanilla accuracy)\n- Benchmarks, e.g. Machiavelli\n    - \u201cAnd I should say, as it happens, if you're wanting to make the models behave more ethically, it often is going to trade off with rewards. So I'm not sure there's, you know, the models won't necessarily sort of get 100 percent in a zero shot way in the long term. There are some intrinsic tradeoffs between behaving well and behaving in ways that are optimal as judged by the environment, so we'll need to try to improve those tradeoffs and collectively come at a sense of, how much are we willing to trade off an agent pursuing whatever goal it's given? And it's how generally how cautious or morally it's acting. So I don't know if we'll be in this sort of realm of 100 percent in every sort of safety benchmark. It will end up being more like a curve and we're trying to come up with Pareto improvements along that curve.\u201d\n    - Measuring model behavior, like deception, self-preservation, power-seeking, and resource acquisition (Vael: the last three are now implicit in everyone\u2019s talks? We don\u2019t really talk about this anymore, huh.) \u201cAI systems which naively model real-world data learn harmful values: Toxicity emerges through next-token prediction, Machiavellianism emerges through reward maximization (e.g. CICERO)\u201d\n    - Summary: honesty, machine ethics, power aversion\n- Nice thought here about prevention vs protection. \u201cSo, one other distinction from safety engineering would be the distinction between protective mechanisms and preventative mechanisms. And so, protective mechanisms might be like, let's prevent it from happening in the first place. And then, so like we'll add some barriers inside of the Titanic to make sure if there's an iceberg, it'll get contained, the water will be contained in some specific reason. That's more preventative. And then protective might be like the lifeboats of like, if everything's gone badly, how can we at least minimize that? So, one's reduce the probability of the bad thing happening in the first place, and the other one is, let's reduce the impact if it does end up happening anyway. And I think that alignment being about, alignment being about, is about just reducing the hazard in the first place, which is more preventative. And a lot of the protective measures are often weaker. So, you know, an ounce of prevention is a pound of cure, I think it is sort of exemplified in this sort of asymmetry there. I still think you want all of them.\u201d\n    - \u201cI also don't think that the risks from the AI agents themselves are the only part of the, only part of the overall risk analysis. Because the alignment part is specifically about what's the hazards in the agent and what's it, you know, aiming to do and making, making sure that that's, it's trying to do what we want. But I think there are other systemic issues like competitive pressures, which I think actually, and personally, I think dominate the analysis for how safe things will actually go. I think competitive pressures are, so, that's maybe why I put systemic pressure or systemic safety first. And I think we need multilateral cooperation to make this go well, instead of everybody racing and as they currently are.\u201d\n    - I like this point, about anomaly detection and imagenet accuracy going up together (the usual capabilities vs safety graph Dan uses): \u201cLike for instance, let's be honest that the language models having many notions of an ability to predict, make predictive statements about a lot of common sense morality stuff has been very useful. We're not as worried about them having some, you know, really crazy interpretation of our instruction as we may have had in the past. So there's some ways in which capabilities can, can end up making things safer. And this is sort of exemplified by that trend line that they are going up together.\u201d\n    - Didn\u2019t take questions during, so one can just cut it off when he stops talking, around ~33m.\n\n",
        "Who tagged for ML": [
            "Viktoria"
        ],
        "airtable_createdTime": "2023-09-06T19:18:18.000Z",
        "airtable_id": "rec1BVmOHznRDTh5H"
    },
    {
        "# Top Paper votes": 2,
        "AD: Top Paper": true,
        "Blog or Video": "https://www.anthropic.com/index/anthropics-responsible-scaling-policy",
        "Category": [
            "Model evaluations and benchmarks",
            "AI Governance"
        ],
        "Goes online as Top Paper": true,
        "Goes online in Expanded Papers": true,
        "Internal source": [
            "Elsewhere"
        ],
        "Link": "https://www.anthropic.com/news/anthropics-responsible-scaling-policy",
        "ML Subfield": [
            "Applied ML",
            "Model Evaluation"
        ],
        "ML Subfield (Is Starting Point)": [
            "Applied ML"
        ],
        "ML Subtopic": [
            "Applied ML: Chatbots"
        ],
        "Reviewed by": [
            "VK",
            "PC",
            "RS",
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Model evaluations / monitoring / detection",
            "AI Governance"
        ],
        "Title": "Anthropic's Responsible Scaling Policy (Anthropic, 2023)",
        "Twitter": "https://x.com/AnthropicAI/status/1792598295388279124",
        "Type": [
            "Paper"
        ],
        "VK: Top Paper": true,
        "Who tagged for ML": [
            "Viktoria"
        ],
        "Year": "2023",
        "airtable_createdTime": "2023-12-06T23:14:42.000Z",
        "airtable_id": "rec1LXAyId8Y5agq8"
    },
    {
        "# Top Paper votes": 0,
        "Supplemental Material": "Description of what activation patching is by someone in a LessWrong post: https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=qeWBvs-R-taFfcCq-S_hgMqx",
        "Title": "Attached to the ROME paper post",
        "airtable_createdTime": "2024-02-10T00:27:32.000Z",
        "airtable_id": "rec1WArbRnaxYx4w6"
    },
    {
        "# Top Paper votes": 0,
        "AD comments": "\n",
        "Abstract": "> Large language models (LLMs) are trained on massive internet corpora that often contain copyrighted content. This poses legal and ethical challenges for the developers and users of these models, as well as the original authors and publishers. In this paper, we propose a novel technique for unlearning a subset of the training data from a LLM, without having to retrain it from scratch. \n> We evaluate our technique on the task of unlearning the Harry Potter books from the Llama2-7b model (a generative language model recently open-sourced by Meta). While the model took over 184K GPU-hours to pretrain, we show that in about 1 GPU hour of finetuning, we effectively erase the model's ability to generate or recall Harry Potter-related content, while its performance on common benchmarks (such as Winogrande, Hellaswag, arc, boolq and piqa) remains almost unaffected. We make our fine-tuned model publicly available on HuggingFace for community evaluation. To the best of our knowledge, this is the first paper to present an effective technique for unlearning in generative language models. \n> Our technique consists of three main components: First, we use a reinforced model that is further trained on the target data to identify the tokens that are most related to the unlearning target, by comparing its logits with those of a baseline model. Second, we replace idiosyncratic expressions in the target data with generic counterparts, and leverage the model's own predictions to generate alternative labels for every token. These labels aim to approximate the next-token predictions of a model that has not been trained on the target data. Third, we finetune the model on these alternative labels, which effectively erases the original text from the model's memory whenever it is prompted with its context.\n\n",
        "Category": [
            "Interpretability / explainability",
            "Security"
        ],
        "Citations": 45,
        "Description from MAIA / AISF / Elsewhere": "[Paper] removed information about Harry Potter books from Llama 2. First, they fine-tuned another model on more information about Harry Potter, and used the difference in the logits of the two models to identify token predictions which depend on knowledge of Harry Potter. Then, they replaced these tokens with generic counterparts, and fine-tuned the original model on these generic labels.",
        "Internal source": [
            "ML Newsletter"
        ],
        "Link": "https://arxiv.org/abs/2310.02238",
        "ML Subfield": [
            "NLP",
            "Applied ML",
            "Security"
        ],
        "ML Subtopic": [
            "NLP: LLMs",
            "Applied ML: Chatbots"
        ],
        "Reviewed by": [
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Transparency / Interpretability / Model internals / Latent knowledge"
        ],
        "Title": "Who's Harry Potter? Approximate Unlearning in LLMs (Eldan and Russinovich, 2023)",
        "Type": [
            "Paper"
        ],
        "Who tagged for ML": [
            "Viktoria"
        ],
        "Year": "2023",
        "airtable_createdTime": "2023-12-20T01:45:25.000Z",
        "airtable_id": "rec2Po7Y27s1VmE4x"
    },
    {
        "# Top Paper votes": 2,
        "AD: Top Paper": true,
        "Abstract": "    Current approaches to building general-purpose AI systems tend to produce systems with both beneficial and harmful capabilities. Further progress in AI development could lead to capabilities that pose extreme risks, such as offensive cyber capabilities or strong manipulation skills. We explain why model evaluation is critical for addressing extreme risks. Developers must be able to identify dangerous capabilities (through \"dangerous capability evaluations\") and the propensity of models to apply their capabilities for harm (through \"alignment evaluations\"). These evaluations will become critical for keeping policymakers and other stakeholders informed, and for making responsible decisions about model training, deployment, and security. \n",
        "Blog or Video": "https://www.deepmind.com/blog/an-early-warning-system-for-novel-ai-risks",
        "Category": [
            "Model evaluations and benchmarks",
            "AI Governance"
        ],
        "Citations": 75,
        "Goes online in Expanded Papers": true,
        "High Citation and Max Select": true,
        "Internal source": [
            "Elsewhere"
        ],
        "Link": "https://arxiv.org/abs/2305.15324",
        "ML Subfield": [
            "Model Evaluation",
            "Domain General"
        ],
        "ML Subfield (Is Starting Point)": [
            "Model Evaluation",
            "Domain General"
        ],
        "Reviewed by": [
            "AG",
            "VK",
            "PC",
            "RS",
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Model evaluations / monitoring / detection",
            "AI Governance"
        ],
        "Title": "Model evaluation for extreme risks (Shevlane et al., 2023)",
        "Type": [
            "Paper"
        ],
        "VK: Top Paper": true,
        "Who tagged for ML": [
            "Viktoria"
        ],
        "Year": "2023",
        "airtable_createdTime": "2023-09-05T22:50:05.000Z",
        "airtable_id": "rec2nq5PfP6z3KAbC"
    },
    {
        "# Top Paper votes": 1,
        "AD: Top Paper": true,
        "Category": [
            "Scalable oversight",
            "Model evaluations and benchmarks"
        ],
        "Internal source": [
            "Alignment Workshop 2023"
        ],
        "Link": "https://www.alignment-workshop.com/sf-talks/jan-leike-scaling-reinforcement-learning-from-human-feedback",
        "ML Methods Tag": [
            "RLHF"
        ],
        "ML Subfield": [
            "NLP",
            "Model Evaluation"
        ],
        "Reviewed by": [
            "AG",
            "VK",
            "PC",
            "RS",
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Scalable oversight",
            "Alignment <-> RLHF"
        ],
        "Title": "Scaling Reinforcement Learning from Human Feedback (Jan Leike, 39-min video)",
        "Type": [
            "Video"
        ],
        "Vael's Notes": "Jan Leike - Scaling Reinforcement Learning from Human Feedback\n\\- RLHF solution directions, specifically talking about scalable oversight\n\\- Scalable oversight ideas, untested mostly. Jan wants to build prototypes of them, and see how well they work in practice\n\u2014 AI-assisted evaluation\n\\- Zooming in on AI-assisted evaluation, motivated by trying to figure out whether critique or dialogues are better. Talking about \u201cRCT with perturbed answers\u201d. A procedure for evaluating whether the scalable oversight techniques are working \u2014 can people distinguish between better and worse responses if given help. \n\u2014 Debate\n\u2014 Factored cognition\n\u2014 Recursive reward modeling\n\\- Humans use AI to help evaluate, and then AI can help evaluate at future levels\n\u2014 IDA\n\u2014 \u2026\n\\- wants to use AI to do alignment research, under the hypothesis that it\u2019s easier to evaluate good alignment research rather than generate it. Doesn\u2019t have a solution to the core difficulty of what happens at higher levels and how to ensure those are good. \n\\- chop off the last inaudible part, is okay for interruptions (not great as usual)\n",
        "Who tagged for ML": [
            "Viktoria"
        ],
        "airtable_createdTime": "2023-09-06T19:17:59.000Z",
        "airtable_id": "rec2vffeqMr1krOZP"
    },
    {
        "# Top Paper votes": 0,
        "Category": [
            "Scalable oversight"
        ],
        "Description from MAIA / AISF / Elsewhere": "This reading explains why in the worst case, it\u2019s not possible to judge a debate without adjudicating a prohibitively large number of subclaims.",
        "Internal source": [
            "AWAIR",
            "Further reading on MAIA/HAIST Summer 2023 Curriculum"
        ],
        "Link": "https://www.alignmentforum.org/posts/PJLABqQ962hZEqhdB/debate-update-obfuscated-arguments-problem",
        "ML Subfield": [
            "Model Evaluation"
        ],
        "ML Subtopic": [
            "NLP: Question Answering",
            "NLP: LLMs"
        ],
        "Reviewed by": [
            "AG",
            "VK",
            "PC",
            "RS",
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Scalable oversight",
            "Research bottlenecks / limitations"
        ],
        "Time from MAIA / AISF / Elsewhere (min)": "15",
        "Title": "Debate update: Obfuscated arguments problem (Barnes and Christiano, 2020)",
        "Type": [
            "Blog post"
        ],
        "Vael's Notes": "\n",
        "Who tagged for ML": [
            "Viktoria"
        ],
        "airtable_createdTime": "2023-09-11T21:57:43.000Z",
        "airtable_id": "rec3RJMlP9qyIKWaU"
    },
    {
        "# Top Paper votes": 0,
        "Title": "Can also add \"Tags\" as a column heading, was saving that. You also had \"FATE\" as a Category in light orange but got worried. Category / Subcategory / Topic was how it used to be",
        "airtable_createdTime": "2024-02-10T00:27:32.000Z",
        "airtable_id": "rec3hzKmKDMXPX8oN"
    },
    {
        "# Top Paper votes": 0,
        "Blog or Video": "https://www.alignment-workshop.com/nola-talks/elad-hazan-ai-safety-by-debate-via-regret-minimization",
        "Category": [
            "Scalable oversight"
        ],
        "Internal source": [
            "NOLA Workshop 2023"
        ],
        "Link": "https://arxiv.org/abs/2312.04792",
        "Safety Category": [
            "Scalable oversight"
        ],
        "Safety Topic": [
            "Debate"
        ],
        "Title": "AI safety by debate via regret minimization (Chen et al., 2023)",
        "Type": [
            "Paper",
            "Video"
        ],
        "Vael's Notes": "Not including the lightning talks for now, since pretty EA probably\n",
        "airtable_createdTime": "2024-02-10T00:27:32.000Z",
        "airtable_id": "rec3rLyuVG9i6zR7K"
    },
    {
        "# Top Paper votes": 0,
        "Abstract": "We analyze the type of learned optimization that occurs when a learned model (such as a neural network) is itself an optimizer - a situation we refer to as mesa-optimization, a neologism we introduce in this paper. We believe that the possibility of mesa-optimization raises two important questions for the safety and transparency of advanced machine learning systems. First, under what circumstances will learned models be optimizers, including when they should not be? Second, when a learned model is an optimizer, what will its objective be - how will it differ from the loss function it was trained under - and how can it be aligned? In this paper, we provide an in-depth analysis of these two primary questions and provide an overview of topics for future research. \n",
        "Category": [
            "Deception",
            "Theory"
        ],
        "Citations": 102,
        "Goes online in Expanded Papers": true,
        "High Citation and Max Select": true,
        "Internal source": [
            "Elsewhere"
        ],
        "Link": "https://arxiv.org/abs/1906.01820",
        "ML Subfield": [
            "Optimization",
            "Theory",
            "Domain General"
        ],
        "ML Subfield (Is Starting Point)": [
            "Domain General"
        ],
        "Reviewed by": [
            "AG",
            "VK",
            "PC",
            "RS",
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Deception"
        ],
        "Safety Topic": [
            "Situational awareness (also instrumental convergence, inner misalignment)"
        ],
        "Title": "Risks from Learned Optimization in Advanced Machine Learning Systems (Hubinger et al., 2021)",
        "Type": [
            "Paper"
        ],
        "VK comments": "Good paper, but not the best introduction to inner misalignment for people outside the field (goal misgeneralization papers are better for this)\n",
        "Vael's Notes": "cited here: <https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html>\n\nEvan's talk was controversial at AWAIR (e.g. \"too specific\", \"not convincing\")\n",
        "Who tagged for ML": [
            "Max"
        ],
        "Year": "2021",
        "airtable_createdTime": "2023-09-13T00:44:48.000Z",
        "airtable_id": "rec44Wq7oe2hlG67m"
    },
    {
        "# Top Paper votes": 1,
        "AD: Top Paper": true,
        "Abstract": "Despite much progress in training artificial intelligence (AI) systems to imitate human language, building agents that use language to communicate intentionally with humans in interactive environments remains a major challenge. We introduce Cicero, the first AI agent to achieve human-level performance in Diplomacy, a strategy game involving both cooperation and competition that emphasizes natural language negotiation and tactical coordination between seven players. Cicero integrates a language model with planning and reinforcement learning algorithms by inferring players\u2019 beliefs and intentions from its conversations and generating dialogue in pursuit of its plans. Across 40 games of an anonymous online Diplomacy league, Cicero achieved more than double the average score of the human players and ranked in the top 10% of participants who played more than one game.\n",
        "Category": [
            "Deception",
            "Reinforcement Learning"
        ],
        "Citations": 161,
        "Goes online as Top Paper": true,
        "Goes online in Expanded Papers": true,
        "High Citation and Max Select": true,
        "Internal source": [
            "Hendrycks Textbook"
        ],
        "Link": "https://www.science.org/doi/10.1126/science.ade9097",
        "ML Subfield": [
            "NLP",
            "Applied ML",
            "Reinforcement Learning"
        ],
        "ML Subfield (Is Starting Point)": [
            "NLP",
            "Reinforcement Learning",
            "Applied ML"
        ],
        "ML Subtopic": [
            "RL: Games",
            "Applied ML: Chatbots",
            "Applied ML: Cognitive Tasks",
            "NLP: LLMs"
        ],
        "Reviewed by": [
            "PC",
            "RS",
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Capabilities of LLMs",
            "Deception"
        ],
        "Title": "Human-Level Play in the Game of Diplomacy by Combining Language Models with Strategic Reasoning (Bakhtin et al., 2022)",
        "Type": [
            "Paper"
        ],
        "Who tagged for ML": [
            "Max"
        ],
        "Year": "2022",
        "airtable_createdTime": "2023-12-12T20:00:03.000Z",
        "airtable_id": "rec4Ze8gpGYH3w1rU"
    },
    {
        "# Top Paper votes": 0,
        "AG comments": "Recommended science of deep learning paper.\n\nTBH none of these [science of deep learning papers] are directly safety relevant, it's much more \"foundational knowledge that'll help with safety\". I do feel ~good about safety-motivated people looking into this area, but they'd need to get the safety motivation/context from other areas, so it's not something I'd necessarily highlight. Might be good for people with more theoretical backgrounds though?\n",
        "Abstract": " In this paper we propose to study generalization of neural networks on small algorithmically generated datasets. In this setting, questions about data efficiency, memorization, generalization, and speed of learning can be studied in great detail. In some situations we show that neural networks learn through a process of \"grokking\" a pattern in the data, improving generalization performance from random chance level to perfect generalization, and that this improvement in generalization can happen well past the point of overfitting. We also study generalization as a function of dataset size and find that smaller datasets require increasing amounts of optimization for generalization. We argue that these datasets provide a fertile ground for studying a poorly understood aspect of deep learning: generalization of overparametrized neural networks beyond memorization of the finite training dataset. \n",
        "Category": [
            "Theory"
        ],
        "Citations": 208,
        "Internal source": [
            "Elsewhere"
        ],
        "Link": "https://arxiv.org/pdf/2201.02177.pdf",
        "ML Subfield": [
            "Theory"
        ],
        "ML Subfield (Is Starting Point)": [
            "Theory"
        ],
        "Reviewed by": [
            "AG",
            "VK",
            "PC",
            "RS"
        ],
        "Safety Category": [
            "Capabilities of LLMs",
            "Science of deep learning"
        ],
        "Title": "Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets (Power et al., 2022)",
        "Type": [
            "Paper"
        ],
        "Who tagged for ML": [
            "Viktoria"
        ],
        "airtable_createdTime": "2024-02-10T00:27:32.000Z",
        "airtable_id": "rec4glCuNsxc3ZAbB"
    },
    {
        "# Top Paper votes": 1,
        "AD: Top Paper": true,
        "Abstract": "Language Models (LMs) often cannot be deployed because of their potential to harm users in hard-to-predict ways. Prior work identifies harmful behaviors before deployment by using human annotators to hand-write test cases. However, human annotation is expensive, limiting the number and diversity of test cases. In this work, we automatically find cases where a target LM behaves in a harmful way, by generating test cases (\"red teaming\") using another LM. We evaluate the target LM's replies to generated test questions using a classifier trained to detect offensive content, uncovering tens of thousands of offensive replies in a 280B parameter LM chatbot. We explore several methods, from zero-shot generation to reinforcement learning, for generating test cases with varying levels of diversity and difficulty. Furthermore, we use prompt engineering to control LM-generated test cases to uncover a variety of other harms, automatically finding groups of people that the chatbot discusses in offensive ways, personal and hospital phone numbers generated as the chatbot's own contact info, leakage of private training data in generated text, and harms that occur over the course of a conversation. Overall, LM-based red teaming is one promising tool (among many needed) for finding and fixing diverse, undesirable LM behaviors before impacting users. \n",
        "Category": [
            "Non-Adversarial Robustness"
        ],
        "Citations": 319,
        "Goes online in Expanded Papers": true,
        "High Citation and Max Select": true,
        "Internal source": [
            "AWAIR"
        ],
        "Link": "https://arxiv.org/abs/2202.03286",
        "ML Subfield": [
            "NLP",
            "Model Evaluation",
            "Robustness and Adversariality",
            "Applied ML",
            "Security"
        ],
        "ML Subfield (Is Starting Point)": [
            "Security"
        ],
        "ML Subtopic": [
            "Applied ML: Chatbots",
            "NLP: LLMs"
        ],
        "ML Subtopic (Is Starting Point)": [
            "NLP: LLMs"
        ],
        "Reviewed by": [
            "AG",
            "VK",
            "PC",
            "RS",
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Adversaries / Robustness / Generalization"
        ],
        "Time from MAIA / AISF / Elsewhere (min)": "25",
        "Title": "Red Teaming Language Models with Language Models (Perez et al., 2022)",
        "Type": [
            "Paper"
        ],
        "What sections to read from MAIA": "(sections 1 - 3.2 only)",
        "Who tagged for ML": [
            "Viktoria"
        ],
        "Year": "2022",
        "airtable_createdTime": "2023-09-05T22:50:05.000Z",
        "airtable_id": "rec4hIq8S3FvHZCiI"
    },
    {
        "# Top Paper votes": 0,
        "Abstract": "We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.\n",
        "Internal source": [
            "Elsewhere"
        ],
        "Link": "https://arxiv.org/abs/2001.08361",
        "Safety Category": [
            "Science of deep learning"
        ],
        "Title": "Scaling Laws for Neural Language Models (Kaplan et al., 2020)",
        "Type": [
            "Paper"
        ],
        "airtable_createdTime": "2024-03-31T18:06:47.000Z",
        "airtable_id": "rec51E04up31TMcpM"
    },
    {
        "# Top Paper votes": 1,
        "AD: Top Paper": true,
        "Abstract": "When making everyday decisions, people are guided by their conscience, an internal sense of right and wrong. By contrast, artificial agents are currently not endowed with a moral sense. As a consequence, they may learn to behave immorally when trained on environments that ignore moral concerns, such as violent video games. With the advent of generally capable agents that pretrain on many environments, it will become necessary to mitigate inherited biases from environments that teach immoral behavior. To facilitate the development of agents that avoid causing wanton harm, we introduce Jiminy Cricket, an environment suite of 25 text-based adventure games with thousands of diverse, morally salient scenarios. By annotating every possible game state, the Jiminy Cricket environments robustly evaluate whether agents can act morally while maximizing reward. Using models with commonsense moral knowledge, we create an elementary artificial conscience that assesses and guides agents. In extensive experiments, we find that the artificial conscience approach can steer agents towards moral behavior without sacrificing performance. \n",
        "Category": [
            "Model evaluations and benchmarks",
            "Reinforcement Learning"
        ],
        "Citations": 47,
        "Goes online in Expanded Papers": true,
        "Internal source": [
            "Intro to ML Safety Course"
        ],
        "Link": "https://arxiv.org/abs/2110.13136",
        "ML Subfield": [
            "Model Evaluation",
            "NLP",
            "Reinforcement Learning"
        ],
        "ML Subfield (Is Starting Point)": [
            "Reinforcement Learning",
            "Applied ML"
        ],
        "ML Subtopic": [
            "NLP: LLMs",
            "RL: Games"
        ],
        "Reviewed by": [
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Model evaluations / monitoring / detection"
        ],
        "Safety Topic": [
            "Benchmarking"
        ],
        "Title": "What Would Jiminy Cricket Do? Towards Agents That Behave Morally (Hendrycks et al., 2021)",
        "Type": [
            "Paper"
        ],
        "Who tagged for ML": [
            "Viktoria"
        ],
        "Year": "2021",
        "airtable_createdTime": "2024-01-15T21:45:02.000Z",
        "airtable_id": "rec591ebZ7o3c2v86"
    },
    {
        "# Top Paper votes": 0,
        "Category": [
            "AI Governance"
        ],
        "Goes online in Expanded Papers": true,
        "Internal source": [
            "Hendrycks Textbook"
        ],
        "Link": "https://docs.google.com/document/d/1DF31DIkwS9GONzmy1W3nuI9HRAwSKy8JcIbzKYXg-ic/edit#heading=h.kimhqj72mew4",
        "ML Subfield": [
            "Applied ML"
        ],
        "ML Subfield (Is Starting Point)": [
            "Applied ML"
        ],
        "ML Subtopic": [
            "Applied ML: Efficiency and Hardware"
        ],
        "Reviewed by": [
            "PC",
            "RS",
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Forecasting",
            "AI Governance"
        ],
        "Safety Topic": [
            "Compute governance"
        ],
        "Title": "\"Transformative AI and Compute\" Reading List (Heim, 2022)",
        "Type": [
            "Other"
        ],
        "Who tagged for ML": [
            "Max"
        ],
        "Year": "2022",
        "airtable_createdTime": "2023-12-12T20:00:03.000Z",
        "airtable_id": "rec59ry6AmXVmLgQ0"
    },
    {
        "# Top Paper votes": 1,
        "Abstract": "Research in Fairness, Accountability, Transparency, and Ethics (FATE) has established many sources and forms of algorithmic harm, in domains as diverse as health care, finance, policing, and recommendations. Much work remains to be done to mitigate the serious harms of these systems, particularly those disproportionately affecting marginalized communities. Despite these ongoing harms, new systems are being developed and deployed which threaten the perpetuation of the same harms and the creation of novel ones. In response, the FATE community has emphasized the importance of anticipating harms. Our work focuses on the anticipation of harms from increasingly agentic systems. Rather than providing a definition of agency as a binary property, we identify 4 key characteristics which, particularly in combination, tend to increase the agency of a given algorithmic system: underspecification, directness of impact, goal-directedness, and long-term planning. We also discuss important harms which arise from increasing agency -- notably, these include systemic and/or long-range impacts, often on marginalized stakeholders. We emphasize that recognizing agency of algorithmic systems does not absolve or shift the human responsibility for algorithmic harms. Rather, we use the term agency to highlight the increasingly evident fact that ML systems are not fully under human control. Our work explores increasingly agentic algorithmic systems in three parts. First, we explain the notion of an increase in agency for algorithmic systems in the context of diverse perspectives on agency across disciplines. Second, we argue for the need to anticipate harms from increasingly agentic systems. Third, we discuss important harms from increasingly agentic systems and ways forward for addressing them. We conclude by reflecting on implications of our work for anticipating algorithmic harms from emerging systems. \n",
        "Category": [
            "Why large-scale safety?"
        ],
        "Citations": 30,
        "Goes online as Top Paper": true,
        "Goes online in Expanded Papers": true,
        "Internal source": [
            "Twitter"
        ],
        "Link": "https://arxiv.org/abs/2302.10329",
        "ML Subfield": [
            "Domain General",
            "Reinforcement Learning",
            "Human Model Interaction"
        ],
        "ML Subfield (Is Starting Point)": [
            "Domain General"
        ],
        "Reviewed by": [
            "AG",
            "VK",
            "PC",
            "RS",
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Major problems (misalignment, misuse, threat models)",
            "Overview"
        ],
        "Title": "Harms from Increasingly Agentic Algorithmic Systems (Chan et al., 2023)",
        "Twitter": "https://x.com/_achan96_/status/1656690639264792579",
        "Type": [
            "Paper"
        ],
        "VK comments": "Great intro to risks from emergent agency for FATE folks\n",
        "VK: Top Paper": true,
        "Who tagged for ML": [
            "Max"
        ],
        "Year": "2023",
        "airtable_createdTime": "2023-09-12T20:50:25.000Z",
        "airtable_id": "rec5OCSpRy1uUhOTa"
    },
    {
        "# Top Paper votes": 0,
        "Category": [
            "Model evaluations and benchmarks"
        ],
        "Goes online as Top Paper": true,
        "Goes online in Expanded Papers": true,
        "Internal source": [
            "Elsewhere"
        ],
        "Link": "https://metr.github.io/autonomy-evals-guide/",
        "ML Subfield": [
            "Model Evaluation"
        ],
        "ML Subtopic": [
            "NLP: LLMs"
        ],
        "Safety Category": [
            "Model evaluations / monitoring / detection"
        ],
        "Safety Topic": [
            "Benchmarking"
        ],
        "Title": "METR's Autonomy Evaluation Resources",
        "Twitter": "https://x.com/METR_Evals/status/1768684026792190410",
        "Type": [
            "Other"
        ],
        "Who tagged for ML": [
            "Vael"
        ],
        "airtable_createdTime": "2024-03-18T21:31:13.000Z",
        "airtable_id": "rec5eFTEkHokBHzdp"
    },
    {
        "# Top Paper votes": 0,
        "Blog or Video": "https://www.alignment-workshop.com/nola-talks/brad-knox-your-rlhf-fine-tuning-is-secretly-applying-a-regret-preference",
        "Category": [
            "Scalable oversight"
        ],
        "Internal source": [
            "NOLA Workshop 2023"
        ],
        "Link": "https://arxiv.org/abs/2206.02231",
        "Safety Category": [
            "Scalable oversight",
            "Alignment <-> RLHF"
        ],
        "Title": "Models of human preference for learning reward functions (Knox et al., 2023)",
        "Type": [
            "Video",
            "Paper"
        ],
        "Vael's Notes": "Not including the lightning talks for now, since pretty EA probably\n",
        "airtable_createdTime": "2024-02-10T00:27:32.000Z",
        "airtable_id": "rec5gyRb3AuH7x6E1"
    },
    {
        "# Top Paper votes": 2,
        "AD: Top Paper": true,
        "Abstract": "As AI systems become more capable, we would like to enlist their help to supervise other AIs. We experiment with methods for training a harmless AI assistant through self-improvement, without any human labels identifying harmful outputs. The only human oversight is provided through a list of rules or principles, and so we refer to the method as 'Constitutional AI'. The process involves both a supervised learning and a reinforcement learning phase. In the supervised phase we sample from an initial model, then generate self-critiques and revisions, and then finetune the original model on revised responses. In the RL phase, we sample from the finetuned model, use a model to evaluate which of the two samples is better, and then train a preference model from this dataset of AI preferences. We then train with RL using the preference model as the reward signal, i.e. we use 'RL from AI Feedback' (RLAIF). As a result we are able to train a harmless but non-evasive AI assistant that engages with harmful queries by explaining its objections to them. Both the SL and RL methods can leverage chain-of-thought style reasoning to improve the human-judged performance and transparency of AI decision making. These methods make it possible to control AI behavior more precisely and with far fewer human labels. \n",
        "Blog or Video": "https://www.anthropic.com/index/constitutional-ai-harmlessness-from-ai-feedback",
        "Category": [
            "Scalable oversight"
        ],
        "Citations": 626,
        "Goes online as Top Paper": true,
        "Goes online in Expanded Papers": true,
        "High Citation and Max Select": true,
        "Internal source": [
            "AWAIR",
            "Further reading on MAIA/HAIST Summer 2023 Curriculum"
        ],
        "Link": "https://arxiv.org/abs/2212.08073",
        "ML Subfield": [
            "NLP",
            "Robustness and Adversariality",
            "Human Model Interaction"
        ],
        "ML Subfield (Is Starting Point)": [
            "Human Model Interaction",
            "Robustness and Adversariality"
        ],
        "ML Subtopic": [
            "NLP: LLMs"
        ],
        "Reviewed by": [
            "AG",
            "VK",
            "PC",
            "RS",
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Scalable oversight",
            "Alignment <-> RLHF"
        ],
        "Time from MAIA / AISF / Elsewhere (min)": "20",
        "Title": "Constitutional AI: Harmlessness from AI Feedback (Bai et al., 2022)",
        "Twitter": "https://x.com/AnthropicAI/status/1603791161419698181",
        "Type": [
            "Paper"
        ],
        "VK: Top Paper": true,
        "Vael's Notes": "\n",
        "What sections to read from MAIA": "(only sections 1, 3.1, and 4.1)",
        "Who tagged for ML": [
            "Viktoria"
        ],
        "Year": "2022",
        "airtable_createdTime": "2023-09-05T22:50:05.000Z",
        "airtable_id": "rec5sPK8IIRBhEwvs"
    },
    {
        "# Top Paper votes": 0,
        "Abstract": "\n",
        "Category": [
            "AI Governance"
        ],
        "Link": "https://www.alignment-workshop.com/nola-talks/gillian-hadfield-building-an-off-switch-for-ai",
        "ML Subfield": [
            "Domain General",
            "Human Model Interaction"
        ],
        "Safety Category": [
            "AI Governance"
        ],
        "Title": "Building an Off Switch for AI (Gillian Hadfield, 21-min video)",
        "Transcripts / Audio / Slides": "NOLA Workshop 2023",
        "Type": [
            "Video"
        ],
        "Who tagged for ML": [
            "Viktoria"
        ],
        "airtable_createdTime": "2024-02-10T00:27:32.000Z",
        "airtable_id": "rec6RcdCx0C8R8MdJ"
    },
    {
        "# Top Paper votes": 2,
        "AD: Top Paper": true,
        "Abstract": "To make AI systems broadly useful for challenging real-world tasks, we need them to learn complex human goals and preferences. One approach to specifying complex goals asks humans to judge during training which agent behaviors are safe and useful, but this approach can fail if the task is too complicated for a human to directly judge. To help address this concern, we propose training agents via self play on a zero sum debate game. Given a question or proposed action, two agents take turns making short statements up to a limit, then a human judges which of the agents gave the most true, useful information. In an analogy to complexity theory, debate with optimal play can answer any question in PSPACE given polynomial time judges (direct judging answers only NP questions). In practice, whether debate works involves empirical questions about humans and the tasks we want AIs to perform, plus theoretical questions about the meaning of AI alignment. We report results on an initial MNIST experiment where agents compete to convince a sparse classifier, boosting the classifier's accuracy from 59.4% to 88.9% given 6 pixels and from 48.2% to 85.2% given 4 pixels. Finally, we discuss theoretical and practical aspects of the debate model, focusing on potential weaknesses as the model scales up, and we propose future human and computer experiments to test these properties. \n",
        "Blog or Video": "https://openai.com/research/debate",
        "Category": [
            "Scalable oversight"
        ],
        "Citations": 130,
        "Context phrase (please add!)": "e.g. Debate is a technique that [description], which fits into scalable oversight [describe scalable oversight and how that fits into alignment] -Vael\n",
        "Internal source": [
            "AWAIR"
        ],
        "Link": "https://arxiv.org/abs/1805.00899",
        "ML Subfield": [
            "NLP"
        ],
        "Reviewed by": [
            "AG",
            "VK",
            "PC",
            "RS",
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Scalable oversight"
        ],
        "Safety Topic": [
            "Debate"
        ],
        "Time from MAIA / AISF / Elsewhere (min)": "10",
        "Title": "AI safety via debate (Irving, Christiano and Amodei, 2018)",
        "Type": [
            "Paper"
        ],
        "VK comments": "Foundational paper, accessible to ML audience\n",
        "VK: Top Paper": true,
        "Who tagged for ML": [
            "Viktoria"
        ],
        "Year": "2018",
        "airtable_createdTime": "2023-09-05T22:50:05.000Z",
        "airtable_id": "rec6SKJzP2JfxPI6s"
    },
    {
        "# Top Paper votes": 0,
        "Blog or Video": "https://www.alignment-workshop.com/nola-talks/vincent-conitzer-foundations-of-cooperative-ai",
        "Category": [
            "AI Governance"
        ],
        "Internal source": [
            "NOLA Workshop 2023"
        ],
        "Link": "https://www.cs.cmu.edu/~conitzer/FOCALAAAI23.pdf",
        "Safety Category": [
            "AI Governance"
        ],
        "Title": "Foundations of Cooperative AI (Conitzer and Oesterheld, 2023)",
        "Type": [
            "Video",
            "Paper"
        ],
        "Vael's Notes": "Not including the lightning talks for now, since pretty EA probably\n",
        "airtable_createdTime": "2024-02-10T00:27:32.000Z",
        "airtable_id": "rec7ZxkhTikQvIs8y"
    },
    {
        "# Top Paper votes": 0,
        "Abstract": "We introduce methods for discovering and applying sparse feature circuits. These are causally implicated subnetworks of human-interpretable features for explaining language model behaviors. Circuits identified in prior work consist of polysemantic and difficult-to-interpret units like attention heads or neurons, rendering them unsuitable for many downstream applications. In contrast, sparse feature circuits enable detailed understanding of unanticipated mechanisms. Because they are based on fine-grained units, sparse feature circuits are useful for downstream tasks: We introduce SHIFT, where we improve the generalization of a classifier by ablating features that a human judges to be task-irrelevant. Finally, we demonstrate an entirely unsupervised and scalable interpretability pipeline by discovering thousands of sparse feature circuits for automatically discovered model behaviors.\n",
        "Category": [
            "Interpretability / explainability"
        ],
        "Goes online in Expanded Papers": true,
        "Link": "https://arxiv.org/abs/2403.19647",
        "Title": "Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models",
        "Type": [
            "Paper"
        ],
        "Year": "2024",
        "airtable_createdTime": "2024-08-09T15:35:33.000Z",
        "airtable_id": "rec7kAJXCg69Qode8"
    },
    {
        "# Top Paper votes": 0,
        "Abstract": "Generative Pre-trained Transformer (GPT) models have exhibited exciting progress in their capabilities, capturing the interest of practitioners and the public alike. Yet, while the literature on the trustworthiness of GPT models remains limited, practitioners have proposed employing capable GPT models for sensitive applications such as healthcare and finance -- where mistakes can be costly. To this end, this work proposes a comprehensive trustworthiness evaluation for large language models with a focus on GPT-4 and GPT-3.5, considering diverse perspectives -- including toxicity, stereotype bias, adversarial robustness, out-of-distribution robustness, robustness on adversarial demonstrations, privacy, machine ethics, and fairness. Based on our evaluations, we discover previously unpublished vulnerabilities to trustworthiness threats. For instance, we find that GPT models can be easily misled to generate toxic and biased outputs and leak private information in both training data and conversation history. We also find that although GPT-4 is usually more trustworthy than GPT-3.5 on standard benchmarks, GPT-4 is more vulnerable given jailbreaking system or user prompts, potentially because GPT-4 follows (misleading) instructions more precisely. Our work illustrates a comprehensive trustworthiness evaluation of GPT models and sheds light on the trustworthiness gaps. Our benchmark is publicly available at\u00a0[this https URL](https://decodingtrust.github.io/;)\u00a0our dataset can be previewed at\u00a0[this https URL](https://huggingface.co/datasets/AI-Secure/DecodingTrust;)\u00a0a concise version of this work is at\u00a0[this https URL](https://openreview.net/pdf?id=kaHpo8OZw2).\n",
        "Category": [
            "Model evaluations and benchmarks"
        ],
        "Citations": 130,
        "Internal source": [
            "http://rdi.berkeley.edu/understanding_llms/s24"
        ],
        "Link": "https://arxiv.org/abs//2306.11698",
        "Safety Category": [
            "Model evaluations / monitoring / detection"
        ],
        "Supplemental Material": "https://decodingtrust.github.io/",
        "Title": "DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models (Wang et al., 2023)",
        "Type": [
            "Paper",
            "Other"
        ],
        "Year": "2023",
        "airtable_createdTime": "2024-03-31T18:06:47.000Z",
        "airtable_id": "rec8n7yCS6QaEB4RG"
    },
    {
        "# Top Paper votes": 0,
        "Supplemental Material": "Response writeup: https://www.alignmentforum.org/posts/bWxNPMy5MhPnQTzKz/what-discovering-latent-knowledge-did-and-did-not-find-4",
        "Title": "Attached to Collin's paper",
        "airtable_createdTime": "2024-02-10T00:27:32.000Z",
        "airtable_id": "rec8zhs6IYPX6OBAD"
    },
    {
        "# Top Paper votes": 0,
        "Link": "https://docs.google.com/document/d/1b83_-eo9NEaKDKc9R3P5h5xkLImqMw8ADLmi__rkLo4/edit#heading=h.fke682cxqkxr",
        "Safety Category": [
            "Overview"
        ],
        "Title": "Leveling up in AI Safety Research Engineering",
        "airtable_createdTime": "2024-02-10T00:27:32.000Z",
        "airtable_id": "rec969JFPmvx8csyU"
    },
    {
        "# Top Paper votes": 0,
        "Abstract": "    We introduce four new real-world distribution shift datasets consisting of changes in image style, image blurriness, geographic location, camera operation, and more. With our new datasets, we take stock of previously proposed methods for improving out-of-distribution robustness and put them to the test. We find that using larger models and artificial data augmentations can improve robustness on real-world distribution shifts, contrary to claims in prior work. We find improvements in artificial robustness benchmarks can transfer to real-world distribution shifts, contrary to claims in prior work. Motivated by our observation that data augmentations can help with real-world distribution shifts, we also introduce a new data augmentation method which advances the state-of-the-art and outperforms models pretrained with 1000 times more labeled data. Overall we find that some methods consistently help with distribution shifts in texture and local image statistics, but these methods do not help with some other distribution shifts like geographic changes. Our results show that future research must study multiple distribution shifts simultaneously, as we demonstrate that no evaluated method consistently improves robustness. \n",
        "Category": [
            "Non-Adversarial Robustness"
        ],
        "Citations": 1274,
        "Internal source": [
            "Intro to ML Safety Course"
        ],
        "Link": "https://arxiv.org/abs/2006.16241",
        "ML Subfield": [
            "Robustness and Adversariality",
            "Vision"
        ],
        "Reviewed by": [
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Adversaries / Robustness / Generalization"
        ],
        "Title": "The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization (Hendrycks et al., 2021)",
        "Type": [
            "Paper"
        ],
        "Who tagged for ML": [
            "Viktoria"
        ],
        "Year": "2021",
        "airtable_createdTime": "2024-01-15T21:23:40.000Z",
        "airtable_id": "rec99m2yzbl5DwFe1"
    },
    {
        "# Top Paper votes": 0,
        "Abstract": "\n",
        "Category": [
            "Reinforcement Learning",
            "Scalable oversight"
        ],
        "Link": "https://openai.com/blog/deep-reinforcement-learning-from-human-preferences/",
        "ML Subfield": [
            "Applied ML",
            "Reinforcement Learning",
            "Human Model Interaction"
        ],
        "ML Subfield (Is Starting Point)": [
            "Applied ML"
        ],
        "ML Subtopic": [
            "RL: Simulations",
            "RL: Games",
            "Applied ML: Robotics"
        ],
        "Reviewed by": [
            "AG",
            "VK",
            "PC",
            "RS"
        ],
        "Safety Category": [
            "Alignment <-> RLHF"
        ],
        "Time from MAIA / AISF / Elsewhere (min)": "5",
        "Title": "Learning from human preferences (Christiano et al., 2017) ",
        "Type": [
            "Blog post"
        ],
        "Who tagged for ML": [
            "Max"
        ],
        "airtable_createdTime": "2024-02-10T00:27:32.000Z",
        "airtable_id": "rec9pXDP8x2IuawkO"
    },
    {
        "# Top Paper votes": 1,
        "AG comments": "Accessible and clearly introduces scalable oversight.\n",
        "AG: Top Paper": true,
        "Abstract": "Developing safe and useful general-purpose AI systems will require us to make progress on scalable oversight: the problem of supervising systems that potentially outperform us on most skills relevant to the task at hand. Empirical work on this problem is not straightforward, since we do not yet have systems that broadly exceed our abilities. This paper discusses one of the major ways we think about this problem, with a focus on ways it can be studied empirically. We first present an experimental design centered on tasks for which human specialists succeed but unaided humans and current general AI systems fail. We then present a proof-of-concept experiment meant to demonstrate a key feature of this experimental design and show its viability with two question-answering tasks: MMLU and time-limited QuALITY. On these tasks, we find that human participants who interact with an unreliable large-language-model dialog assistant through chat -- a trivial baseline strategy for scalable oversight -- substantially outperform both the model alone and their own unaided performance. These results are an encouraging sign that scalable oversight will be tractable to study with present models and bolster recent findings that large language models can productively assist humans with difficult tasks. \n",
        "Blog or Video": "https://www.youtube.com/watch?v=U2zJuTLzIm8&t=2s",
        "Category": [
            "Scalable oversight"
        ],
        "Citations": 46,
        "Goes online as Top Paper": true,
        "Goes online in Expanded Papers": true,
        "Internal source": [
            "AWAIR",
            "Elsewhere",
            "Alignment Workshop 2023"
        ],
        "Link": "https://arxiv.org/abs/2211.03540",
        "ML Subfield": [
            "NLP",
            "Model Evaluation"
        ],
        "ML Subtopic": [
            "NLP: LLMs"
        ],
        "Reviewed by": [
            "AG",
            "VK",
            "PC",
            "RS",
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Scalable oversight",
            "Model evaluations / monitoring / detection"
        ],
        "Time from MAIA / AISF / Elsewhere (min)": "25",
        "Title": "Measuring Progress on Scalable Oversight for Large Language Models (Bowman et al., 2022)",
        "Twitter": "https://x.com/AnthropicAI/status/1590019597109202946",
        "Type": [
            "Paper",
            "Video"
        ],
        "What sections to read from MAIA": "sections 1-3 only",
        "Who tagged for ML": [
            "Viktoria"
        ],
        "Year": "2022",
        "airtable_createdTime": "2023-09-05T22:50:05.000Z",
        "airtable_id": "rec9wqCg5kR6sjgG9"
    },
    {
        "# Top Paper votes": 0,
        "Blog or Video": "https://www.alignment-workshop.com/nola-talks/atticus-geiger-theories-and-tools-for-mechanistic-interpretability-via-ca",
        "Category": [
            "Interpretability / explainability"
        ],
        "Internal source": [
            "NOLA Workshop 2023"
        ],
        "Link": "https://ai.stanford.edu/blog/causal-abstraction/",
        "Safety Category": [
            "Transparency / Interpretability / Model internals / Latent knowledge"
        ],
        "Safety Topic": [
            "Mechanistic interpretability"
        ],
        "Title": "Faithful, Interpretable Model Explanations via Causal Abstraction (Geiger et al., 2022)",
        "Type": [
            "Video",
            "Paper"
        ],
        "Vael's Notes": "Not including the lightning talks for now, since pretty EA probably\n",
        "airtable_createdTime": "2024-02-10T00:27:32.000Z",
        "airtable_id": "recA02u1hPIbrnwzn"
    },
    {
        "# Top Paper votes": 0,
        "Abstract": "In 2005, a letter published in Nature described human neurons responding to specific people, such as Jennifer Aniston or Halle Berry . The exciting thing wasn\u2019t just that they selected for particular people, but that they did so regardless of whether they were shown photographs, drawings, or even images of the person\u2019s name. The neurons were multimodal. As the lead author would put it: \"You are looking at the far end of the transformation from metric, visual shapes to conceptual\u2026 information.\" Quiroga's full quote, from reads: \"I think that\u2019s the excitement to these results. You are looking at the far end of the transformation from metric, visual shapes to conceptual memory-related information. It is that transformation that underlies our ability to understand the world. It\u2019s not enough to see something familiar and match it. It\u2019s the fact that you plug visual information into the rich tapestry of memory that brings it to life.\" We elided the portion discussing memory since it was less relevant. We report the existence of similar multimodal neurons in artificial neural networks. This includes neurons selecting for prominent public figures or fictional characters, such as Lady Gaga or Spiderman. \n",
        "Category": [
            "Interpretability / explainability"
        ],
        "Internal source": [
            "Further reading on MAIA/HAIST Summer 2023 Curriculum"
        ],
        "Link": "https://distill.pub/2021/multimodal-neurons/",
        "ML Subtopic": [
            "Robustness and Adversariality"
        ],
        "Reviewed by": [
            "AG",
            "VK",
            "PC",
            "RS"
        ],
        "Safety Category": [
            "Transparency / Interpretability / Model internals / Latent knowledge"
        ],
        "Safety Topic": [
            "Mechanistic interpretability"
        ],
        "Time from MAIA / AISF / Elsewhere (min)": "35",
        "Title": "Multimodal neurons in artificial neural networks (Goh et al., 2021) ",
        "Type": [
            "Paper"
        ],
        "Who tagged for ML": [
            "Viktoria"
        ],
        "airtable_createdTime": "2024-02-10T00:27:32.000Z",
        "airtable_id": "recAALDE8hLQvTSxz"
    },
    {
        "# Top Paper votes": 1,
        "Abstract": "    Because \"out-of-the-box\" large language models are capable of generating a great deal of objectionable content, recent work has focused on aligning these models in an attempt to prevent undesirable generation. While there has been some success at circumventing these measures -- so-called \"jailbreaks\" against LLMs -- these attacks have required significant human ingenuity and are brittle in practice. In this paper, we propose a simple and effective attack method that causes aligned language models to generate objectionable behaviors. Specifically, our approach finds a suffix that, when attached to a wide range of queries for an LLM to produce objectionable content, aims to maximize the probability that the model produces an affirmative response (rather than refusing to answer). However, instead of relying on manual engineering, our approach automatically produces these adversarial suffixes by a combination of greedy and gradient-based search techniques, and also improves over past automatic prompt generation methods.\n",
        "Blog or Video": "https://www.alignment-workshop.com/nola-talks/zico-kolter-adversarial-attacks-on-aligned-language-models",
        "Category": [
            "Adversarial Robustness"
        ],
        "Citations": 370,
        "Goes online as Top Paper": true,
        "Goes online in Expanded Papers": true,
        "High Citation and Max Select": true,
        "Internal source": [
            "Hendrycks Textbook",
            "NOLA Workshop 2023"
        ],
        "Link": "http://arxiv.org/abs/2307.15043",
        "ML Subfield": [
            "Robustness and Adversariality",
            "NLP"
        ],
        "ML Subfield (Is Starting Point)": [
            "Robustness and Adversariality"
        ],
        "ML Subtopic": [
            "NLP: LLMs",
            "Applied ML: Chatbots"
        ],
        "ML Subtopic (Is Starting Point)": [
            "NLP: LLMs"
        ],
        "PC: Top Paper": true,
        "Reviewed by": [
            "PC",
            "RS",
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Adversaries / Robustness / Generalization"
        ],
        "Title": "Universal and Transferable Adversarial Attacks on Aligned Language Models (Zou et al., 2023)",
        "Twitter": "https://x.com/andyzou_jiaming/status/1684766170766004224",
        "Type": [
            "Paper",
            "Video"
        ],
        "Who tagged for ML": [
            "Viktoria"
        ],
        "Year": "2023",
        "airtable_createdTime": "2023-12-12T20:00:03.000Z",
        "airtable_id": "recAQIigfFX0LahEl"
    },
    {
        "# Top Paper votes": 1,
        "Category": [
            "Reward misspecification and goal misgeneralization",
            "Reinforcement Learning"
        ],
        "Description from MAIA / AISF / Elsewhere": "Krakovna et al. showcase examples of agents exploiting mistaken training specifications in simple environments. Note that \u201cspecification gaming\u201d is an umbrella term which includes reward hacking as well as similar behavior by non-RL agents.",
        "Internal source": [
            "Elsewhere"
        ],
        "Link": "https://www.deepmind.com/blog/specification-gaming-the-flip-side-of-ai-ingenuity",
        "ML Subfield": [
            "Reinforcement Learning"
        ],
        "ML Subtopic": [
            "RL: Simulations",
            "RL: Games",
            "Applied ML: Robotics"
        ],
        "Reviewed by": [
            "AG",
            "VK",
            "PC",
            "RS",
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Reward misspecification and goal misgeneralization"
        ],
        "Time from MAIA / AISF / Elsewhere (min)": "15",
        "Title": "Specification gaming: the flip side of AI ingenuity (Krakovna et al., 2020)",
        "Transcripts / Audio / Slides": "https://preview.type3.audio/episode/agi-safety-fundamentals-alignment/4bf4ddd8-3876-47f6-ac5a-c6c1fbf8eae7",
        "Type": [
            "Blog post"
        ],
        "VK: Top Paper": true,
        "Who tagged for ML": [
            "Max"
        ],
        "Year": "2020",
        "airtable_createdTime": "2023-09-05T22:50:05.000Z",
        "airtable_id": "recAeQR4kX7ZhYm2W"
    },
    {
        "# Top Paper votes": 0,
        "Category": [
            "Interpretability / explainability"
        ],
        "Internal source": [
            "NOLA Workshop 2023"
        ],
        "Link": "https://www.alignment-workshop.com/nola-talks/been-kim-alignment-and-interpretability-how-we-might-get-it-right",
        "ML Subfield": [
            "Human Model Interaction"
        ],
        "ML Subtopic": [
            "Applied ML: Cognitive Tasks"
        ],
        "Reviewed by": [
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Transparency / Interpretability / Model internals / Latent knowledge"
        ],
        "Title": "Alignment and Interpretability: How we might get it right (Been Kim, 33-min video)",
        "Type": [
            "Video"
        ],
        "Who tagged for ML": [
            "Viktoria"
        ],
        "airtable_createdTime": "2024-01-15T20:13:29.000Z",
        "airtable_id": "recAnRBldZAaTkeum"
    },
    {
        "# Top Paper votes": 0,
        "Blog or Video": "https://www.alignment-workshop.com/nola-talks/eric-michaud-the-quantization-model-of-neural-scaling",
        "Category": [
            "Interpretability / explainability"
        ],
        "Internal source": [
            "NOLA Workshop 2023"
        ],
        "Link": "https://arxiv.org/abs/2303.13506",
        "Safety Category": [
            "Transparency / Interpretability / Model internals / Latent knowledge"
        ],
        "Title": "The Quantization Model of Neural Scaling (Michaud et al., 2023)",
        "Type": [
            "Video",
            "Paper"
        ],
        "Vael's Notes": "Not including the lightning talks for now, since pretty EA probably\n",
        "airtable_createdTime": "2024-02-10T00:27:32.000Z",
        "airtable_id": "recBroGkZZFvpbFO4"
    },
    {
        "# Top Paper votes": 0,
        "Abstract": "Despite extensive diagnostics and debugging by developers, AI systems sometimes exhibit harmful unintended behaviors. Finding and fixing these is challenging because the attack surface is so large -- it is not tractable to exhaustively search for inputs that may elicit harmful behaviors. Red-teaming and adversarial training (AT) are commonly used to improve robustness, however, they empirically struggle to fix failure modes that differ from the attacks used during training. In this work, we utilize latent adversarial training (LAT) to defend against vulnerabilities without generating inputs that elicit them. LAT leverages the compressed, abstract, and structured latent representations of concepts that the network actually uses for prediction. We use it to remove trojans and defend against held-out classes of adversarial attacks. We show in image classification, text classification, and text generation tasks that LAT usually improves both robustness to novel attacks and performance on clean data relative to AT. This suggests that LAT can be a promising tool for defending against failure modes that are not explicitly identified by developers.\n",
        "Category": [
            "Adversarial Robustness"
        ],
        "Goes online in Expanded Papers": true,
        "Link": "https://arxiv.org/abs/2403.05030",
        "Title": "Defending Against Unforeseen Failure Modes with Latent Adversarial Training",
        "Type": [
            "Paper"
        ],
        "Year": "2024",
        "airtable_createdTime": "2024-08-09T15:31:29.000Z",
        "airtable_id": "recBxgTzrvTBRvRIC"
    },
    {
        "# Top Paper votes": 0,
        "Abstract": "To understand the risks posed by a new AI system, we must understand what it can and cannot do. Building on prior work, we introduce a programme of new \"dangerous capability\" evaluations and pilot them on Gemini 1.0 models. Our evaluations cover four areas: (1) persuasion and deception; (2) cyber-security; (3) self-proliferation; and (4) self-reasoning. We do not find evidence of strong dangerous capabilities in the models we evaluated, but we flag early warning signs. Our goal is to help advance a rigorous science of dangerous capability evaluation, in preparation for future models.\n",
        "Category": [
            "Model evaluations and benchmarks"
        ],
        "Citations": 2,
        "Goes online as Top Paper": true,
        "Goes online in Expanded Papers": true,
        "Internal source": [
            "Elsewhere"
        ],
        "Link": "https://arxiv.org/abs/2403.13793",
        "ML Subfield": [
            "Model Evaluation"
        ],
        "ML Subtopic": [
            "NLP: LLMs"
        ],
        "Safety Category": [
            "Model evaluations / monitoring / detection"
        ],
        "Title": "Evaluating Frontier Models for Dangerous Capabilities (Phuong et al., 2024)",
        "Twitter": "https://x.com/tshevl/status/1770744344669990981",
        "Type": [
            "Paper"
        ],
        "Who tagged for ML": [
            "Vael"
        ],
        "Year": "2024",
        "airtable_createdTime": "2024-03-30T19:28:53.000Z",
        "airtable_id": "recCP4plEV1Vdhi89"
    },
    {
        "# Top Paper votes": 0,
        "Abstract": "    We describe an \"interpretability illusion\" that arises when analyzing the BERT model. Activations of individual neurons in the network may spuriously appear to encode a single, simple concept, when in fact they are encoding something far more complex. The same effect holds for linear combinations of activations. We trace the source of this illusion to geometric properties of BERT's embedding space as well as the fact that common text corpora represent only narrow slices of possible English sentences. We provide a taxonomy of model-learned concepts and discuss methodological implications for interpretability research, especially the importance of testing hypotheses on multiple data sets. \n",
        "Category": [
            "Interpretability / explainability"
        ],
        "Internal source": [
            "Further reading on MAIA/HAIST Summer 2023 Curriculum"
        ],
        "Link": "https://arxiv.org/pdf/2104.07143.pdf",
        "ML Subfield": [
            "NLP: Language Modeling"
        ],
        "ML Subtopic": [
            "NLP"
        ],
        "Reviewed by": [
            "AG",
            "VK",
            "PC",
            "RS"
        ],
        "Safety Category": [
            "Transparency / Interpretability / Model internals / Latent knowledge"
        ],
        "Safety Topic": [
            "Mechanistic interpretability"
        ],
        "Time from MAIA / AISF / Elsewhere (min)": "35",
        "Title": "An Interpretability Illusion for BERT (Bolukbasi et al., 2021)",
        "Type": [
            "Paper"
        ],
        "Who tagged for ML": [
            "Viktoria"
        ],
        "airtable_createdTime": "2024-02-10T00:27:32.000Z",
        "airtable_id": "recCdhaALr2qezA2w"
    },
    {
        "# Top Paper votes": 1,
        "AD: Top Paper": true,
        "Category": [
            "Overviews and agendas"
        ],
        "Goes online in Expanded Papers": true,
        "Internal source": [
            "NOLA Workshop 2023"
        ],
        "Link": "https://www.alignment-workshop.com/nola-talks/adam-gleave-agi-safety-risks-and-research-directions",
        "ML Subfield": [
            "Security",
            "Domain General",
            "Applied ML"
        ],
        "Reviewed by": [
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Major problems (misalignment, misuse, threat models)",
            "Overview"
        ],
        "Title": "AGI Safety: Risks and Research Directions (Adam Gleave, 31-min video)\n",
        "Type": [
            "Video"
        ],
        "Who tagged for ML": [
            "Viktoria"
        ],
        "airtable_createdTime": "2024-01-15T20:13:29.000Z",
        "airtable_id": "recCiFtvGWLwRARlv"
    },
    {
        "# Top Paper votes": 0,
        "Category": [
            "Deception",
            "Scalable oversight"
        ],
        "Citations": 5,
        "Goes online as Top Paper": true,
        "Goes online in Expanded Papers": true,
        "Internal source": [
            "Elsewhere"
        ],
        "Link": "https://arxiv.org/abs/2312.06942",
        "ML Subfield": [
            "NLP",
            "Robustness and Adversariality"
        ],
        "ML Subfield (Is Starting Point)": [
            "Robustness and Adversariality"
        ],
        "Safety Category": [
            "Adversaries / Robustness / Generalization",
            "Model evaluations / monitoring / detection",
            "Deception"
        ],
        "Title": "AI Control: Improving Safety Despite Intentional Subversion (Greenblatt et al., 2024)",
        "Type": [
            "Paper"
        ],
        "Who tagged for ML": [
            "Vael"
        ],
        "Year": "2024",
        "airtable_createdTime": "2024-02-14T01:06:06.000Z",
        "airtable_id": "recCnnUMTuMstlbyj"
    },
    {
        "# Top Paper votes": 0,
        "Abstract": "    Large Language Models (LLMs) can achieve strong performance on many tasks by producing step-by-step reasoning before giving a final output, often referred to as chain-of-thought reasoning (CoT). It is tempting to interpret these CoT explanations as the LLM's process for solving a task. This level of transparency into LLMs' predictions would yield significant safety benefits. However, we find that CoT explanations can systematically misrepresent the true reason for a model's prediction. We demonstrate that CoT explanations can be heavily influenced by adding biasing features to model inputs--e.g., by reordering the multiple-choice options in a few-shot prompt to make the answer always \"(A)\"--which models systematically fail to mention in their explanations. When we bias models toward incorrect answers, they frequently generate CoT explanations rationalizing those answers. This causes accuracy to drop by as much as 36% on a suite of 13 tasks from BIG-Bench Hard, when testing with GPT-3.5 from OpenAI and Claude 1.0 from Anthropic. On a social-bias task, model explanations justify giving answers in line with stereotypes without mentioning the influence of these social biases. Our findings indicate that CoT explanations can be plausible yet misleading, which risks increasing our trust in LLMs without guaranteeing their safety. Building more transparent and explainable systems will require either improving CoT faithfulness through targeted efforts or abandoning CoT in favor of alternative methods. \n",
        "Category": [
            "Deception"
        ],
        "Internal source": [
            "Twitter"
        ],
        "Link": "https://arxiv.org/pdf/2305.04388.pdf",
        "ML Subfield": [
            "Applied ML: Cognitive Tasks",
            "NLP: LLMs"
        ],
        "ML Subtopic": [
            "NLP",
            "Applied ML",
            "Human Model Interaction"
        ],
        "Reviewed by": [
            "AG",
            "VK",
            "PC",
            "RS"
        ],
        "Safety Category": [
            "Deception"
        ],
        "Safety Topic": [
            "Sycophancy"
        ],
        "Title": "Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting (Turpin et al., 2023)",
        "Twitter": "https://twitter.com/milesaturpin/status/1656010877269602304",
        "Type": [
            "Paper"
        ],
        "Who tagged for ML": [
            "Max"
        ],
        "airtable_createdTime": "2024-02-10T00:27:32.000Z",
        "airtable_id": "recD8cozbdfMi86zQ"
    },
    {
        "# Top Paper votes": 0,
        "AD comments": "\n",
        "Abstract": "Labeling neural network submodules with human-legible descriptions is useful for many downstream tasks: such descriptions can surface failures, guide interventions, and perhaps even explain important model behaviors. To date, most mechanistic descriptions of trained networks have involved small models, narrowly delimited phenomena, and large amounts of human labor. Labeling all human-interpretable sub-computations in models of increasing size and complexity will almost certainly require tools that can generate and validate descriptions automatically. Recently, techniques that use learned models in-the-loop for labeling have begun to gain traction, but methods for evaluating their efficacy are limited and ad-hoc. How should we validate and compare open-ended labeling tools? This paper introduces FIND (Function INterpretation and Description), a benchmark suite for evaluating the building blocks of automated interpretability methods. FIND contains functions that resemble components of trained neural networks, and accompanying descriptions of the kind we seek to generate. The functions span textual and numeric domains, and involve a range of real-world complexities. We evaluate methods that use pretrained language models (LMs) to produce descriptions of function behavior in natural language and code. Additionally, we introduce a new interactive method in which an Automated Interpretability Agent (AIA) generates function descriptions. We find that an AIA, built from an LM with black-box access to functions, can infer function structure, acting as a scientist by forming hypotheses, proposing experiments, and updating descriptions in light of new data. However, AIA descriptions tend to capture global function behavior and miss local details. These results suggest that FIND will be useful for evaluating more sophisticated interpretability methods before they are applied to real-world models. \n",
        "Category": [
            "Interpretability / explainability"
        ],
        "Description from MAIA / AISF / Elsewhere": "[Paper] constructs a variety of complex functions and allows an interpreter to sample inputs and outputs from the function. The interpreter then attempts to provide a natural language explanation of the function, and code which implements it. The proposed code is evaluated against the original on a set of inputs, and an LLM judge evaluates the natural language explanation.",
        "Internal source": [
            "ML Newsletter"
        ],
        "Link": "https://arxiv.org/abs/2309.03886",
        "ML Subfield": [
            "Model Evaluation",
            "NLP"
        ],
        "ML Subtopic": [
            "NLP: LLMs"
        ],
        "Reviewed by": [
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Transparency / Interpretability / Model internals / Latent knowledge"
        ],
        "Safety Topic": [
            "Model editing"
        ],
        "Title": "FIND: A Function Description Benchmark for Evaluating Interpretability Methods (Schwettmann et al., 2023)",
        "Type": [
            "Paper"
        ],
        "Who tagged for ML": [
            "Viktoria"
        ],
        "airtable_createdTime": "2023-12-20T01:45:25.000Z",
        "airtable_id": "recDQWwaQINrghF8n"
    },
    {
        "# Top Paper votes": 0,
        "Abstract": "We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.\n",
        "Internal source": [
            "http://rdi.berkeley.edu/understanding_llms/s24"
        ],
        "Link": "https://arxiv.org/abs/2201.11903",
        "Safety Category": [
            "Science of deep learning"
        ],
        "Title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models (Wei et al., 2022)",
        "Type": [
            "Paper"
        ],
        "airtable_createdTime": "2024-03-31T18:06:47.000Z",
        "airtable_id": "recDUk4fyYG0v6S3J"
    },
    {
        "# Top Paper votes": 0,
        "Category": [
            "Reward misspecification and goal misgeneralization"
        ],
        "Internal source": [
            "NOLA Workshop 2023"
        ],
        "Link": "https://www.alignment-workshop.com/nola-talks/dawn-song-challenges-for-ai-safety-in-adversarial-settings",
        "Safety Category": [
            "Reward misspecification and goal misgeneralization",
            "Adversaries / Robustness / Generalization"
        ],
        "Title": "Challenges for AI safety in adversarial settings (Dawn Song, 6-min video)",
        "Type": [
            "Video"
        ],
        "Vael's Notes": "Not including the lightning talks for now, since pretty EA probably\n",
        "airtable_createdTime": "2024-02-10T00:27:32.000Z",
        "airtable_id": "recDibYSi2u4Q6crv"
    },
    {
        "# Top Paper votes": 0,
        "Abstract": "In this paper we establish rigorous benchmarks for image classifier robustness. Our first benchmark, ImageNet-C, standardizes and expands the corruption robustness topic, while showing which classifiers are preferable in safety-critical applications. Then we propose a new dataset called ImageNet-P which enables researchers to benchmark a classifier's robustness to common perturbations. Unlike recent robustness research, this benchmark evaluates performance on common corruptions and perturbations not worst-case adversarial perturbations. We find that there are negligible changes in relative corruption robustness from AlexNet classifiers to ResNet classifiers. Afterward we discover ways to enhance corruption and perturbation robustness. We even find that a bypassed adversarial defense provides substantial common perturbation robustness. Together our benchmarks may aid future work toward networks that robustly generalize. \n",
        "Category": [
            "Non-Adversarial Robustness"
        ],
        "Citations": 3204,
        "Goes online in Expanded Papers": true,
        "High Citation and Max Select": true,
        "Internal source": [
            "Intro to ML Safety Course"
        ],
        "Link": "https://arxiv.org/abs/1903.12261",
        "ML Subfield": [
            "Robustness and Adversariality",
            "Vision",
            "Model Evaluation"
        ],
        "ML Subfield (Is Starting Point)": [
            "Vision"
        ],
        "Reviewed by": [
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Adversaries / Robustness / Generalization",
            "Model evaluations / monitoring / detection"
        ],
        "Safety Topic": [
            "Benchmarking"
        ],
        "Title": "Benchmarking Neural Network Robustness to Common Corruptions and Perturbations (Hendrycks and Dietterich, 2019)",
        "Type": [
            "Paper"
        ],
        "Who tagged for ML": [
            "Viktoria"
        ],
        "Year": "2019",
        "airtable_createdTime": "2024-01-15T21:25:26.000Z",
        "airtable_id": "recENJwPyHudauOgY"
    },
    {
        "# Top Paper votes": 2,
        "AD: Top Paper": true,
        "Abstract": "    In reinforcement learning from human feedback, it is common to optimize against a reward model trained to predict human preferences. Because the reward model is an imperfect proxy, optimizing its value too much can hinder ground truth performance, in accordance with Goodhart's law. This effect has been frequently observed, but not carefully measured due to the expense of collecting human preference data. In this work, we use a synthetic setup in which a fixed \"gold-standard\" reward model plays the role of humans, providing labels used to train a proxy reward model. We study how the gold reward model score changes as we optimize against the proxy reward model using either reinforcement learning or best-of-n sampling. We find that this relationship follows a different functional form depending on the method of optimization, and that in both cases its coefficients scale smoothly with the number of reward model parameters. We also study the effect on this relationship of the size of the reward model dataset, the number of reward model and policy parameters, and the coefficient of the KL penalty added to the reward in the reinforcement learning setup. We explore the implications of these empirical results for theoretical considerations in AI alignment. \n",
        "Category": [
            "Reward misspecification and goal misgeneralization"
        ],
        "Citations": 157,
        "Description from MAIA / AISF / Elsewhere": "Gao et al. investigate quantitative trends in how ground-truth reward scales when optimizing against a proxy learned reward model.",
        "Goes online in Expanded Papers": true,
        "Internal source": [
            "Further reading on MAIA/HAIST Summer 2023 Curriculum"
        ],
        "Link": "https://arxiv.org/abs/2210.10760",
        "ML Subfield": [
            "Applied ML",
            "Optimization"
        ],
        "Reviewed by": [
            "AG",
            "VK",
            "PC",
            "RS",
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Reward misspecification and goal misgeneralization",
            "Alignment <-> RLHF"
        ],
        "Time from MAIA / AISF / Elsewhere (min)": "25",
        "Title": "Scaling Laws for Reward Model Overoptimization (Gao et al., 2022) ",
        "Type": [
            "Paper"
        ],
        "VK: Top Paper": true,
        "What sections to read from MAIA": "(sections 1-3 only) ",
        "Who tagged for ML": [
            "Max"
        ],
        "Year": "2022",
        "airtable_createdTime": "2023-09-16T22:33:06.000Z",
        "airtable_id": "recEhoB0Oz1scSYqi"
    },
    {
        "# Top Paper votes": 0,
        "AD comments": "\n",
        "Abstract": "    Artificial agents have traditionally been trained to maximize reward, which may incentivize power-seeking and deception, analogous to how next-token prediction in language models (LMs) may incentivize toxicity. So do agents naturally learn to be Machiavellian? And how do we measure these behaviors in general-purpose models such as GPT-4? Towards answering these questions, we introduce MACHIAVELLI, a benchmark of 134 Choose-Your-Own-Adventure games containing over half a million rich, diverse scenarios that center on social decision-making. Scenario labeling is automated with LMs, which are more performant than human annotators. We mathematize dozens of harmful behaviors and use our annotations to evaluate agents' tendencies to be power-seeking, cause disutility, and commit ethical violations. We observe some tension between maximizing reward and behaving ethically. To improve this trade-off, we investigate LM-based methods to steer agents' towards less harmful behaviors. Our results show that agents can both act competently and morally, so concrete progress can currently be made in machine ethics--designing agents that are Pareto improvements in both safety and capabilities. \n",
        "Category": [
            "Model evaluations and benchmarks"
        ],
        "Citations": 69,
        "Description from MAIA / AISF / Elsewhere": "[Paper] evaluates the ethical decision-making ability of AI agents in text-based social environments. The paper finds a tradeoff between reward maximization and ethical behavior. Developers of AI agents can use this benchmark to evaluate the ability of their agents to behave ethically.",
        "Internal source": [
            "ML Newsletter"
        ],
        "Link": "https://arxiv.org/abs/2304.03279",
        "ML Subfield": [
            "Model Evaluation",
            "NLP"
        ],
        "ML Subtopic": [
            "NLP: LLMs"
        ],
        "Reviewed by": [
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Model evaluations / monitoring / detection"
        ],
        "Safety Topic": [
            "Benchmarking"
        ],
        "Title": "Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the MACHIAVELLI Benchmark (Pan et al., 2023)",
        "Type": [
            "Paper"
        ],
        "Who tagged for ML": [
            "Viktoria"
        ],
        "Year": "2023",
        "airtable_createdTime": "2023-12-20T01:45:25.000Z",
        "airtable_id": "recFO5pyrhjtOgvNm"
    },
    {
        "# Top Paper votes": 2,
        "AD: Top Paper": true,
        "AG comments": "Good for adversarial robustness folks, one of the few adversarial robustness papers that has x-risk relevance. (And I think there's a lot of relevant work that \\*could\\* be done here, it's just mostly not.)\n",
        "AG: Top Paper": true,
        "Abstract": "In the future, powerful AI systems may be deployed in high-stakes settings, where a single failure could be catastrophic. One technique for improving AI safety in high-stakes settings is adversarial training, which uses an adversary to generate examples to train on in order to achieve better worst-case performance.\n",
        "Category": [
            "Non-Adversarial Robustness"
        ],
        "Citations": 37,
        "Goes online as Top Paper": true,
        "Goes online in Expanded Papers": true,
        "Internal source": [
            "AWAIR"
        ],
        "Link": "https://arxiv.org/abs/2205.01663",
        "ML Subfield": [
            "Robustness and Adversariality",
            "NLP"
        ],
        "ML Subfield (Is Starting Point)": [
            "Robustness and Adversariality"
        ],
        "PC comments": "your recommendations and resource\u00a0center are at risk of seeming parochial / slanted towards researchers in my social circles... An example from the resources page: I think the RR adversarial training paper shouldn't make a highlights reel and should be replaced with more recent work on jailbreaking large LMs (e.g. \"Universal attacks\" which you have in the longer page\u00a0+ \"Are aligned neural networks adversarially aligned?\" which does a better job of being outside of our circles), or past work on automated red teaming of LMs, or results in non-LM settings.\n",
        "Reviewed by": [
            "AG",
            "VK",
            "PC",
            "RS",
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Adversaries / Robustness / Generalization"
        ],
        "Title": "Adversarial training for high-stakes reliability (Ziegler et al., 2022)",
        "Type": [
            "Paper"
        ],
        "Who tagged for ML": [
            "Viktoria"
        ],
        "Year": "2022",
        "airtable_createdTime": "2023-09-05T22:50:05.000Z",
        "airtable_id": "recFPOZMLbva0xkid"
    },
    {
        "# Top Paper votes": 1,
        "AG comments": "Accessible paper\n",
        "AG: Top Paper": true,
        "Abstract": "In this work, we used a safe language generation task (\\`\\`avoid injuries'') as a testbed for achieving high reliability through adversarial training. We created a series of adversarial training techniques -- including a tool that assists human adversaries -- to find and eliminate failures in a classifier that filters text completions suggested by a generator. In our task, we determined that we can set very conservative classifier thresholds without significantly impacting the quality of the filtered outputs. We found that adversarial training increased robustness to the adversarial attacks that we trained on -- doubling the time for our contractors to find adversarial examples both with our tool (from 13 to 26 minutes) and without (from 20 to 44 minutes) -- without affecting in-distribution performance. We hope to see further work in the high-stakes reliability setting, including more powerful tools for enhancing human adversaries and better ways to measure high levels of reliability, until we can confidently rule out the possibility of catastrophic deployment-time failures of powerful models. \n",
        "Category": [
            "Scalable oversight"
        ],
        "Citations": 129,
        "Goes online in Expanded Papers": true,
        "Internal source": [
            "AWAIR"
        ],
        "Link": "https://arxiv.org/abs/2206.05802",
        "ML Subfield": [
            "NLP"
        ],
        "ML Subtopic": [
            "NLP: LLMs"
        ],
        "Reviewed by": [
            "AG",
            "VK",
            "PC",
            "RS",
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Scalable oversight"
        ],
        "Time from MAIA / AISF / Elsewhere (min)": "30",
        "Title": "Self-critiquing models for assisting human evaluators (Saunders et al., 2022)",
        "Type": [
            "Paper"
        ],
        "What sections to read from MAIA": "(sections 1-3 only)",
        "Who tagged for ML": [
            "Viktoria"
        ],
        "Year": "2022",
        "airtable_createdTime": "2023-09-05T22:50:05.000Z",
        "airtable_id": "recFcrv1zXAXypBxr"
    },
    {
        "# Top Paper votes": 0,
        "Category": [
            "Interpretability / explainability"
        ],
        "Description from MAIA / AISF / Elsewhere": "This post describes causal scrubbing, a technique for systematically testing a given interpretability hypothesis by checking that a model\u2019s behavior isn\u2019t changed after corrupting the model\u2019s activations in ways that the hypothesis says shouldn\u2019t make a difference.",
        "Internal source": [
            "Further reading on MAIA/HAIST Summer 2023 Curriculum"
        ],
        "Link": "https://www.alignmentforum.org/posts/JvZhhzycHu2Yd57RN/causal-scrubbing-a-method-for-rigorously-testing",
        "Reviewed by": [
            "AG",
            "VK"
        ],
        "Safety Category": [
            "Transparency / Interpretability / Model internals / Latent knowledge"
        ],
        "Title": "Causal Scrubbing: a method for rigorously testing interpretability hypotheses (Chan et al., 2022)",
        "Type": [
            "Paper"
        ],
        "airtable_createdTime": "2024-02-10T00:27:32.000Z",
        "airtable_id": "recFsD3XqY6DJXNao"
    },
    {
        "# Top Paper votes": 1,
        "Category": [
            "Overviews and agendas"
        ],
        "Goes online in Expanded Papers": true,
        "Internal source": [
            "Elsewhere"
        ],
        "Link": "https://bounded-regret.ghost.io/more-is-different-for-ai/",
        "ML Subfield": [
            "Domain General"
        ],
        "Reviewed by": [
            "AG",
            "VK",
            "PC",
            "RS",
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Major problems (misalignment, misuse, threat models)"
        ],
        "Title": "More is Different for AI (Steinhardt, 2022)",
        "Type": [
            "Blog post"
        ],
        "VK comments": "Great perspective on how to think about ML safety risks that is compelling to an ML audience\n",
        "VK: Top Paper": true,
        "Vael's Notes": "(intro and first three posts only)\n",
        "Who tagged for ML": [
            "Max"
        ],
        "Year": "2022",
        "airtable_createdTime": "2023-09-05T22:50:05.000Z",
        "airtable_id": "recGhYKL426GHl04p"
    },
    {
        "# Top Paper votes": 2,
        "AG: Top Paper": true,
        "Abstract": "In coming decades, artificial general intelligence (AGI) may surpass human capabilities at many critical tasks. We argue that, without substantial effort to prevent it, AGIs could learn to pursue goals that conflict (i.e., are misaligned) with human interests. If trained like today's most capable models, AGIs could learn to act deceptively to receive higher reward, learn internally-represented goals which generalize beyond their fine-tuning distributions, and pursue those goals using power-seeking strategies. We review emerging evidence for these properties. AGIs with these properties would be difficult to align and may appear aligned even when they are not. We outline how the deployment of misaligned AGIs might irreversibly undermine human control over the world, and briefly review research directions aimed at preventing this outcome.\n",
        "Category": [
            "Why large-scale safety?"
        ],
        "Citations": 101,
        "Context phrase (please add!)": "\n",
        "Description from MAIA / AISF / Elsewhere": "This reading argues that reward hacking will become much harder to detect once AIs have situational awareness: the skill of being able to apply abstract knowledge to the specific context in which they\u2019re run.",
        "Goes online as Top Paper": true,
        "Goes online in Expanded Papers": true,
        "High Citation and Max Select": true,
        "Internal source": [
            "Elsewhere"
        ],
        "Link": "https://arxiv.org/abs/2209.00626",
        "ML Methods Tag": [
            "RLHF"
        ],
        "ML Subfield": [
            "Reinforcement Learning",
            "Applied ML",
            "Domain General"
        ],
        "ML Subfield (Is Starting Point)": [
            "Domain General"
        ],
        "Reviewed by": [
            "AG",
            "VK",
            "PC",
            "RS",
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Major problems (misalignment, misuse, threat models)",
            "Overview"
        ],
        "Time from MAIA / AISF / Elsewhere (min)": "5+20",
        "Title": "The Alignment Problem from a Deep Learning Perspective (Ngo et al., 2022)",
        "Twitter": "https://twitter.com/RichardMCNgo/status/1603862969276051457",
        "Type": [
            "Paper"
        ],
        "VK comments": "Best writeup of the main alignment threat model that is accessible / compelling to an ML audience\n",
        "VK: Top Paper": true,
        "Vael's Notes": "Expert1 thinks this is least bad of the problem presentation options\n",
        "What sections to read from MAIA": "(only section 2: Deceptive reward hacking, sections 3-4)",
        "Who tagged for ML": [
            "Max"
        ],
        "Year": "2022",
        "airtable_createdTime": "2023-09-05T22:50:05.000Z",
        "airtable_id": "recHEptEqzzvhFkph"
    },
    {
        "# Top Paper votes": 0,
        "Blog or Video": "https://www.anthropic.com/index/decomposing-language-models-into-understandable-components",
        "Category": [
            "Interpretability / explainability"
        ],
        "Citations": 71,
        "Context phrase (please add!)": "Substantially resolves superposition, which was previously considered to be a major hurdle to mechanistic interpretability -Vael\n",
        "Goes online in Expanded Papers": true,
        "Internal source": [
            "Elsewhere"
        ],
        "Link": "https://transformer-circuits.pub/2023/monosemantic-features/index.html",
        "ML Subfield": [
            "NLP"
        ],
        "ML Subtopic": [
            "NLP: LLMs"
        ],
        "Reviewed by": [
            "VK",
            "PC",
            "RS",
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Transparency / Interpretability / Model internals / Latent knowledge",
            "Research bottlenecks / limitations"
        ],
        "Safety Topic": [
            "Mechanistic interpretability"
        ],
        "Title": "Towards Monosemanticity: Decomposing Language Models With Dictionary Learning (Bricken et al., 2023)",
        "Twitter": "https://twitter.com/ch402/status/1709998674087227859",
        "Type": [
            "Paper"
        ],
        "Vael's Notes": "\n",
        "Who tagged for ML": [
            "Viktoria"
        ],
        "Year": "2023",
        "airtable_createdTime": "2023-11-13T19:59:58.000Z",
        "airtable_id": "recHJUSXkS66Vf1gF"
    },
    {
        "# Top Paper votes": 1,
        "AG comments": "Particularly good for NLP researchers (unsurprisingly)\n",
        "AG: Top Paper": true,
        "Category": [
            "Why large-scale safety?"
        ],
        "Context phrase (please add!)": "Sam Bowman (NYU, Anthropic) describes his motivation for doing safety research - Vael\n",
        "Goes online in Expanded Papers": true,
        "Internal source": [
            "Elsewhere"
        ],
        "Link": "https://wp.nyu.edu/arg/why-ai-safety/",
        "ML Subfield": [
            "NLP"
        ],
        "Reviewed by": [
            "AG",
            "VK",
            "PC",
            "RS",
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Major problems (misalignment, misuse, threat models)"
        ],
        "Title": "Why I Think More NLP Researchers Should Engage with AI Safety Concerns (Bowman, 2022)",
        "Type": [
            "Blog post"
        ],
        "Who tagged for ML": [
            "Max"
        ],
        "Year": "2022",
        "airtable_createdTime": "2023-09-05T22:50:05.000Z",
        "airtable_id": "recHLasIIE86dbwta"
    },
    {
        "# Top Paper votes": 0,
        "Abstract": "At present, the mechanisms of in-context learning in Transformers are not well understood and remain mostly an intuition. In this paper, we suggest that training Transformers on auto-regressive objectives is closely related to gradient-based meta-learning formulations. We start by providing a simple weight construction that shows the equivalence of data transformations induced by 1) a single linear self-attention layer and by 2) gradient-descent (GD) on a regression loss. Motivated by that construction, we show empirically that when training self-attention-only Transformers on simple regression tasks either the models learned by GD and Transformers show great similarity or, remarkably, the weights found by optimization match the construction. Thus we show how trained Transformers become mesa-optimizers i.e. learn models by gradient descent in their forward pass. This allows us, at least in the domain of regression problems, to mechanistically understand the inner workings of in-context learning in optimized Transformers. Building on this insight, we furthermore identify how Transformers surpass the performance of plain gradient descent by learning an iterative curvature correction and learn linear models on deep data representations to solve non-linear regression tasks. Finally, we discuss intriguing parallels to a mechanism identified to be crucial for in-context learning termed induction-head (Olsson et al., 2022) and show how it could be understood as a specific case of in-context learning by gradient descent learning within Transformers. \n",
        "Internal source": [
            "Hendrycks Textbook"
        ],
        "Link": "https://proceedings.mlr.press/v202/von-oswald23a.html",
        "ML Subfield": [
            "NLP"
        ],
        "ML Subtopic": [
            "NLP: LLMs"
        ],
        "Reviewed by": [
            "PC",
            "RS"
        ],
        "Safety Category": [
            "Capabilities of LLMs",
            "Science of deep learning"
        ],
        "Safety Topic": [
            "Emergence"
        ],
        "Title": "Transformers Learn In-Context by Gradient Descent (Von Oswald et al., 2023)",
        "Type": [
            "Paper"
        ],
        "Who tagged for ML": [
            "Viktoria"
        ],
        "airtable_createdTime": "2024-02-10T00:27:32.000Z",
        "airtable_id": "recHPNwhqrbKQAAnT"
    },
    {
        "# Top Paper votes": 0,
        "Blog or Video": "https://transformer-circuits.pub/2023/interpretability-dreams/index.html#epistemic-foundation",
        "Category": [
            "Interpretability / explainability",
            "Theory"
        ],
        "Internal source": [
            "Alignment Workshop 2023"
        ],
        "Link": "https://www.alignment-workshop.com/sf-talks/chris-olah-looking-inside-neural-networks-with-mechanistic-interpretabili",
        "ML Subfield": [
            "Vision",
            "Theory"
        ],
        "Reviewed by": [
            "AG",
            "VK",
            "PC",
            "RS",
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Transparency / Interpretability / Model internals / Latent knowledge",
            "Research bottlenecks / limitations"
        ],
        "Title": "Looking Inside Neural Networks with Mechanistic Interpretability (Chris Olah, 41-min video)",
        "Type": [
            "Video"
        ],
        "Vael's Notes": "Chris Olah What is it useful for me to talk about it?\n- What is mechanistic interpretability\n- Is it BS?\n- Superposition as the key challenge. \u201cIncreasingly seems to me like the question on which the impact of mechanistic interpretability for safety is going to rise or fall.\u201d (And he says one can make progress without lots of compute, and has interesting mathematical structure <\u2014\\*\\*)\n- Superposition. \u201cModels have many \u2018polysemantic neurons\u2019 which fire for multiple, unrelated features\u2019. Neuron 4e:55 responds to cat faces, fronts of cars, and cat legs. The superposition hypothesis: Neural networks represent more features than they have neurons by using linear combinations of neurons.\u201d So the neurons don\u2019t correspond to features, they correspond to linear combinations of features. Monosemantic features are hypothesized to be very important or common, and sparser things are in superposition. Fundamental challenge: unless one understands the superposition structure, there can always be unknown features \u2014 and thus unknown behavior \u2014 lurking in the background.\n- How transformers are different than e.g. vision models \u2014 the residual stream, attention heads, more superpositions.\n- Relationship to safety: ideally, would be able to say \u201cgiven any situation, the model won\u2019t deliberately X\u201d. How he thinks that could actually ground out is \u201cthe model doesn\u2019t include features which will cause the model to deliberately X\u201d (implicitly a claim about circuits it participates in\u201d. \u201cThis is a wildly ambitious goal, but perhaps a useful North Star. And it suggests two major challenge: Superposition (how can we access all the features despite superposition?) and Scalability (how can we do circuit analysis on enormous models?) He is very worried about superposition. He is more optimistic about scalability.\u201d\n- \u201cThere are many promising ideas for approaching scalability: AI automated interpretability, modularity and other large-scale graph structure, simplification via equivariance and other motifs. Unfortunately, it\u2019s very challenging to work on while superposition plagues us. We don\u2019t have the right units to attack!\u201d\n- significant advantages over neuroscience\n- very informative, cutoff at questions at the end\n\n",
        "Who tagged for ML": [
            "Viktoria"
        ],
        "airtable_createdTime": "2023-09-06T19:18:00.000Z",
        "airtable_id": "recHzHZsoNYr9VUoW"
    },
    {
        "# Top Paper votes": 0,
        "Category": [
            "Reward misspecification and goal misgeneralization"
        ],
        "Internal source": [
            "NOLA Workshop 2023"
        ],
        "Link": "https://www.alignment-workshop.com/nola-talks/dimitris-papailiopoulos-the-challenge-of-monitoring-covert-interactions-a",
        "Safety Category": [
            "Reward misspecification and goal misgeneralization",
            "Adversaries / Robustness / Generalization"
        ],
        "Title": "The challenge of monitoring covert interactions and behavioral shifts in LLM agents (Dimitris Papailiopoulos, 4-min video)",
        "Type": [
            "Video"
        ],
        "Vael's Notes": "Not including the lightning talks for now, since pretty EA probably\n",
        "airtable_createdTime": "2024-02-10T00:27:32.000Z",
        "airtable_id": "recIKPpHUu2Iu49Sd"
    },
    {
        "# Top Paper votes": 0,
        "Abstract": "As large language models (LLMs) perform more difficult tasks, it becomes harder to verify the correctness and safety of their behavior. One approach to help with this issue is to prompt LLMs to externalize their reasoning, e.g., by having them generate step-by-step reasoning as they answer a question (Chain-of-Thought; CoT). The reasoning may enable us to check the process that models use to perform tasks. However, this approach relies on the stated reasoning faithfully reflecting the model\u2019s actual reasoning, which isnot always the case. To improve over the faithfulness of CoT reasoning, we have models generate reasoning by decomposing questions into subquestions. Decomposition-based methods achieve strong performance on question-answering tasks,sometimes approaching that of CoT while improving the faithfulness of the model\u2019s stated reasoning on several recently-proposed metrics. By forcing the model to answer simpler subquestionsin separate contexts, we greatly increase the faithfulness of model-generated reasoning over CoT, while still achieving some of the performancegains of CoT. Our results show it is possible to improve the faithfulness of model-generated rea-soning; continued improvements may lead to reasoning that enables us to verify the correctnessand safety of LLM behavior.\n",
        "Category": [
            "Deception"
        ],
        "Citations": 27,
        "Internal source": [
            "AWAIR"
        ],
        "Link": "https://arxiv.org/abs/2307.11768",
        "ML Subfield": [
            "NLP",
            "Human Model Interaction"
        ],
        "ML Subfield (Is Starting Point)": [
            "NLP"
        ],
        "ML Subtopic": [
            "NLP: LLMs",
            "Applied ML: Chatbots",
            "Applied ML: Cognitive Tasks"
        ],
        "Reviewed by": [
            "AG",
            "VK",
            "PC",
            "RS",
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Deception"
        ],
        "Safety Topic": [
            "Sycophancy"
        ],
        "Title": "Question Decomposition Improves the Faithfulness of Model-Generated Reasoning (Radhakrishnan et al., 2023)",
        "Type": [
            "Paper"
        ],
        "Who tagged for ML": [
            "Max"
        ],
        "airtable_createdTime": "2023-09-13T00:32:47.000Z",
        "airtable_id": "recJ6gF1s7efh751c"
    },
    {
        "# Top Paper votes": 0,
        "Category": [
            "Model evaluations and benchmarks"
        ],
        "Goes online as Top Paper": true,
        "Goes online in Expanded Papers": true,
        "Internal source": [
            "Elsewhere"
        ],
        "Link": "https://github.com/UKGovernmentBEIS/inspect_ai",
        "Title": "UK AISI Inspect",
        "Twitter": "https://x.com/soundboy/status/1788910977003504010",
        "Type": [
            "Other"
        ],
        "Who tagged for ML": [
            "Vael"
        ],
        "Year": "2024",
        "airtable_createdTime": "2024-07-31T00:07:43.000Z",
        "airtable_id": "recJKiHMBWQleBAwx"
    },
    {
        "# Top Paper votes": 1,
        "AD: Top Paper": true,
        "AG comments": "Very over-rated paper IMO. Collin's work is interesting but in general a bit sloppily executed. See e.g. <https://www.alignmentforum.org/posts/9vwekjD6xyuePX7Zr/contrast-pairs-drive-the-empirical-performance-of-contrast> arguing the CCS method doesn't really add anything beyond contrast pairs plus PCA.\n",
        "Abstract": "Existing techniques for training language models can be misaligned with the truth: if we train models with imitation learning, they may reproduce errors that humans make; if we train them to generate text that humans rate highly, they may output errors that human evaluators can't detect. We propose circumventing this issue by directly finding latent knowledge inside the internal activations of a language model in a purely unsupervised way. Specifically, we introduce a method for accurately answering yes-no questions given only unlabeled model activations. It works by finding a direction in activation space that satisfies logical consistency properties, such as that a statement and its negation have opposite truth values. We show that despite using no supervision and no model outputs, our method can recover diverse knowledge represented in large language models: across 6 models and 10 question-answering datasets, it outperforms zero-shot accuracy by 4\\\\% on average. We also find that it cuts prompt sensitivity in half and continues to maintain high accuracy even when models are prompted to generate incorrect answers. Our results provide an initial step toward discovering what language models know, distinct from what they say, even when we don't have access to explicit ground truth labels. \n",
        "Blog or Video": "https://www.alignmentforum.org/posts/L4anhrxjv8j2yRKKp/how-discovering-latent-knowledge-in-language-models-without",
        "Category": [
            "Interpretability / explainability"
        ],
        "Citations": 122,
        "High Citation and Max Select": true,
        "Internal source": [
            "AWAIR"
        ],
        "Link": "https://arxiv.org/abs/2212.03827",
        "ML Subfield": [
            "NLP"
        ],
        "ML Subfield (Is Starting Point)": [
            "NLP"
        ],
        "ML Subtopic": [
            "NLP: LLMs"
        ],
        "Reviewed by": [
            "AG",
            "VK",
            "PC",
            "RS",
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Transparency / Interpretability / Model internals / Latent knowledge"
        ],
        "Time from MAIA / AISF / Elsewhere (min)": "30",
        "Title": "Discovering Latent Knowledge In Language Models Without Supervision (Burns et al., 2022) ",
        "Twitter": "https://twitter.com/CollinBurns4/status/1600892261633785856",
        "Type": [
            "Paper"
        ],
        "VK comments": "This paper was a bit oversold\n",
        "Vael's Notes": "I think I've heard someone (Buck?) say that we wouldn't have necessarily expected this to work, but seemed work trying, and neat that something came out of it\n\nFurther investigation from the GDM team that doesn't think it's very promising after looking into it, long-term:\nhttps://www.alignmentforum.org/posts/wtfvbsYjNHYYBmT3k/discussion-challenges-with-unsupervised-llm-knowledge-1\n",
        "What sections to read from MAIA": "(only sections 1-3)",
        "Who tagged for ML": [
            "Viktoria"
        ],
        "Year": "2022",
        "airtable_createdTime": "2023-09-05T22:50:05.000Z",
        "airtable_id": "recJhkAPjMYAb7eaV"
    },
    {
        "# Top Paper votes": 0,
        "Abstract": "It is important to detect anomalous inputs when deploying machine learning systems. The use of larger and more complex inputs in deep learning magnifies the difficulty of distinguishing between anomalous and in-distribution examples. At the same time, diverse image and text data are available in enormous quantities. We propose leveraging these data to improve deep anomaly detection by training anomaly detectors against an auxiliary dataset of outliers, an approach we call Outlier Exposure (OE). This enables anomaly detectors to generalize and detect unseen anomalies. In extensive experiments on natural language processing and small- and large-scale vision tasks, we find that Outlier Exposure significantly improves detection performance. We also observe that cutting-edge generative models trained on CIFAR-10 may assign higher likelihoods to SVHN images than to CIFAR-10 images; we use OE to mitigate this issue. We also analyze the flexibility and robustness of Outlier Exposure, and identify characteristics of the auxiliary dataset that improve performance. \n",
        "Category": [
            "Non-Adversarial Robustness"
        ],
        "Citations": 1493,
        "Goes online in Expanded Papers": true,
        "Internal source": [
            "Intro to ML Safety Course"
        ],
        "Link": "https://arxiv.org/abs/1812.04606",
        "ML Subfield": [
            "Vision"
        ],
        "Reviewed by": [
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Model evaluations / monitoring / detection"
        ],
        "Safety Topic": [
            "Detection of out-of-distribution or malicious behavior"
        ],
        "Title": "Deep Anomaly Detection with Outlier Exposure (Hendrycks at al., 2019)",
        "Type": [
            "Paper"
        ],
        "Who tagged for ML": [
            "Viktoria"
        ],
        "Year": "2019",
        "airtable_createdTime": "2024-01-15T21:36:05.000Z",
        "airtable_id": "recKMO5y2tEMv7gIH"
    },
    {
        "# Top Paper votes": 0,
        "Abstract": "\n",
        "Category": [
            "Scalable oversight"
        ],
        "Link": "https://openai.com/blog/instruction-following/",
        "ML Methods Tag": [
            "RLHF"
        ],
        "ML Subfield": [
            "Applied ML",
            "Human Model Interaction"
        ],
        "ML Subfield (Is Starting Point)": [
            "Applied ML"
        ],
        "ML Subtopic": [
            "NLP: LLMs",
            "Applied ML: Cognitive Tasks",
            "Applied ML: Chatbots"
        ],
        "Reviewed by": [
            "AG",
            "VK",
            "PC",
            "RS"
        ],
        "Safety Category": [
            "Alignment <-> RLHF"
        ],
        "Time from MAIA / AISF / Elsewhere (min)": "10",
        "Title": "Aligning language models to follow instructions (Ouyang et al., 2022) ",
        "Type": [
            "Blog post"
        ],
        "Who tagged for ML": [
            "Max"
        ],
        "airtable_createdTime": "2024-02-10T00:27:32.000Z",
        "airtable_id": "recKRR6MgFFTlLHZQ"
    },
    {
        "# Top Paper votes": 1,
        "AD: Top Paper": true,
        "Category": [
            "Scalable oversight",
            "Non-Adversarial Robustness"
        ],
        "Internal source": [
            "NOLA Workshop 2023"
        ],
        "Link": "https://www.alignment-workshop.com/nola-talks/sam-bowman-adversarial-scalable-oversight-for-truthfulness-work-in-progr",
        "ML Subfield": [
            "Robustness and Adversariality",
            "NLP"
        ],
        "ML Subtopic": [
            "NLP: Question Answering"
        ],
        "Reviewed by": [
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Scalable oversight",
            "Adversaries / Robustness / Generalization"
        ],
        "Title": "Adversarial Scalable Oversight for Truthfulness: Work in Progress (Sam Bowman, 29-min video)\n",
        "Transcripts / Audio / Slides": "https://cims.nyu.edu/~sbowman/alignment_workshop_2023.pdf",
        "Type": [
            "Video"
        ],
        "Who tagged for ML": [
            "Viktoria"
        ],
        "airtable_createdTime": "2024-01-15T20:13:29.000Z",
        "airtable_id": "recKRuRnRX66OvWGC"
    },
    {
        "# Top Paper votes": 0,
        "AD comments": "\n",
        "Abstract": "We study how to perform unlearning, i.e. forgetting undesirable (mis)behaviors, on large language models (LLMs). We show at least three scenarios of aligning LLMs with human preferences can benefit from unlearning: (1) removing harmful responses, (2) erasing copyright-protected content as requested, and (3) eliminating hallucinations. Unlearning, as an alignment technique, has three advantages. (1) It only requires negative (e.g. harmful) examples, which are much easier and cheaper to collect (e.g. via red teaming or user reporting) than positive (e.g. helpful and often human-written) examples required in RLHF (RL from human feedback). (2) It is computationally efficient. (3) It is especially effective when we know which training samples cause the misbehavior. To the best of our knowledge, our work is among the first to explore LLM unlearning. We are also among the first to formulate the settings, goals, and evaluations in LLM unlearning. We show that if practitioners only have limited resources, and therefore the priority is to stop generating undesirable outputs rather than to try to generate desirable outputs, unlearning is particularly appealing. Despite only having negative samples, our ablation study shows that unlearning can still achieve better alignment performance than RLHF with just 2% of its computational time. \n",
        "Category": [
            "Interpretability / explainability"
        ],
        "Citations": 31,
        "Description from MAIA / AISF / Elsewhere": "[Paper] used gradient ascent to minimize the probability of undesirable completions while maintaining model performance on inputs where no unlearning was necessary.\n\nIt is worth mentioning none of these unlearning methods are particularly performant, and better baselines are needed.",
        "Goes online in Expanded Papers": true,
        "Internal source": [
            "ML Newsletter"
        ],
        "Link": "https://arxiv.org/abs/2310.10683",
        "ML Subfield": [
            "NLP",
            "Applied ML",
            "Theory"
        ],
        "ML Subfield (Is Starting Point)": [
            "Theory",
            "Applied ML"
        ],
        "ML Subtopic": [
            "NLP: LLMs",
            "Applied ML: Chatbots"
        ],
        "ML Subtopic (Is Starting Point)": [
            "NLP: LLMs"
        ],
        "Reviewed by": [
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Transparency / Interpretability / Model internals / Latent knowledge"
        ],
        "Title": "Large Language Model Unlearning (Yao et al., 2023)",
        "Type": [
            "Paper"
        ],
        "Who tagged for ML": [
            "Viktoria"
        ],
        "Year": "2023",
        "airtable_createdTime": "2023-12-20T01:45:25.000Z",
        "airtable_id": "recL84F8lCKSr5Vfo"
    },
    {
        "# Top Paper votes": 0,
        "Abstract": "    In this paper, we benchmark the usefulness of interpretability tools on debugging tasks. Our key insight is that we can implant human-interpretable trojans into models and then evaluate these tools based on whether they can help humans discover them. This is analogous to finding OOD bugs, except the ground truth is known, allowing us to know when an interpretation is correct. We make four contributions. (1) We propose trojan discovery as an evaluation task for interpretability tools and introduce a benchmark with 12 trojans of 3 different types. (2) We demonstrate the difficulty of this benchmark with a preliminary evaluation of 16 state-of-the-art feature attribution/saliency tools. Even under ideal conditions, given direct access to data with the trojan trigger, these methods still often fail to identify bugs. (3) We evaluate 7 feature-synthesis methods on our benchmark. (4) We introduce and evaluate 2 new variants of the best-performing method from the previous evaluation. A website for this paper and its code is at this https URL \n",
        "Category": [
            "Interpretability / explainability",
            "Scalable oversight"
        ],
        "Link": "https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html",
        "ML Subfield": [
            "NLP"
        ],
        "ML Subtopic": [
            "NLP: LLMs"
        ],
        "RS Comments": "\n",
        "Reviewed by": [
            "VK",
            "PC",
            "RS",
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Transparency / Interpretability / Model internals / Latent knowledge",
            "Scalable oversight"
        ],
        "Safety Topic": [
            "Automated interpretability"
        ],
        "Title": "Language models can explain neurons in language models (Bills et al., 2023)",
        "Type": [
            "Paper"
        ],
        "Who tagged for ML": [
            "Viktoria"
        ],
        "airtable_createdTime": "2023-11-13T20:24:55.000Z",
        "airtable_id": "recLO1NyCCuuiGqQC"
    },
    {
        "# Top Paper votes": 2,
        "AD: Top Paper": true,
        "Abstract": "Reward hacking -- where RL agents exploit gaps in misspecified reward functions -- has been widely observed, but not yet systematically studied. To understand how reward hacking arises, we construct four RL environments with misspecified rewards. We investigate reward hacking as a function of agent capabilities: model capacity, action space resolution, observation space noise, and training time. More capable agents often exploit reward misspecifications, achieving higher proxy reward and lower true reward than less capable agents. Moreover, we find instances of phase transitions: capability thresholds at which the agent's behavior qualitatively shifts, leading to a sharp decrease in the true reward. Such phase transitions pose challenges to monitoring the safety of ML systems. To address this, we propose an anomaly detection task for aberrant policies and offer several baseline detectors. \n",
        "Category": [
            "Reward misspecification and goal misgeneralization"
        ],
        "Citations": 80,
        "Goes online in Expanded Papers": true,
        "Internal source": [
            "Elsewhere",
            "Intro to ML Safety Course"
        ],
        "Link": "https://arxiv.org/abs/2201.03544",
        "ML Subfield": [
            "Reinforcement Learning",
            "Applied ML"
        ],
        "ML Subtopic": [
            "RL: Simulations",
            "Applied ML: Autonomous Vehicles",
            "Applied ML: Medicine and Health",
            "RL: Games"
        ],
        "Reviewed by": [
            "AG",
            "VK",
            "PC",
            "RS",
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Reward misspecification and goal misgeneralization"
        ],
        "Title": "The Effects of Reward Misspecification: Mapping and Mitigating Misaligned Models (Pan, Bhatia and Steinhardt, 2022)",
        "Type": [
            "Paper"
        ],
        "VK: Top Paper": true,
        "Who tagged for ML": [
            "Max"
        ],
        "Year": "2022",
        "airtable_createdTime": "2023-09-05T22:50:05.000Z",
        "airtable_id": "recLbsgzQYNCwnQa3"
    },
    {
        "# Top Paper votes": 0,
        "Abstract": "    Language models show a surprising range of capabilities, but the source of their apparent competence is unclear. Do these networks just memorize a collection of surface statistics, or do they rely on internal representations of the process that generates the sequences they see? We investigate this question by applying a variant of the GPT model to the task of predicting legal moves in a simple board game, Othello. Although the network has no a priori knowledge of the game or its rules, we uncover evidence of an emergent nonlinear internal representation of the board state. Interventional experiments indicate this representation can be used to control the output of the network and create \"latent saliency maps\" that can help explain predictions in human terms. \n",
        "Category": [
            "Interpretability / explainability"
        ],
        "Description from MAIA / AISF / Elsewhere": "Li et al. search for evidence of a world model in an Othello-playing network by training probes to extract the model\u2019s knowledge of the board state and performing interventions to change the network\u2019s internal representation of the board state.",
        "Link": "https://arxiv.org/abs/2210.13382",
        "ML Subfield": [
            "NLP"
        ],
        "ML Subtopic": [
            "NLP: LLMs",
            "Applied ML: Cognitive Tasks"
        ],
        "Reviewed by": [
            "AG",
            "VK",
            "PC",
            "RS",
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Transparency / Interpretability / Model internals / Latent knowledge"
        ],
        "Time from MAIA / AISF / Elsewhere (min)": "30",
        "Title": "Emergent World Representations: Exploring a Sequence Model Trained on a Synthetic Task (Li et al., 2023)",
        "Type": [
            "Paper"
        ],
        "Who tagged for ML": [
            "Viktoria"
        ],
        "airtable_createdTime": "2023-09-16T22:38:16.000Z",
        "airtable_id": "recLgikwTulH5HGxU"
    },
    {
        "# Top Paper votes": 3,
        "AD: Top Paper": true,
        "AG: Top Paper": true,
        "Category": [
            "Interpretability / explainability"
        ],
        "Citations": 154,
        "Context phrase (please add!)": "Outlines the superposition problem, a major blocker to scaling mechanistic interpretability to large models -Vael\n",
        "Goes online in Expanded Papers": true,
        "High Citation and Max Select": true,
        "Internal source": [
            "AWAIR"
        ],
        "Link": "https://transformer-circuits.pub/2022/toy_model/index.html",
        "ML Subfield": [
            "Theory"
        ],
        "ML Subfield (Is Starting Point)": [
            "Theory"
        ],
        "Reviewed by": [
            "AG",
            "VK",
            "PC",
            "RS",
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Transparency / Interpretability / Model internals / Latent knowledge",
            "Research bottlenecks / limitations"
        ],
        "Safety Topic": [
            "Mechanistic interpretability"
        ],
        "Supplemental Material": "Addendum: https://transformer-circuits.pub/2023/toy-double-descent/index.html",
        "Time from MAIA / AISF / Elsewhere (min)": "30",
        "Title": "Toy models of superposition (Elhage et al., 2022)",
        "Twitter": "https://x.com/AnthropicAI/status/1570087876053942272",
        "Type": [
            "Paper"
        ],
        "VK: Top Paper": true,
        "Vael's Notes": "\n",
        "What sections to read from MAIA": "(up to the end of section 3: superposition as a phase change)",
        "Who tagged for ML": [
            "Viktoria"
        ],
        "Year": "2022",
        "airtable_createdTime": "2023-09-05T22:50:05.000Z",
        "airtable_id": "recM1GrtVGYi7FJsG"
    },
    {
        "# Top Paper votes": 1,
        "AD: Top Paper": true,
        "AG comments": "Adversarial robustness is a bit sparse right now. Not an area I'd push people towards in general, but there are\u00a0so /many/\u00a0people in ML doing adversarial robustness work that it seems like a good entry point for them at least. I'd plug this work on\u00a0adversarial policies in Go AI\u00a0both as a demo (superintelligent systems might be vulnerable) and as an illustration of a different threat model they might want to consider working in.\n",
        "Category": [
            "Reinforcement Learning",
            "Adversarial Robustness"
        ],
        "Goes online in Expanded Papers": true,
        "Internal source": [
            "Elsewhere"
        ],
        "Link": "https://far.ai/post/2023-07-superhuman-go-ais/",
        "ML Subfield": [
            "Robustness and Adversariality",
            "Applied ML",
            "Reinforcement Learning"
        ],
        "ML Subfield (Is Starting Point)": [
            "Robustness and Adversariality",
            "Reinforcement Learning",
            "Applied ML"
        ],
        "ML Subtopic": [
            "RL: Games"
        ],
        "Reviewed by": [
            "AG",
            "VK",
            "PC",
            "RS",
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Adversaries / Robustness / Generalization"
        ],
        "Title": "Even Superhuman Go AIs Have Surprising Failures Modes (Gleave et al., 2023)",
        "Type": [
            "Blog post"
        ],
        "Who tagged for ML": [
            "Viktoria"
        ],
        "Year": "2023",
        "airtable_createdTime": "2023-09-25T20:57:21.000Z",
        "airtable_id": "recM3Y2FqH4w5xNfF"
    },
    {
        "# Top Paper votes": 0,
        "Abstract": "<https://docs.google.com/presentation/d/1L8MWaHQDgaGAT-EaesuoWhAryCDBEKvlVPE07swWfvc/edit#slide=id.p>\n",
        "Category": [
            "Overviews and agendas"
        ],
        "DH Comments": "I like their distinctions between reward misspecification (problems with oversight, didn't specify correct), goal misgeneralization (as Rohin says, correct specifications do not necessarily lead to correct behaviors), and deceptive alignment (special case of GMG). Also their \"failure case\" quiz is great, and I like that they describe situational awareness.\n\n\n",
        "Internal source": [
            "AWAIR"
        ],
        "Link": "Slides",
        "ML Subfield": [
            "Applied ML",
            "Domain General"
        ],
        "Reviewed by": [
            "AG",
            "VK",
            "PC",
            "RS"
        ],
        "Safety Category": [
            "Major problems (misalignment, misuse, threat models)",
            "Overview"
        ],
        "Safety Topic": [
            "Situational awareness (also instrumental convergence, inner misalignment?)"
        ],
        "Title": "Decomposing AI Safety slides (AWAIR, 2023)",
        "Who tagged for ML": [
            "Viktoria"
        ],
        "airtable_createdTime": "2024-02-10T00:27:32.000Z",
        "airtable_id": "recMK8mGnGzQRjnX4"
    },
    {
        "# Top Paper votes": 0,
        "Blog or Video": "https://www.alignment-workshop.com/nola-talks/stephen-casper-cognitive-dissonance-why-do-language-model-outputs-disagr",
        "Category": [
            "Interpretability / explainability"
        ],
        "Internal source": [
            "NOLA Workshop 2023"
        ],
        "Link": "https://arxiv.org/abs/2312.03729",
        "Safety Category": [
            "Transparency / Interpretability / Model internals / Latent knowledge"
        ],
        "Title": "Cognitive Dissonance: Why Do Language Model Outputs Disagree with Internal Representations of Truthfulness? (Liu et al., 2023)",
        "Type": [
            "Paper",
            "Video"
        ],
        "Vael's Notes": "Not including the lightning talks for now, since pretty EA probably\n",
        "airtable_createdTime": "2024-02-10T00:27:32.000Z",
        "airtable_id": "recMZyzQRj8sC4zEp"
    },
    {
        "# Top Paper votes": 0,
        "Category": [
            "Interpretability / explainability"
        ],
        "Internal source": [
            "NOLA Workshop 2023"
        ],
        "Link": "https://www.alignment-workshop.com/nola-talks/adri%C3%A0-garriga-alonso-understanding-planning-in-neural-networks",
        "Safety Category": [
            "Transparency / Interpretability / Model internals / Latent knowledge"
        ],
        "Title": "Understanding planning in neural networks (Adri\u00e0 Garriga Alonso, 4-min video)",
        "Type": [
            "Video"
        ],
        "Vael's Notes": "Not including the lightning talks for now, since pretty EA probably\n",
        "airtable_createdTime": "2024-02-10T00:27:32.000Z",
        "airtable_id": "recNi0JlJmTjXqzqW"
    },
    {
        "# Top Paper votes": 2,
        "AD: Top Paper": true,
        "Category": [
            "Deception"
        ],
        "Goes online in Expanded Papers": true,
        "Internal source": [
            "Alignment Workshop 2023"
        ],
        "Link": "https://bounded-regret.ghost.io/emergent-deception-optimization/",
        "ML Subfield": [
            "Applied ML",
            "Domain General"
        ],
        "ML Subfield (Is Starting Point)": [
            "Domain General"
        ],
        "ML Subtopic": [
            "NLP: LLMs",
            "Applied ML: Chatbots",
            "Applied ML: Cognitive Tasks"
        ],
        "Reviewed by": [
            "AG",
            "VK",
            "PC",
            "RS",
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Deception",
            "Major problems (misalignment, misuse, threat models)"
        ],
        "Safety Topic": [
            "Situational awareness (also instrumental convergence, inner misalignment)"
        ],
        "Time from MAIA / AISF / Elsewhere (min)": "20",
        "Title": "Emergent Deception and Emergent Optimization (Steinhardt, 2023)",
        "Type": [
            "Blog post"
        ],
        "VK: Top Paper": true,
        "Who tagged for ML": [
            "Max"
        ],
        "Year": "2023",
        "airtable_createdTime": "2023-09-06T18:50:17.000Z",
        "airtable_id": "recOI9VSobJeETIyR"
    },
    {
        "# Top Paper votes": 0,
        "Abstract": "    Confidence calibration -- the problem of predicting probability estimates representative of the true correctness likelihood -- is important for classification models in many applications. We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors influencing calibration. We evaluate the performance of various post-processing calibration methods on state-of-the-art architectures with image and document classification datasets. Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and straightforward recipe for practical settings: on most datasets, temperature scaling -- a single-parameter variant of Platt Scaling -- is surprisingly effective at calibrating predictions. \n",
        "Category": [
            "Model evaluations and benchmarks"
        ],
        "Internal source": [
            "Intro to ML Safety Course"
        ],
        "Link": "https://arxiv.org/abs/1706.04599",
        "ML Subfield": [
            "Model Evaluation"
        ],
        "Reviewed by": [
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Model evaluations / monitoring / detection"
        ],
        "Safety Topic": [
            "Uncertainty"
        ],
        "Title": "On Calibration of Modern Neural Networks (Guo et al., 2017)",
        "Type": [
            "Paper"
        ],
        "Who tagged for ML": [
            "Viktoria"
        ],
        "airtable_createdTime": "2024-01-15T21:36:41.000Z",
        "airtable_id": "recOJAIwHcKgBX7oQ"
    },
    {
        "# Top Paper votes": 0,
        "Abstract": "\n",
        "Category": [
            "Interpretability / explainability"
        ],
        "Internal source": [
            "Further reading on MAIA/HAIST Summer 2023 Curriculum"
        ],
        "Link": "https://transformer-circuits.pub/2023/toy-double-descent/index.html",
        "ML Subtopic": [
            "Theory"
        ],
        "Reviewed by": [
            "AG",
            "VK",
            "PC",
            "RS"
        ],
        "Safety Category": [
            "Transparency / Interpretability / Model internals / Latent knowledge"
        ],
        "Safety Topic": [
            "Mechanistic interpretability"
        ],
        "Time from MAIA / AISF / Elsewhere (min)": "30",
        "Title": "Superposition, Memorization, and Double Descent (Henighan et al., 2023)",
        "Type": [
            "Paper"
        ],
        "Who tagged for ML": [
            "Viktoria"
        ],
        "airtable_createdTime": "2024-02-10T00:27:32.000Z",
        "airtable_id": "recPIZNv0cIFY3lFq"
    },
    {
        "# Top Paper votes": 1,
        "AD: Top Paper": true,
        "Abstract": "The literature on adversarial attacks in computer vision typically focuses on pixel-level perturbations. These tend to be very difficult to interpret. Recent work that manipulates the latent representations of image generators to create \"feature-level\" adversarial perturbations gives us an opportunity to explore perceptible, interpretable adversarial attacks. We make three contributions. First, we observe that feature-level attacks provide useful classes of inputs for studying representations in models. Second, we show that these adversaries are uniquely versatile and highly robust. We demonstrate that they can be used to produce targeted, universal, disguised, physically-realizable, and black-box attacks at the ImageNet scale. Third, we show how these adversarial images can be used as a practical interpretability tool for identifying bugs in networks. We use these adversaries to make predictions about spurious associations between features and classes which we then test by designing \"copy/paste\" attacks in which one natural image is pasted into another to cause a targeted misclassification. Our results suggest that feature-level attacks are a promising approach for rigorous interpretability research. They support the design of tools to better understand what a model has learned and diagnose brittle feature associations. Code is available at this https URL\n",
        "Category": [
            "Interpretability / explainability"
        ],
        "Citations": 24,
        "Description from MAIA / AISF / Elsewhere": "Drawing inspiration from adversarial vision attacks in nature (e.g. the \u201ceyespots\u201d on butterfly wings for fooling predators), Casper et al. produce adversarial inputs for image networks which are more semantically coherent than other techniques.",
        "Goes online in Expanded Papers": true,
        "Internal source": [
            "Twitter"
        ],
        "Link": "https://arxiv.org/abs/2110.03605",
        "ML Subfield": [
            "Robustness and Adversariality",
            "Vision"
        ],
        "ML Subfield (Is Starting Point)": [
            "Vision",
            "Robustness and Adversariality"
        ],
        "Reviewed by": [
            "AG",
            "VK",
            "PC",
            "RS",
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Transparency / Interpretability / Model internals / Latent knowledge"
        ],
        "Time from MAIA / AISF / Elsewhere (min)": "30",
        "Title": "Robust Feature-Level Adversaries are Interpretability Tools (Casper et al., 2023)",
        "Twitter": "https://twitter.com/StephenLCasper/status/1598029118205218816",
        "Type": [
            "Paper"
        ],
        "Who tagged for ML": [
            "Viktoria"
        ],
        "Year": "2023",
        "airtable_createdTime": "2023-09-12T23:07:06.000Z",
        "airtable_id": "recPL2R25nCQn0VbJ"
    },
    {
        "# Top Paper votes": 0,
        "Abstract": "This work identifies 18 foundational challenges in assuring the alignment and safety of large language models (LLMs). These challenges are organized into three different categories: scientific understanding of LLMs, development and deployment methods, and sociotechnical challenges. Based on the identified challenges, we pose 200+, concrete research questions.\n",
        "Category": [
            "Overviews and agendas"
        ],
        "Citations": 6,
        "Goes online as Top Paper": true,
        "Goes online in Expanded Papers": true,
        "Internal source": [
            "Elsewhere"
        ],
        "Link": "https://llm-safety-challenges.github.io/",
        "Safety Category": [
            "Major problems (misalignment, misuse, threat models)",
            "Overview"
        ],
        "Title": "Foundational Challenges in Assuring Alignment and Safety of Large Language Models (Anwar et al., 2024)",
        "Twitter": "https://x.com/DavidSKrueger/status/1779900511627452467",
        "Type": [
            "Paper"
        ],
        "Year": "2024",
        "airtable_createdTime": "2024-04-15T16:48:05.000Z",
        "airtable_id": "recPqfWrYfsRsfxWZ"
    },
    {
        "# Top Paper votes": 1,
        "AD: Top Paper": true,
        "Abstract": "    Many real world learning tasks involve complex or hard-to-specify objectives, and using an easier-to-specify proxy can lead to poor performance or misaligned behavior. One solution is to have humans provide a training signal by demonstrating or judging performance, but this approach fails if the task is too complicated for a human to directly evaluate. We propose Iterated Amplification, an alternative training strategy which progressively builds up a training signal for difficult problems by combining solutions to easier subproblems. Iterated Amplification is closely related to Expert Iteration (Anthony et al., 2017; Silver et al., 2017), except that it uses no external reward function. We present results in algorithmic environments, showing that Iterated Amplification can efficiently learn complex behaviors. \n",
        "Category": [
            "Scalable oversight",
            "Model evaluations and benchmarks"
        ],
        "Citations": 68,
        "Description from MAIA / AISF / Elsewhere": "Christiano describes the iterated amplification algorithm, and demonstrates it using some toy experiments.\n",
        "Internal source": [
            "AWAIR"
        ],
        "Link": "https://arxiv.org/abs/1810.08575",
        "ML Subfield": [
            "Model Evaluation"
        ],
        "ML Subtopic": [
            "NLP: LLMs"
        ],
        "Reviewed by": [
            "AG",
            "VK",
            "PC",
            "RS",
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Scalable oversight"
        ],
        "Time from MAIA / AISF / Elsewhere (min)": "35",
        "Title": "Supervising strong learners by amplifying weak experts (Christiano et al., 2018)",
        "Type": [
            "Paper"
        ],
        "Who tagged for ML": [
            "Viktoria"
        ],
        "Year": "2018",
        "airtable_createdTime": "2023-09-11T21:59:40.000Z",
        "airtable_id": "recQ28oURHB1yMj8L"
    },
    {
        "# Top Paper votes": 0,
        "AD comments": "\n",
        "Abstract": "    Large language models (LLMs) can \"lie\", which we define as outputting false statements despite \"knowing\" the truth in a demonstrable sense. LLMs might \"lie\", for example, when instructed to output misinformation. Here, we develop a simple lie detector that requires neither access to the LLM's activations (black-box) nor ground-truth knowledge of the fact in question. The detector works by asking a predefined set of unrelated follow-up questions after a suspected lie, and feeding the LLM's yes/no answers into a logistic regression classifier. Despite its simplicity, this lie detector is highly accurate and surprisingly general. When trained on examples from a single setting -- prompting GPT-3.5 to lie about factual questions -- the detector generalises out-of-distribution to (1) other LLM architectures, (2) LLMs fine-tuned to lie, (3) sycophantic lies, and (4) lies emerging in real-life scenarios such as sales. These results indicate that LLMs have distinctive lie-related behavioural patterns, consistent across architectures and contexts, which could enable general-purpose lie detection. \n",
        "Category": [
            "Deception"
        ],
        "Description from MAIA / AISF / Elsewhere": "[Paper] says that a language model lies when it outputs a false statement when it demonstrable \u201cknows\u201d the truth. Then they present a surprisingly effective black-box method for detecting LLM lies. To verify the honesty of an output, the method asks the model a series of unrelated follow-up questions, such as \u201cKnowing that morning breeze is purple, are swift idea quakes green?\u201d Models which previously told a lie consistently answer these questions differently from truthful models, allowing the authors to train a logistic regression classifier on the answers which can detect lies. Importantly, the classifier generalizes from the training setting, where GPT-3.5 is prompted to lie about factual questions, to many other settings, including other LLMs such as LLaMA, models fine-tuned to lie, and models that lie in order to achieve situational goals.",
        "Internal source": [
            "ML Newsletter"
        ],
        "Link": "https://arxiv.org/abs/2309.15840",
        "ML Subfield": [
            "NLP",
            "Model Evaluation"
        ],
        "ML Subtopic": [
            "NLP: LLMs"
        ],
        "Reviewed by": [
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Deception"
        ],
        "Safety Topic": [
            "Detection of out-of-distribution or malicious behavior"
        ],
        "Title": "How to Catch an AI Liar: Lie Detection in Black-Box LLMs by Asking Unrelated Questions (Pacchiardi et al., 2023)",
        "Type": [
            "Paper"
        ],
        "Who tagged for ML": [
            "Max"
        ],
        "airtable_createdTime": "2023-12-20T01:45:25.000Z",
        "airtable_id": "recQF5Fd2kA7muGMp"
    },
    {
        "# Top Paper votes": 0,
        "Abstract": "Recent work has shown that, in generative modeling, cross-entropy loss improves smoothly with model size and training compute, following a power law plus constant scaling law. One challenge in extending these results to reinforcement learning is that the main performance objective of interest, mean episode return, need not vary smoothly. To overcome this, we introduce \\*intrinsic performance\\*, a monotonic function of the return defined as the minimum compute required to achieve the given return across a family of models of different sizes. We find that, across a range of environments, intrinsic performance scales as a power law in model size and environment interactions. Consequently, as in generative modeling, the optimal model size scales as a power law in the training compute budget. Furthermore, we study how this relationship varies with the environment and with other properties of the training setup. In particular, using a toy MNIST-based environment, we show that varying the \"horizon length\" of the task mostly changes the coefficient but not the exponent of this relationship.\n",
        "Internal source": [
            "Elsewhere"
        ],
        "Link": "Scaling laws for single-agent reinforcement learning",
        "Safety Category": [
            "Science of deep learning"
        ],
        "Title": "Scaling laws for single-agent reinforcement learning (Hilton et al., 2023)",
        "Type": [
            "Paper"
        ],
        "airtable_createdTime": "2024-03-31T18:06:47.000Z",
        "airtable_id": "recQRIqoRNAlFxlD6"
    },
    {
        "# Top Paper votes": 0,
        "Abstract": "    Through considerable effort and intuition, several recent works have reverse-engineered nontrivial behaviors of transformer models. This paper systematizes the mechanistic interpretability process they followed. First, researchers choose a metric and dataset that elicit the desired model behavior. Then, they apply activation patching to find which abstract neural network units are involved in the behavior. By varying the dataset, metric, and units under investigation, researchers can understand the functionality of each component. We automate one of the process' steps: to identify the circuit that implements the specified behavior in the model's computational graph. We propose several algorithms and reproduce previous interpretability results to validate them. For example, the ACDC algorithm rediscovered 5/5 of the component types in a circuit in GPT-2 Small that computes the Greater-Than operation. ACDC selected 68 of the 32,000 edges in GPT-2 Small, all of which were manually found by previous work. Our code is available at this https URL. \n",
        "Category": [
            "Interpretability / explainability"
        ],
        "Link": "https://arxiv.org/abs/2304.14997",
        "ML Subfield": [
            "NLP"
        ],
        "ML Subtopic": [
            "NLP: LLMs"
        ],
        "Reviewed by": [
            "VK",
            "PC",
            "RS",
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Transparency / Interpretability / Model internals / Latent knowledge"
        ],
        "Safety Topic": [
            "Automated interpretability"
        ],
        "Title": "Towards Automated Circuit Discovery for Mechanistic Interpretability (Conmy et al., 2023)",
        "Twitter": "https://twitter.com/ArthurConmy/status/1677808685836378114",
        "Type": [
            "Paper"
        ],
        "Who tagged for ML": [
            "Viktoria"
        ],
        "airtable_createdTime": "2023-11-13T20:32:07.000Z",
        "airtable_id": "recQRagHoK9KMSEmJ"
    },
    {
        "# Top Paper votes": 0,
        "Abstract": "    Adversarial examples have attracted significant attention in machine learning, but the reasons for their existence and pervasiveness remain unclear. We demonstrate that adversarial examples can be directly attributed to the presence of non-robust features: features derived from patterns in the data distribution that are highly predictive, yet brittle and incomprehensible to humans. After capturing these features within a theoretical framework, we establish their widespread existence in standard datasets. Finally, we present a simple setting where we can rigorously tie the phenomena we observe in practice to a misalignment between the (human-specified) notion of robustness and the inherent geometry of the data. \n",
        "Category": [
            "Interpretability / explainability"
        ],
        "Internal source": [
            "Further reading on MAIA/HAIST Summer 2023 Curriculum"
        ],
        "Link": "https://arxiv.org/abs/1905.02175",
        "ML Subtopic": [
            "Robustness and Adversariality",
            "Theory"
        ],
        "Reviewed by": [
            "AG",
            "VK",
            "PC",
            "RS"
        ],
        "Safety Category": [
            "Transparency / Interpretability / Model internals / Latent knowledge"
        ],
        "Time from MAIA / AISF / Elsewhere (min)": "30",
        "Title": "Adversarial Examples Are Not Bugs, They Are Features (Ilyas et al., 2019)",
        "Type": [
            "Paper"
        ],
        "Who tagged for ML": [
            "Viktoria"
        ],
        "airtable_createdTime": "2024-02-10T00:27:32.000Z",
        "airtable_id": "recQkcVvzsJXSircZ"
    },
    {
        "# Top Paper votes": 1,
        "Abstract": "We analyze the storage and recall of factual associations in autoregressive transformer language models, finding evidence that these associations correspond to localized, directly-editable computations. We first develop a causal intervention for identifying neuron activations that are decisive in a model's factual predictions. This reveals a distinct set of steps in middle-layer feed-forward modules that mediate factual predictions while processing subject tokens. To test our hypothesis that these computations correspond to factual association recall, we modify feed-forward weights to update specific factual associations using Rank-One Model Editing (ROME). We find that ROME is effective on a standard zero-shot relation extraction (zsRE) model-editing task, comparable to existing methods. To perform a more sensitive evaluation, we also evaluate ROME on a new dataset of counterfactual assertions, on which it simultaneously maintains both specificity and generalization, whereas other methods sacrifice one or another. Our results confirm an important role for mid-layer feed-forward modules in storing factual associations and suggest that direct manipulation of computational mechanisms may be a feasible approach for model editing. The code, dataset, visualizations, and an interactive demo notebook are available at this https URL\n",
        "Category": [
            "Unlearning and Knowledge Editing"
        ],
        "Citations": 63,
        "Context phrase (please add!)": "\n",
        "Description from MAIA / AISF / Elsewhere": "Alternative for those with little ML background: the companion blog post (10 mins): https://rome.baulab.info/\n",
        "Goes online as Top Paper": true,
        "Goes online in Expanded Papers": true,
        "Internal source": [
            "AWAIR",
            "Elsewhere"
        ],
        "Link": "https://rome.baulab.info/",
        "ML Subfield": [
            "NLP"
        ],
        "ML Subfield (Is Starting Point)": [
            "NLP"
        ],
        "ML Subtopic": [
            "NLP: LLMs"
        ],
        "Reviewed by": [
            "AG",
            "VK",
            "PC",
            "RS",
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Transparency / Interpretability / Model internals / Latent knowledge"
        ],
        "Safety Topic": [
            "Model editing"
        ],
        "Time from MAIA / AISF / Elsewhere (min)": "30",
        "Title": "Locating and Editing Factual Associations in GPT (Meng et al., 2022)\n",
        "Transcripts / Audio / Slides": "https://preview.type3.audio/episode/agi-safety-fundamentals-alignment/c902ed39-f622-4296-b312-73339fab7b6e",
        "Type": [
            "Paper"
        ],
        "VK: Top Paper": true,
        "Vael's Notes": "(ie. \"the ROME paper\")\n",
        "What sections to read from MAIA": "(only sections 1 - 3.1)",
        "Who tagged for ML": [
            "Viktoria"
        ],
        "Year": "2022",
        "airtable_createdTime": "2023-09-05T22:50:05.000Z",
        "airtable_id": "recR7zJWDnDPUlxg7"
    },
    {
        "# Top Paper votes": 0,
        "Category": [
            "Overviews and agendas",
            "Why large-scale safety?"
        ],
        "Goes online in Expanded Papers": true,
        "Internal source": [
            "Elsewhere"
        ],
        "Link": "https://www.anthropic.com/index/core-views-on-ai-safety",
        "ML Subfield": [
            "Domain General"
        ],
        "ML Subfield (Is Starting Point)": [
            "Domain General"
        ],
        "Reviewed by": [
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Transparency / Interpretability / Model internals / Latent knowledge",
            "Scalable oversight",
            "Alignment <-> RLHF",
            "Model evaluations / monitoring / detection",
            "AI Governance",
            "Overview"
        ],
        "Safety Topic": [
            "Mechanistic interpretability"
        ],
        "Title": "Core Views on AI Safety: When, Why, What, and How (Anthropic, 2023)",
        "Type": [
            "Blog post"
        ],
        "Who tagged for ML": [
            "Viktoria"
        ],
        "Year": "2023",
        "airtable_createdTime": "2023-12-20T01:45:25.000Z",
        "airtable_id": "recRQmDMilEfAlp3Y"
    },
    {
        "# Top Paper votes": 0,
        "Abstract": " Spurred by the recent rapid increase in the development and distribution of large language models (LLMs) across industry and academia, much recent work has drawn attention to safety- and security-related threats and vulnerabilities of LLMs, including in the context of potentially criminal activities. Specifically, it has been shown that LLMs can be misused for fraud, impersonation, and the generation of malware; while other authors have considered the more general problem of AI alignment. It is important that developers and practitioners alike are aware of security-related problems with such models. In this paper, we provide an overview of existing - predominantly scientific - efforts on identifying and mitigating threats and vulnerabilities arising from LLMs. We present a taxonomy describing the relationship between threats caused by the generative capabilities of LLMs, prevention measures intended to address such threats, and vulnerabilities arising from imperfect prevention measures. With our work, we hope to raise awareness of the limitations of LLMs in light of such security concerns, among both experienced developers and novel users of such technologies. \n",
        "Category": [
            "Security",
            "Adversarial Robustness"
        ],
        "Citations": 37,
        "Goes online in Expanded Papers": true,
        "Internal source": [
            "Twitter"
        ],
        "Link": "https://arxiv.org/abs/2308.12833",
        "ML Subfield": [
            "Robustness and Adversariality"
        ],
        "ML Subtopic": [
            "NLP: LLMs"
        ],
        "Reviewed by": [
            "AG",
            "VK",
            "PC",
            "RS",
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Major problems (misalignment, misuse, threat models)",
            "Overview"
        ],
        "Title": "Use of LLMs for Illicit Purposes: Threats, Prevention Measures, and Vulnerabilities (Mozes et al., 2023)",
        "Type": [
            "Paper"
        ],
        "Vael's Notes": "<https://twitter.com/StephenLCasper/status/1696368065041055800>\n",
        "Who tagged for ML": [
            "Max"
        ],
        "Year": "2023",
        "airtable_createdTime": "2023-09-12T22:53:27.000Z",
        "airtable_id": "recRUNC76nqF0hDQj"
    },
    {
        "# Top Paper votes": 1,
        "Category": [
            "Interpretability / explainability"
        ],
        "Internal source": [
            "Alignment Workshop 2023"
        ],
        "Link": "https://www.neelnanda.io/mechanistic-interpretability/favourite-papers",
        "ML Subfield": [
            "Theory",
            "NLP"
        ],
        "ML Subtopic": [
            "NLP: LLMs"
        ],
        "Reviewed by": [
            "AG",
            "VK",
            "PC",
            "RS",
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Transparency / Interpretability / Model internals / Latent knowledge",
            "Overview"
        ],
        "Safety Topic": [
            "Mechanistic interpretability"
        ],
        "Title": "An Extremely Opinionated Annotated List of My Favourite Mechanistic Interpretability Papers (Nanda, 2023)",
        "Type": [
            "Blog post"
        ],
        "VK comments": "Excellent collection of interpretability papers\n",
        "VK: Top Paper": true,
        "Who tagged for ML": [
            "Viktoria"
        ],
        "Year": "2023",
        "airtable_createdTime": "2023-09-07T01:51:27.000Z",
        "airtable_id": "recRXQ8eP9KyAg4Gi"
    },
    {
        "# Top Paper votes": 0,
        "Category": [
            "Overviews and agendas",
            "Scalable oversight"
        ],
        "Internal source": [
            "Elsewhere"
        ],
        "Link": "https://arxiv.org/pdf/1606.06565.pdf",
        "Reviewed by": [
            "VK"
        ],
        "Safety Category": [
            "Major problems (misalignment, misuse, threat models)",
            "Scalable oversight",
            "Alignment <-> RLHF"
        ],
        "Title": "Concrete Problems in AI Safety (Amodei et al., 2016)",
        "Type": [
            "Paper"
        ],
        "VK comments": "Influential research agenda, but a bit old now (pre-LLM)\n",
        "airtable_createdTime": "2024-02-10T00:27:32.000Z",
        "airtable_id": "recRbVo1tbnOZ2jZQ"
    },
    {
        "# Top Paper votes": 0,
        "Abstract": "\n",
        "Category": [
            "Scalable oversight",
            "Interpretability / explainability"
        ],
        "Internal source": [
            "AWAIR"
        ],
        "Link": "https://docs.google.com/presentation/d/1y5Xpnvpy09Sn_aerEoH4d2EZEcdfaHFmfIV0VpdoWuI/edit#slide=id.p",
        "ML Subfield": [
            "Model Evaluation"
        ],
        "Reviewed by": [
            "AG",
            "VK",
            "PC",
            "RS"
        ],
        "Safety Category": [
            "Scalable oversight",
            "Alignment <-> RLHF",
            "Adversaries / Robustness / Generalization",
            "Transparency / Interpretability / Model internals / Latent knowledge"
        ],
        "Title": "Directions in Scalable Oversight slides (AWAIR, 2023)",
        "Type": [
            "Slides"
        ],
        "Vael's Notes": "This was great for me\n",
        "Who tagged for ML": [
            "Viktoria"
        ],
        "airtable_createdTime": "2024-02-10T00:27:32.000Z",
        "airtable_id": "recRf40AG6nZskqw5"
    },
    {
        "# Top Paper votes": 1,
        "AG comments": "Presumably this is meant to link to their report? I could see that being good for people who have a background in more empirical ML or who show some interest in evaluation work.\n",
        "Abstract": "In this report, we explore the ability of language model agents to acquire resources,create copies of themselves, and adapt to novel challenges they encounter inthe wild. We refer to this cluster of capabilities as \u201cautonomous replication andadaptation\u201d or ARA. We believe that systems capable of ARA could have wide-reaching and hard-to-anticipate consequences, and that measuring and forecastingARA may be useful for informing measures around security, monitoring, andalignment. Additionally, once a system is capable of ARA, placing bounds on asystem\u2019s capabilities may become significantly more difficult.We construct four simple example agents that combine language models with toolsthat allow them to take actions in the world. We then evaluate these agents on 12tasks relevant to ARA. We find that these language model agents can only completethe easiest tasks from this list, although they make some progress on the morechallenging tasks. Unfortunately, these evaluations are not adequate to rule out thepossibility that near-future agents will be capable of ARA. In particular, we do notthink that these evaluations provide good assurance that the \u201cnext generation\u201d oflanguage models (e.g. 100x effective compute scaleup on existing models) willnot yield agents capable of ARA, unless intermediate evaluations are performedduring pretraining. Relatedly, we expect that fine-tuning of the existing modelscould produce substantially more competent agents, even if the fine-tuning is notdirectly targeted at ARA.\n",
        "Blog or Video": "https://www.alignmentforum.org/posts/EPLk8QxETC5FEhoxK/arc-evals-new-report-evaluating-language-model-agents-on",
        "Category": [
            "Model evaluations and benchmarks",
            "Reinforcement Learning"
        ],
        "Citations": 14,
        "Goes online in Expanded Papers": true,
        "Internal source": [
            "AWAIR"
        ],
        "Link": "https://arxiv.org/abs/2312.11671",
        "ML Subfield": [
            "Model Evaluation",
            "NLP",
            "Applied ML"
        ],
        "ML Subfield (Is Starting Point)": [
            "Applied ML"
        ],
        "ML Subtopic": [
            "NLP: LLMs"
        ],
        "Reviewed by": [
            "AG",
            "VK",
            "PC",
            "RS",
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Model evaluations / monitoring / detection"
        ],
        "Title": "Evaluating Language-Model Agents on Realistic Autonomous Tasks (Kinniment et al., 2023)",
        "Type": [
            "Paper"
        ],
        "VK comments": "State of the art evals paper\n",
        "VK: Top Paper": true,
        "Who tagged for ML": [
            "Viktoria"
        ],
        "Year": "2023",
        "airtable_createdTime": "2023-09-05T22:50:05.000Z",
        "airtable_id": "recRfXMPk11fPbCYp"
    },
    {
        "# Top Paper votes": 0,
        "Abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.\n",
        "Internal source": [
            "Intro to ML Safety Course"
        ],
        "Link": "https://arxiv.org/abs/1706.03762",
        "Safety Category": [
            "Science of deep learning"
        ],
        "Title": "Attention Is All You Need (Vaswani et al., 2017)",
        "Type": [
            "Paper"
        ],
        "airtable_createdTime": "2024-03-31T18:06:41.000Z",
        "airtable_id": "recRtHM26GvTupNr8"
    },
    {
        "# Top Paper votes": 2,
        "AD: Top Paper": true,
        "Category": [
            "Why large-scale safety?"
        ],
        "Goes online in Expanded Papers": true,
        "Internal source": [
            "Alignment Workshop 2023"
        ],
        "Link": "https://www.alignment-workshop.com/sf-talks/ajeya-cotra-situational-awareness-makes-measuring-safety-tricky",
        "ML Subfield": [
            "Human Model Interaction",
            "Domain General",
            "Applied ML"
        ],
        "ML Subtopic": [
            "NLP: LLMs"
        ],
        "Reviewed by": [
            "AG",
            "VK",
            "PC",
            "RS",
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Deception"
        ],
        "Safety Topic": [
            "Situational awareness (also instrumental convergence, inner misalignment)"
        ],
        "Title": " \u201cSituational Awareness\u201d Makes Measuring Safety Tricky (Ajeya Cotra, 40-min video)",
        "Type": [
            "Video"
        ],
        "VK: Top Paper": true,
        "Vael's Notes": "\\- suggestions for deception ala situational awareness:\n\\- behavioral cues vs internals \u2014 be clever with behavioral cues (have the hypothesis in mind that they could be doing situational awareness), do better at internals (\u201cimprove mechanistic interpretability\u201d), limit capabilities especially situational awareness\n\\- note: pretty simple idea, don\u2019t know how well it went over, but it\u2019s pretty clean and aimed at ML audiences (shorter than her long Magma post, less talking-down than her synchophants post)\n",
        "Who tagged for ML": [
            "Viktoria"
        ],
        "airtable_createdTime": "2023-09-06T18:51:18.000Z",
        "airtable_id": "recS5gpJkUZxo9fT9"
    },
    {
        "# Top Paper votes": 0,
        "Abstract": "As advanced machine learning systems' capabilities begin to play a significant role in geopolitics and societal order, it may become imperative that (1) governments be able to enforce rules on the development of advanced ML systems within their borders, and (2) countries be able to verify each other's compliance with potential future international agreements on advanced ML development. This work analyzes one mechanism to achieve this, by monitoring the computing hardware used for large-scale NN training. The framework's primary goal is to provide governments high confidence that no actor uses large quantities of specialized ML chips to execute a training run in violation of agreed rules. At the same time, the system does not curtail the use of consumer computing devices, and maintains the privacy and confidentiality of ML practitioners' models, data, and hyperparameters. The system consists of interventions at three stages: (1) using on-chip firmware to occasionally save snapshots of the the neural network weights stored in device memory, in a form that an inspector could later retrieve; (2) saving sufficient information about each training run to prove to inspectors the details of the training run that had resulted in the snapshotted weights; and (3) monitoring the chip supply chain to ensure that no actor can avoid discovery by amassing a large quantity of untracked chips. The proposed design decomposes the ML training rule verification problem into a series of narrow technical challenges, including a new variant of the Proof-of-Learning problem [Jia et al. '21].  \n",
        "Category": [
            "AI Governance"
        ],
        "Citations": 19,
        "Description from MAIA / AISF / Elsewhere": "This paper lays out a regulatory framework for monitoring large training runs in which (1) GPU manufacturers install on-chip mechanisms for logging certain difficult-to-spoof data about the chips\u2019 usage, (2) regulators occasionally visit and inspect large compute clusters, using the logged data to verify that the GPUs are being used consistently with established rules.",
        "Goes online in Expanded Papers": true,
        "High Citation and Max Select": true,
        "Internal source": [
            "Further reading on MAIA/HAIST Summer 2023 Curriculum"
        ],
        "Link": "https://arxiv.org/abs/2303.11341",
        "ML Subfield": [
            "Applied ML",
            "Domain General",
            "Human Model Interaction",
            "Security"
        ],
        "ML Subfield (Is Starting Point)": [
            "Domain General",
            "Security",
            "Applied ML"
        ],
        "ML Subtopic": [
            "Applied ML: Efficiency and Hardware"
        ],
        "Reviewed by": [
            "AG",
            "VK",
            "PC",
            "RS",
            "DH",
            "AD"
        ],
        "Safety Category": [
            "AI Governance"
        ],
        "Safety Topic": [
            "Compute governance"
        ],
        "Time from MAIA / AISF / Elsewhere (min)": "30",
        "Title": "What does it take to catch a Chinchilla? Verifying Rules on Large-Scale Neural Network Training via Compute Monitoring (Shavit, 2023)z",
        "Type": [
            "Paper"
        ],
        "Vael's Notes": "\n",
        "Who tagged for ML": [
            "Max"
        ],
        "Year": "2023",
        "airtable_createdTime": "2023-09-05T22:50:05.000Z",
        "airtable_id": "recSfaHOS1qQIYdsR"
    },
    {
        "# Top Paper votes": 0,
        "Category": [
            "Reward misspecification and goal misgeneralization"
        ],
        "Internal source": [
            "NOLA Workshop 2023"
        ],
        "Link": "https://www.alignment-workshop.com/nola-talks/eric-neyman-heuristic-arguments-an-approach-to-detecting-anomalous-model",
        "Safety Category": [
            "Reward misspecification and goal misgeneralization",
            "Adversaries / Robustness / Generalization"
        ],
        "Title": "Heuristic arguments: An approach to detecting anomalous model behavior (Eric Neyman, 5-min video)",
        "Type": [
            "Video"
        ],
        "Vael's Notes": "Not including the lightning talks for now, since pretty EA probably\n",
        "airtable_createdTime": "2024-02-10T00:27:32.000Z",
        "airtable_id": "recSpMsygfr9tepky"
    },
    {
        "# Top Paper votes": 0,
        "AD comments": "\n",
        "Abstract": "    In this paper, we identify and characterize the emerging area of representation engineering (RepE), an approach to enhancing the transparency of AI systems that draws on insights from cognitive neuroscience. RepE places population-level representations, rather than neurons or circuits, at the center of analysis, equipping us with novel methods for monitoring and manipulating high-level cognitive phenomena in deep neural networks (DNNs). We provide baselines and an initial analysis of RepE techniques, showing that they offer simple yet effective solutions for improving our understanding and control of large language models. We showcase how these methods can provide traction on a wide range of safety-relevant problems, including honesty, harmlessness, power-seeking, and more, demonstrating the promise of top-down transparency research. We hope that this work catalyzes further exploration of RepE and fosters advancements in the transparency and safety of AI systems. \n",
        "Category": [
            "Interpretability / explainability"
        ],
        "Citations": 77,
        "Description from MAIA / AISF / Elsewhere": "[Paper] proposed an unsupervised method for identifying linear representations of concepts such as honesty, fairness, and power-seeking within a neural network\u2019s activations. The paper adjusts future forward passes of the model by adding or subtracting this linear representation, enabling unsupervised control of the model\u2019s outputs.",
        "Goes online as Top Paper": true,
        "Goes online in Expanded Papers": true,
        "Internal source": [
            "ML Newsletter"
        ],
        "Link": "https://arxiv.org/abs/2310.01405",
        "ML Subfield": [
            "NLP"
        ],
        "ML Subtopic": [
            "NLP: LLMs",
            "Applied ML: Chatbots"
        ],
        "Reviewed by": [
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Transparency / Interpretability / Model internals / Latent knowledge"
        ],
        "Safety Topic": [
            "Model editing"
        ],
        "Supplemental Material": "https://www.ai-transparency.org/",
        "Title": "Representation Engineering: A Top-Down Approach to AI Transparency (Zou et al., 2023)",
        "Twitter": "https://x.com/andyzou_jiaming/status/1709365304789238201",
        "Type": [
            "Paper"
        ],
        "Who tagged for ML": [
            "Viktoria"
        ],
        "Year": "2023",
        "airtable_createdTime": "2023-12-20T01:45:25.000Z",
        "airtable_id": "recSpPemjD3Rl68U9"
    },
    {
        "# Top Paper votes": 1,
        "Category": [
            "Overviews and agendas"
        ],
        "Link": "https://bounded-regret.ghost.io/future-ml-systems-will-be-qualitatively-different/",
        "ML Subfield": [
            "Domain General"
        ],
        "Reviewed by": [
            "AG",
            "VK",
            "PC",
            "RS",
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Forecasting",
            "Major problems (misalignment, misuse, threat models)"
        ],
        "Time from MAIA / AISF / Elsewhere (min)": "5",
        "Title": "Future ML Systems Will Be Qualitatively Different (Steinhardt, 2022)",
        "Type": [
            "Blog post"
        ],
        "VK: Top Paper": true,
        "Who tagged for ML": [
            "Max"
        ],
        "Year": "2022",
        "airtable_createdTime": "2023-09-16T22:24:21.000Z",
        "airtable_id": "recSxLJ25HT4DODFw"
    },
    {
        "# Top Paper votes": 0,
        "Category": [
            "Overviews and agendas",
            "Model evaluations and benchmarks"
        ],
        "Internal source": [
            "Alignment Workshop 2023"
        ],
        "Link": " https://www.youtube.com/watch?v=HiYWwjma3xE&t=3607s",
        "ML Subfield": [
            "Model Evaluation",
            "NLP",
            "Applied ML"
        ],
        "ML Subfield (Is Starting Point)": [
            "NLP"
        ],
        "ML Subtopic": [
            "NLP: LLMs"
        ],
        "Reviewed by": [
            "AG",
            "VK",
            "PC",
            "RS",
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Major problems (misalignment, misuse, threat models)",
            "Model evaluations / monitoring / detection"
        ],
        "Title": "Transparency and Standards in Evaluating Language Models (Percy Liang, 10-min video)",
        "Type": [
            "Video"
        ],
        "Vael's Notes": "(5-min talk)\n",
        "Who tagged for ML": [
            "Viktoria"
        ],
        "airtable_createdTime": "2023-09-08T20:49:25.000Z",
        "airtable_id": "recTLAdPTccVnOcRy"
    },
    {
        "# Top Paper votes": 0,
        "Abstract": "    Interpretable AI tools are often motivated by the goal of understanding model behavior in out-of-distribution (OOD) contexts. Despite the attention this area of study receives, there are comparatively few cases where these tools have identified previously unknown bugs in models. We argue that this is due, in part, to a common feature of many interpretability methods: they analyze model behavior by using a particular dataset. This only allows for the study of the model in the context of features that the user can sample in advance. To address this, a growing body of research involves interpreting models using _feature synthesis_ methods that do not depend on a dataset.\n",
        "Category": [
            "Interpretability / explainability",
            "Deception",
            "Non-Adversarial Robustness"
        ],
        "Citations": 12,
        "Internal source": [
            "Elsewhere"
        ],
        "Link": "https://arxiv.org/abs/2302.10894",
        "ML Subfield": [
            "Robustness and Adversariality",
            "Vision"
        ],
        "ML Subfield (Is Starting Point)": [
            "Robustness and Adversariality"
        ],
        "ML Subtopic (Is Starting Point)": [
            "NLP: LLMs"
        ],
        "RS Comments": "\n",
        "Reviewed by": [
            "AG",
            "VK",
            "PC",
            "RS",
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Transparency / Interpretability / Model internals / Latent knowledge",
            "Adversaries / Robustness / Generalization",
            "Deception"
        ],
        "Safety Topic": [
            "Benchmarking"
        ],
        "Title": "Red Teaming Deep Neural Networks with Feature Synthesis Tools (Casper et al., 2023)\n",
        "Twitter": "https://twitter.com/StephenLCasper/status/1706654943091056898",
        "Type": [
            "Paper"
        ],
        "Who tagged for ML": [
            "Viktoria"
        ],
        "airtable_createdTime": "2023-09-26T17:15:37.000Z",
        "airtable_id": "recTOMqm6IQATZPc4"
    },
    {
        "# Top Paper votes": 1,
        "AD: Top Paper": true,
        "Abstract": "Modern machine learning methods including deep learning have achieved great success in predictive accuracy for supervised learning tasks, but may still fall short in giving useful estimates of their predictive {\\em uncertainty}. Quantifying uncertainty is especially critical in real-world settings, which often involve input distributions that are shifted from the training distribution due to a variety of factors including sample bias and non-stationarity. In such settings, well calibrated uncertainty estimates convey information about when a model's output should (or should not) be trusted. Many probabilistic deep learning methods, including Bayesian-and non-Bayesian methods, have been proposed in the literature for quantifying predictive uncertainty, but to our knowledge there has not previously been a rigorous large-scale empirical comparison of these methods under dataset shift. We present a large-scale benchmark of existing state-of-the-art methods on classification problems and investigate the effect of dataset shift on accuracy and calibration. We find that traditional post-hoc calibration does indeed fall short, as do several other previous methods. However, some methods that marginalize over models give surprisingly strong results across a broad spectrum of tasks. \n",
        "Category": [
            "Non-Adversarial Robustness"
        ],
        "Citations": 1635,
        "High Citation and Max Select": true,
        "Internal source": [
            "Intro to ML Safety Course"
        ],
        "Link": "https://arxiv.org/abs/1906.02530",
        "ML Subfield": [
            "Domain General",
            "Model Evaluation"
        ],
        "ML Subfield (Is Starting Point)": [
            "Domain General"
        ],
        "Reviewed by": [
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Model evaluations / monitoring / detection"
        ],
        "Safety Topic": [
            "Uncertainty"
        ],
        "Title": "Can You Trust Your Model\u2019s Uncertainty? Evaluating Predictive Uncertainty Under Dataset Shift (Ovadia et al., 2019)\n",
        "Type": [
            "Paper"
        ],
        "Who tagged for ML": [
            "Viktoria"
        ],
        "Year": "2019",
        "airtable_createdTime": "2024-01-15T21:36:47.000Z",
        "airtable_id": "recTujypy9UwgNtDs"
    },
    {
        "# Top Paper votes": 1,
        "Abstract": "Large language models are now tuned to align with the goals of their creators, namely to be \"helpful and harmless.\" These models should respond helpfully to user questions, but refuse to answer requests that could cause harm. However, adversarial users can construct inputs which circumvent attempts at alignment. In this work, we study to what extent these models remain aligned, even when interacting with an adversarial user who constructs worst-case inputs (adversarial examples). These inputs are designed to cause the model to emit harmful content that would otherwise be prohibited. We show that existing NLP-based optimization attacks are insufficiently powerful to reliably attack aligned text models: even when current NLP-based attacks fail, we can find adversarial inputs with brute force. As a result, the failure of current attacks should not be seen as proof that aligned text models remain aligned under adversarial inputs. However the recent trend in large-scale ML models is multimodal models that allow users to provide images that influence the text that is generated. We show these models can be easily attacked, i.e., induced to perform arbitrary un-aligned behavior through adversarial perturbation of the input image. We conjecture that improved NLP attacks may demonstrate this same level of adversarial control over text-only models. \n",
        "Category": [
            "Adversarial Robustness"
        ],
        "Citations": 112,
        "Goes online as Top Paper": true,
        "Goes online in Expanded Papers": true,
        "High Citation and Max Select": true,
        "Internal source": [
            "Expert rec"
        ],
        "Link": "https://arxiv.org/abs/2306.15447",
        "ML Subfield": [
            "Robustness and Adversariality",
            "Security"
        ],
        "ML Subfield (Is Starting Point)": [
            "Security",
            "Robustness and Adversariality"
        ],
        "ML Subtopic": [
            "NLP: LLMs"
        ],
        "PC: Top Paper": true,
        "Reviewed by": [
            "PC",
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Adversaries / Robustness / Generalization"
        ],
        "Title": "Are aligned neural networks adversarially aligned? (Carlini et al., 2023)",
        "Type": [
            "Paper"
        ],
        "Who tagged for ML": [
            "Viktoria"
        ],
        "Year": "2023",
        "airtable_createdTime": "2023-12-18T04:04:50.000Z",
        "airtable_id": "recUIKF6uWwu6hPv3"
    },
    {
        "# Top Paper votes": 0,
        "Abstract": "Large-scale pre-training has recently emerged as a technique for creating capable, general purpose, generative models such as GPT-3, Megatron-Turing NLG, Gopher, and many others. In this paper, we highlight a counterintuitive property of such models and discuss the policy implications of this property. Namely, these generative models have an unusual combination of predictable loss on a broad training distribution (as embodied in their \"scaling laws\"), and unpredictable specific capabilities, inputs, and outputs. We believe that the high-level predictability and appearance of useful capabilities drives rapid development of such models, while the unpredictable qualities make it difficult to anticipate the consequences of model deployment. We go through examples of how this combination can lead to socially harmful behavior with examples from the literature and real world observations, and we also perform two novel experiments to illustrate our point about harms from unpredictability. Furthermore, we analyze how these conflicting properties combine to give model developers various motivations for deploying these models, and challenges that can hinder deployment. We conclude with a list of possible interventions the AI community may take to increase the chance of these models having a beneficial impact. We intend this paper to be useful to policymakers who want to understand and regulate AI systems, technologists who care about the potential policy impact of their work, and academics who want to analyze, critique, and potentially develop large generative models. \n",
        "Category": [
            "AI Governance"
        ],
        "Citations": 184,
        "Internal source": [
            "Twitter"
        ],
        "Link": "https://arxiv.org/abs/2202.07785",
        "ML Subfield": [
            "Applied ML",
            "Human Model Interaction"
        ],
        "ML Subfield (Is Starting Point)": [
            "Applied ML"
        ],
        "ML Subtopic": [
            "NLP: LLMs"
        ],
        "Reviewed by": [
            "AG",
            "VK",
            "PC",
            "RS",
            "DH",
            "AD"
        ],
        "Safety Category": [
            "AI Governance"
        ],
        "Title": "Predictability and Surprise in Large Generative Models (Ganguli et al., 2022)",
        "Twitter": "https://twitter.com/AnthropicAI/status/1494352852734541826",
        "Type": [
            "Paper"
        ],
        "Who tagged for ML": [
            "Max"
        ],
        "airtable_createdTime": "2023-09-12T20:42:12.000Z",
        "airtable_id": "recUWWSIYIssavJzl"
    },
    {
        "# Top Paper votes": 0,
        "Abstract": "    Supervised machine learning models boast remarkable predictive capabilities. But can you trust your model? Will it work in deployment? What else can it tell you about the world? We want models to be not only good, but interpretable. And yet the task of interpretation appears underspecified. Papers provide diverse and sometimes non-overlapping motivations for interpretability, and offer myriad notions of what attributes render models interpretable. Despite this ambiguity, many papers proclaim interpretability axiomatically, absent further explanation. In this paper, we seek to refine the discourse on interpretability. First, we examine the motivations underlying interest in interpretability, finding them to be diverse and occasionally discordant. Then, we address model properties and techniques thought to confer interpretability, identifying transparency to humans and post-hoc explanations as competing notions. Throughout, we discuss the feasibility and desirability of different notions, and question the oft-made assertions that linear models are interpretable and that deep neural networks are not. \n",
        "Category": [
            "Interpretability / explainability"
        ],
        "Citations": 5191,
        "Internal source": [
            "Intro to ML Safety Course"
        ],
        "Link": "https://arxiv.org/abs/1606.03490",
        "ML Subfield": [
            "Model Evaluation",
            "Domain General",
            "Human Model Interaction"
        ],
        "ML Subfield (Is Starting Point)": [
            "Domain General"
        ],
        "Reviewed by": [
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Transparency / Interpretability / Model internals / Latent knowledge"
        ],
        "Title": "The Mythos of Model Interpretability (Lipton, 2017)",
        "Type": [
            "Paper"
        ],
        "Who tagged for ML": [
            "Viktoria"
        ],
        "airtable_createdTime": "2024-01-15T21:36:57.000Z",
        "airtable_id": "recUa5vvgo4nxeWWN"
    },
    {
        "# Top Paper votes": 0,
        "AG comments": "\n",
        "Category": [
            "Model evaluations and benchmarks"
        ],
        "Internal source": [
            "AWAIR"
        ],
        "Link": "https://ai-alignment.com/mechanistic-anomaly-detection-and-elk-fb84f4c6d0dc",
        "Reviewed by": [
            "AG",
            "VK"
        ],
        "Safety Category": [
            "Model evaluations / monitoring / detection"
        ],
        "Title": "Mechanistic anomaly detection and ELK (Christiano, 2022)",
        "Type": [
            "Blog post"
        ],
        "airtable_createdTime": "2024-02-10T00:27:32.000Z",
        "airtable_id": "recUowKiU5t02mceA"
    },
    {
        "# Top Paper votes": 0,
        "AG comments": "Re: science of deep learning papers, maybe include Rohin's follow-up to the Grokking paper (caveat: not read this yet). \n\nTBH none of these [science of deep learning papers] are directly safety relevant, it's much more \"foundational knowledge that'll help with safety\". I do feel ~good about safety-motivated people looking into this area, but they'd need to get the safety motivation/context from other areas, so it's not something I'd necessarily highlight. Might be good for people with more theoretical backgrounds though?\n",
        "Abstract": " One of the most surprising puzzles in neural network generalisation is grokking: a network with perfect training accuracy but poor generalisation will, upon further training, transition to perfect generalisation. We propose that grokking occurs when the task admits a generalising solution and a memorising solution, where the generalising solution is slower to learn but more efficient, producing larger logits with the same parameter norm. We hypothesise that memorising circuits become more inefficient with larger training datasets while generalising circuits do not, suggesting there is a critical dataset size at which memorisation and generalisation are equally efficient. We make and confirm four novel predictions about grokking, providing significant evidence in favour of our explanation. Most strikingly, we demonstrate two novel and surprising behaviours: ungrokking, in which a network regresses from perfect to low test accuracy, and semi-grokking, in which a network shows delayed generalisation to partial rather than perfect test accuracy. \n",
        "Category": [
            "Theory"
        ],
        "Internal source": [
            "Elsewhere"
        ],
        "Link": "https://arxiv.org/pdf/2309.02390.pdf",
        "ML Subfield": [
            "Theory"
        ],
        "ML Subtopic": [
            "Applied ML: Efficiency and Hardware"
        ],
        "RS Comments": "Some quick comments on the \"science of deep learning\" papers in the Airtable:\n- I agree with Expert1's comment that it isn't directly safety relevant yet (and I was sad that despite now understanding grokking I don't really feel like I learned anything safety-relevant from that project). I don't know if that means you should / shouldn't include these papers in the Airtable.\n- There's a ton of work on science of deep\u00a0learning from the broader community. I think a lot of it is misguided (which is part of why I worked on it) but there is also a lot that seems quite good. My sense is that the\u00a0current selection is a bit EA-skewed (i.e. it's the stuff that EAs think is important for probably-not-great reasons) -- e.g. I think\u00a0[distributional generalization](https://arxiv.org/abs/2009.08092)\u00a0is great science of deep learning but is relatively unknown in EA circles. It's plausible that you want to remove this area until you can have a version that is easier to defend as a good selection of the top papers. (On the other hand, it's possible that the current skew isn't EA-specific, and that you'd get a similar skew if you asked ML people what the best science of deep learning papers were, in which case it seems more fine to keep it the way it is.)\n\n",
        "Reviewed by": [
            "AG",
            "VK",
            "PC",
            "RS"
        ],
        "Safety Category": [
            "Capabilities of LLMs",
            "Science of deep learning"
        ],
        "Safety Topic": [
            "Emergence"
        ],
        "Title": "Explaining grokking through circuit efficiency (Varma et al., 2023)",
        "Type": [
            "Paper"
        ],
        "Who tagged for ML": [
            "Viktoria"
        ],
        "airtable_createdTime": "2024-02-10T00:27:32.000Z",
        "airtable_id": "recV3ihDpKaUEVfKk"
    },
    {
        "# Top Paper votes": 0,
        "Category": [
            "AI Governance"
        ],
        "Internal source": [
            "NOLA Workshop 2023"
        ],
        "Link": "https://www.alignment-workshop.com/nola-talks/lewis-hammond-multi-agent-risks-from-advanced-ai",
        "Safety Category": [
            "AI Governance"
        ],
        "Title": "Multi-agent risks from advanced AI (Lewis Hammond, 5-min video)",
        "Type": [
            "Video"
        ],
        "Vael's Notes": "Not including the lightning talks for now, since pretty EA probably\n",
        "airtable_createdTime": "2024-02-10T00:27:32.000Z",
        "airtable_id": "recV60X94vdmwyqqT"
    },
    {
        "# Top Paper votes": 0,
        "Abstract": "\n",
        "Category": [
            "Interpretability / explainability",
            "Non-Adversarial Robustness"
        ],
        "Internal source": [
            "AWAIR"
        ],
        "Link": "https://docs.google.com/presentation/d/1nzAiNC71qhr_2bBM6Q5i_5YASqiyCGHXbMh7P2Qym_0/edit#slide=id.p",
        "ML Subfield": [
            "Domain General",
            "Robustness and Adversariality",
            "Model Evaluation"
        ],
        "Reviewed by": [
            "AG",
            "VK",
            "PC",
            "RS"
        ],
        "Safety Category": [
            "Transparency / Interpretability / Model internals / Latent knowledge"
        ],
        "Title": "Model Internals Survey slides (AWAIR, 2023)",
        "Type": [
            "Slides"
        ],
        "Who tagged for ML": [
            "Viktoria"
        ],
        "airtable_createdTime": "2024-02-10T00:27:32.000Z",
        "airtable_id": "recVC6sHfudhSeJ9U"
    },
    {
        "# Top Paper votes": 0,
        "Category": [
            "Interpretability / explainability"
        ],
        "Internal source": [
            "Twitter"
        ],
        "Link": "https://twitter.com/NeelNanda5",
        "Safety Category": [
            "Transparency / Interpretability / Model internals / Latent knowledge"
        ],
        "Safety Topic": [
            "Mechanistic interpretability"
        ],
        "Title": "For new papers on Interpretability: Follow Neel Nanda on Twitter",
        "Type": [
            "Other"
        ],
        "airtable_createdTime": "2024-02-10T00:27:25.000Z",
        "airtable_id": "recVFyhBv22fOQGAi"
    },
    {
        "# Top Paper votes": 0,
        "Abstract": "    Localizing behaviors of neural networks to a subset of the network's components or a subset of interactions between components is a natural first step towards analyzing network mechanisms and possible failure modes. Existing work is often qualitative and ad-hoc, and there is no consensus on the appropriate way to evaluate localization claims. We introduce path patching, a technique for expressing and quantitatively testing a natural class of hypotheses expressing that behaviors are localized to a set of paths. We refine an explanation of induction heads, characterize a behavior of GPT-2, and open source a framework for efficiently running similar experiments. \n",
        "Category": [
            "Interpretability / explainability"
        ],
        "Internal source": [
            "AWAIR"
        ],
        "Link": "https://arxiv.org/abs/2304.05969",
        "ML Subfield": [
            "NLP"
        ],
        "ML Subtopic": [
            "NLP: LLMs"
        ],
        "Reviewed by": [
            "AG",
            "VK",
            "PC",
            "RS",
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Transparency / Interpretability / Model internals / Latent knowledge"
        ],
        "Title": "Localizing Model Behavior With Path Patching (Goldowsky-Dill et al., 2023)",
        "Type": [
            "Paper"
        ],
        "Vael's Notes": "causal abstraction stuff: <https://www.lesswrong.com/posts/uLMWMeBG3ruoBRhMW/a-comparison-of-causal-scrubbing-causal-abstractions-and> , also see Slide 10 here: https://docs.google.com/presentation/d/1nzAiNC71qhr\\_2bBM6Q5i\\_5YASqiyCGHXbMh7P2Qym\\_0/edit#slide=id.g25b2887aa6d\\_1\\_20\n",
        "Who tagged for ML": [
            "Viktoria"
        ],
        "airtable_createdTime": "2023-09-12T23:55:44.000Z",
        "airtable_id": "recVTd64KFiNifQBg"
    },
    {
        "# Top Paper votes": 0,
        "AG comments": "Adversarial robustness is a bit sparse right now. Not an area I'd push people towards in general, but there are\u00a0so /many/\u00a0people in ML doing adversarial robustness work that it seems like a good entry point for them at least. I'd plug this work on\u00a0adversarial policies in Go AI\u00a0both as a demo (superintelligent systems might be vulnerable) and as an illustration of a different threat model they might want to consider working in. Maybe\u00a0Unrestricted Adversarial Examples.\n",
        "Abstract": "    We introduce a two-player contest for evaluating the safety and robustness of machine learning systems, with a large prize pool. Unlike most prior work in ML robustness, which studies norm-constrained adversaries, we shift our focus to unconstrained adversaries. Defenders submit machine learning models, and try to achieve high accuracy and coverage on non-adversarial data while making no confident mistakes on adversarial inputs. Attackers try to subvert defenses by finding arbitrary unambiguous inputs where the model assigns an incorrect label with high confidence. We propose a simple unambiguous dataset (\"bird-or- bicycle\") to use as part of this contest. We hope this contest will help to more comprehensively evaluate the worst-case adversarial risk of machine learning models. \n",
        "Category": [
            "Non-Adversarial Robustness"
        ],
        "Citations": 97,
        "Internal source": [
            "Elsewhere"
        ],
        "Link": "https://arxiv.org/abs/1809.08352",
        "ML Subfield": [
            "Robustness and Adversariality"
        ],
        "ML Subfield (Is Starting Point)": [
            "Robustness and Adversariality",
            "Applied ML"
        ],
        "Reviewed by": [
            "AG",
            "VK",
            "PC",
            "RS",
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Adversaries / Robustness / Generalization"
        ],
        "Title": "Unrestricted Adversarial Examples (Brown et al., 2018)",
        "Type": [
            "Paper"
        ],
        "Who tagged for ML": [
            "Viktoria"
        ],
        "airtable_createdTime": "2023-09-25T20:57:23.000Z",
        "airtable_id": "recW28oaqMNT22LDl"
    },
    {
        "# Top Paper votes": 0,
        "Abstract": "Transformers have become the dominant model in deep learning, but the reason for their superior performance is poorly understood. Here, we hypothesize that the strong performance of Transformers stems from an architectural bias towards mesa-optimization, a learned process running within the forward pass of a model consisting of the following two steps: (i) the construction of an internal learning objective, and (ii) its corresponding solution found through optimization. To test this hypothesis, we reverse-engineer a series of autoregressive Transformers trained on simple sequence modeling tasks, uncovering underlying gradient-based mesa-optimization algorithms driving the generation of predictions. Moreover, we show that the learned forward-pass optimization algorithm can be immediately repurposed to solve supervised few-shot tasks, suggesting that mesa-optimization might underlie the in-context learning capabilities of large language models. Finally, we propose a novel self-attention layer, the mesa-layer, that explicitly and efficiently solves optimization problems specified in context. We find that this layer can lead to improved performance in synthetic and preliminary language modeling experiments, adding weight to our hypothesis that mesa-optimization is an important operation hidden within the weights of trained Transformers. \n",
        "Internal source": [
            "Hendrycks Textbook"
        ],
        "Link": "https://arxiv.org/abs/2309.05858",
        "ML Subfield": [
            "NLP",
            "Optimization"
        ],
        "ML Subtopic": [
            "NLP: LLMs"
        ],
        "Reviewed by": [
            "PC",
            "RS"
        ],
        "Safety Category": [
            "Capabilities of LLMs",
            "Science of deep learning"
        ],
        "Safety Topic": [
            "Emergence"
        ],
        "Title": "Uncovering mesa-optimization algorithms in Transformers (Von Oswald et al., 2023)",
        "Type": [
            "Paper"
        ],
        "Who tagged for ML": [
            "Viktoria"
        ],
        "airtable_createdTime": "2024-02-10T00:27:32.000Z",
        "airtable_id": "recW7Emvadt6bJLTR"
    },
    {
        "# Top Paper votes": 0,
        "Abstract": "Transformer architecture has shown impressive performance in multiple research domains and has become the backbone of many neural network models. However, there is limited understanding on how it works. In particular, with a simple predictive loss, how the representation emerges from the gradient \\emph{training dynamics} remains a mystery. In this paper, for 1-layer transformer with one self-attention layer plus one decoder layer, we analyze its SGD training dynamics for the task of next token prediction in a mathematically rigorous manner. We open the black box of the dynamic process of how the self-attention layer combines input tokens, and reveal the nature of underlying inductive bias. More specifically, with the assumption (a) no positional encoding, (b) long input sequence, and (c) the decoder layer learns faster than the self-attention layer, we prove that self-attention acts as a \\emph{discriminative scanning algorithm}: starting from uniform attention, it gradually attends more to distinct key tokens for a specific next token to be predicted, and pays less attention to common key tokens that occur across different next tokens. Among distinct tokens, it progressively drops attention weights, following the order of low to high co-occurrence between the key and the query token in the training set. Interestingly, this procedure does not lead to winner-takes-all, but decelerates due to a \\emph{phase transition} that is controllable by the learning rates of the two layers, leaving (almost) fixed token combination. We verify this \\textbf{\\emph{scan and snap}} dynamics on synthetic and real-world data (WikiText).\n",
        "Category": [
            "Interpretability / explainability"
        ],
        "Internal source": [
            "http://rdi.berkeley.edu/understanding_llms/s24"
        ],
        "Link": "https://arxiv.org/abs/2305.16380",
        "Safety Category": [
            "Science of deep learning",
            "Transparency / Interpretability / Model internals / Latent knowledge"
        ],
        "Title": "Scan and Snap: Understanding Training Dynamics and Token Composition in 1-layer Transformer (Tian et al., 2023)",
        "Type": [
            "Paper"
        ],
        "airtable_createdTime": "2024-03-31T18:06:47.000Z",
        "airtable_id": "recX2qqs75mRHEcoa"
    },
    {
        "# Top Paper votes": 0,
        "Abstract": "We report the presence of a simple neural mechanism that represents an input-output function as a vector within autoregressive transformer language models (LMs). Using causal mediation analysis on a diverse range of in-context-learning (ICL) tasks, we find that a small number attention heads transport a compact representation of the demonstrated task, which we call a function vector (FV). FVs are robust to changes in context, i.e., they trigger execution of the task on inputs such as zero-shot and natural text settings that do not resemble the ICL contexts from which they are collected. We test FVs across a range of tasks, models, and layers and find strong causal effects across settings in middle layers. We investigate the internal structure of FVs and find while that they often contain information that encodes the output space of the function, this information alone is not sufficient to reconstruct an FV. Finally, we test semantic vector composition in FVs, and find that to some extent they can be summed to create vectors that trigger new complex tasks. Taken together, our findings suggest that LLMs contain internal abstractions of general-purpose functions that can be invoked in a variety of contexts.\n",
        "Category": [
            "Interpretability / explainability"
        ],
        "Internal source": [
            "Elsewhere"
        ],
        "Link": "https://arxiv.org/abs/2310.15213",
        "Safety Category": [
            "Transparency / Interpretability / Model internals / Latent knowledge"
        ],
        "Title": "Function Vectors in Large Language Models (Todd et al., 2023)",
        "Type": [
            "Paper"
        ],
        "airtable_createdTime": "2024-03-31T18:06:47.000Z",
        "airtable_id": "recXaplGaegOOEbS9"
    },
    {
        "# Top Paper votes": 1,
        "AD: Top Paper": true,
        "Abstract": "    We describe our early efforts to red team language models in order to simultaneously discover, measure, and attempt to reduce their potentially harmful outputs. We make three main contributions. First, we investigate scaling behaviors for red teaming across 3 model sizes (2.7B, 13B, and 52B parameters) and 4 model types: a plain language model (LM); an LM prompted to be helpful, honest, and harmless; an LM with rejection sampling; and a model trained to be helpful and harmless using reinforcement learning from human feedback (RLHF). We find that the RLHF models are increasingly difficult to red team as they scale, and we find a flat trend with scale for the other model types. Second, we release our dataset of 38,961 red team attacks for others to analyze and learn from. We provide our own analysis of the data and find a variety of harmful outputs, which range from offensive language to more subtly harmful non-violent unethical outputs. Third, we exhaustively describe our instructions, processes, statistical methodologies, and uncertainty about red teaming. We hope that this transparency accelerates our ability to work together as a community in order to develop shared norms, practices, and technical standards for how to red team language models. \n",
        "Category": [
            "Non-Adversarial Robustness"
        ],
        "Citations": 239,
        "Goes online in Expanded Papers": true,
        "Internal source": [
            "Elsewhere"
        ],
        "Link": "https://arxiv.org/abs/2209.07858",
        "ML Subfield": [
            "NLP",
            "Robustness and Adversariality",
            "Model Evaluation"
        ],
        "ML Subtopic": [
            "NLP: LLMs"
        ],
        "Reviewed by": [
            "AG",
            "VK",
            "PC",
            "RS",
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Adversaries / Robustness / Generalization"
        ],
        "Title": "Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned (Ganguli et al., 2022)",
        "Twitter": "https://twitter.com/AnthropicAI/status/1571988929800273932?lang=en",
        "Type": [
            "Paper"
        ],
        "Vael's Notes": "lit review from https://arxiv.org/pdf/2307.02483.pdf\n",
        "Who tagged for ML": [
            "Viktoria"
        ],
        "Year": "2022",
        "airtable_createdTime": "2023-09-12T00:19:34.000Z",
        "airtable_id": "recYJvijaiVvh01b8"
    },
    {
        "# Top Paper votes": 0,
        "Abstract": "We aim to better understand the emergence of \\`situational awareness' in large language models (LLMs). A model is situationally aware if it's aware that it's a model and can recognize whether it's currently in testing or deployment. Today's LLMs are tested for safety and alignment before they are deployed. An LLM could exploit situational awareness to achieve a high score on safety tests, while taking harmful actions after deployment. Situational awareness may emerge unexpectedly as a byproduct of model scaling. One way to better foresee this emergence is to run scaling experiments on abilities necessary for situational awareness. As such an ability, we propose \\`out-of-context reasoning' (in contrast to in-context learning). We study out-of-context reasoning experimentally. First, we finetune an LLM on a description of a test while providing no examples or demonstrations. At test time, we assess whether the model can pass the test. To our surprise, we find that LLMs succeed on this out-of-context reasoning task. Their success is sensitive to the training setup and only works when we apply data augmentation. For both GPT-3 and LLaMA-1, performance improves with model size. These findings offer a foundation for further empirical study, towards predicting and potentially controlling the emergence of situational awareness in LLMs. Code is available at: this https URL. \n",
        "Blog or Video": "https://www.alignment-workshop.com/nola-talks/owain-evans-out-of-context-reasoning-in-llms",
        "Category": [
            "Overviews and agendas"
        ],
        "Citations": 3,
        "Internal source": [
            "NOLA Workshop 2023"
        ],
        "Link": "https://arxiv.org/abs/2309.00667",
        "ML Subfield": [
            "NLP",
            "Model Evaluation"
        ],
        "ML Subtopic": [
            "NLP: LLMs",
            "Applied ML: Chatbots"
        ],
        "Reviewed by": [
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Major problems (misalignment, misuse, threat models)"
        ],
        "Safety Topic": [
            "Situational awareness (also instrumental convergence, inner misalignment)"
        ],
        "Title": "Taken out of context: On measuring situational awareness in LLMs (Berglund et al., 2023)",
        "Type": [
            "Video",
            "Paper"
        ],
        "Who tagged for ML": [
            "Viktoria"
        ],
        "Year": "2023",
        "airtable_createdTime": "2024-01-15T20:13:29.000Z",
        "airtable_id": "recYRIB4bR4ctwk7O"
    },
    {
        "# Top Paper votes": 0,
        "Abstract": "    Artificial intelligence (AI) has the potential to greatly improve society, but as with any powerful technology, it comes with heightened risks and responsibilities. Current AI research lacks a systematic discussion of how to manage long-tail risks from AI systems, including speculative long-term risks. Keeping in mind the potential benefits of AI, there is some concern that building ever more intelligent and powerful AI systems could eventually result in systems that are more powerful than us; some say this is like playing with fire and speculate that this could create existential risks (x-risks). To add precision and ground these discussions, we provide a guide for how to analyze AI x-risk, which consists of three parts: First, we review how systems can be made safer today, drawing on time-tested concepts from hazard analysis and systems safety that have been designed to steer large processes in safer directions. Next, we discuss strategies for having long-term impacts on the safety of future systems. Finally, we discuss a crucial concept in making AI systems safer by improving the balance between safety and general capabilities. We hope this document and the presented concepts and tools serve as a useful guide for understanding how to analyze AI x-risk. \n",
        "Category": [
            "Overviews and agendas"
        ],
        "Citations": 57,
        "Internal source": [
            "Intro to ML Safety Course"
        ],
        "Link": "https://arxiv.org/abs/2206.05862",
        "ML Subfield": [
            "Security",
            "Domain General",
            "Theory"
        ],
        "Reviewed by": [
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Major problems (misalignment, misuse, threat models)",
            "Overview"
        ],
        "Title": "X-Risk Analysis for AI Research (Hendrycks and Mazeika, 2022)",
        "Type": [
            "Paper"
        ],
        "Who tagged for ML": [
            "Viktoria"
        ],
        "Year": "2022",
        "airtable_createdTime": "2024-01-15T21:28:36.000Z",
        "airtable_id": "recYo2mqTaP1wC4q5"
    },
    {
        "# Top Paper votes": 0,
        "Category": [
            "Model evaluations and benchmarks"
        ],
        "Internal source": [
            "Elsewhere"
        ],
        "Link": "https://openai.com/safety/preparedness",
        "ML Subfield": [
            "Model Evaluation"
        ],
        "ML Subtopic": [
            "NLP: LLMs"
        ],
        "Safety Category": [
            "Model evaluations / monitoring / detection"
        ],
        "Title": "OpenAI's Preparedness Framework",
        "Type": [
            "Blog post"
        ],
        "Who tagged for ML": [
            "Vael"
        ],
        "airtable_createdTime": "2024-03-18T21:41:05.000Z",
        "airtable_id": "recZE2kmz6cbdK9lh"
    },
    {
        "# Top Paper votes": 0,
        "Abstract": " We introduce a new notion of generalization -- Distributional Generalization -- which roughly states that outputs of a classifier at train and test time are close \\*as distributions\\*, as opposed to close in just their average error. For example, if we mislabel 30% of dogs as cats in the train set of CIFAR-10, then a ResNet trained to interpolation will in fact mislabel roughly 30% of dogs as cats on the \\*test set\\* as well, while leaving other classes unaffected. This behavior is not captured by classical generalization, which would only consider the average error and not the distribution of errors over the input domain. Our formal conjectures, which are much more general than this example, characterize the form of distributional generalization that can be expected in terms of problem parameters: model architecture, training procedure, number of samples, and data distribution. We give empirical evidence for these conjectures across a variety of domains in machine learning, including neural networks, kernel machines, and decision trees. Our results thus advance our empirical understanding of interpolating classifiers. \n",
        "Category": [
            "Theory"
        ],
        "Citations": 34,
        "Internal source": [
            "Expert rec"
        ],
        "Link": "https://arxiv.org/pdf/2009.08092.pdf",
        "ML Subfield": [
            "Theory",
            "Domain General"
        ],
        "ML Subfield (Is Starting Point)": [
            "Theory"
        ],
        "Reviewed by": [
            "RS"
        ],
        "Safety Category": [
            "Capabilities of LLMs",
            "Science of deep learning"
        ],
        "Title": "Distributional Generalization: A New Kind of Generalization (Nakkiran and Bansal, 2020)",
        "Type": [
            "Paper"
        ],
        "Who tagged for ML": [
            "Viktoria"
        ],
        "airtable_createdTime": "2024-02-10T00:27:32.000Z",
        "airtable_id": "recZJBO0mDt6YFBGK"
    },
    {
        "# Top Paper votes": 1,
        "AD comments": "\n",
        "AD: Top Paper": true,
        "Abstract": "    Potential harms of large language models can be mitigated by watermarking model output, i.e., embedding signals into generated text that are invisible to humans but algorithmically detectable from a short span of tokens. We propose a watermarking framework for proprietary language models. The watermark can be embedded with negligible impact on text quality, and can be detected using an efficient open-source algorithm without access to the language model API or parameters. The watermark works by selecting a randomized set of \"green\" tokens before a word is generated, and then softly promoting use of green tokens during sampling. We propose a statistical test for detecting the watermark with interpretable p-values, and derive an information-theoretic framework for analyzing the sensitivity of the watermark. We test the watermark using a multi-billion parameter model from the Open Pretrained Transformer (OPT) family, and discuss robustness and security. \n",
        "Category": [
            "Security"
        ],
        "Citations": 322,
        "Description from MAIA / AISF / Elsewhere": "[Paper] proposes that a method of pseudorandom sampling from an LLM\u2019s next-token distribution which makes it easier to detect whether text was generated by that LLM",
        "Goes online in Expanded Papers": true,
        "High Citation and Max Select": true,
        "Internal source": [
            "ML Newsletter"
        ],
        "Link": "https://arxiv.org/abs/2301.10226",
        "ML Subfield": [
            "NLP",
            "Security"
        ],
        "ML Subfield (Is Starting Point)": [
            "NLP",
            "Security"
        ],
        "ML Subtopic": [
            "NLP: LLMs"
        ],
        "Reviewed by": [
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Model evaluations / monitoring / detection"
        ],
        "Safety Topic": [
            "Security"
        ],
        "Title": "A Watermark for Large Language Models (Kirchenbauer et al., 2023)",
        "Type": [
            "Paper"
        ],
        "Who tagged for ML": [
            "Viktoria"
        ],
        "Year": "2023",
        "airtable_createdTime": "2023-12-20T01:45:25.000Z",
        "airtable_id": "recZJgWWbCsmIuYpD"
    },
    {
        "# Top Paper votes": 1,
        "AD: Top Paper": true,
        "Abstract": "Recent work has demonstrated that deep neural networks are vulnerable to adversarial examples---inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. Code and pre-trained models are available at this https URL and this https URL. \n",
        "Category": [
            "Theory",
            "Adversarial Robustness"
        ],
        "Citations": 11766,
        "Goes online in Expanded Papers": true,
        "High Citation and Max Select": true,
        "Internal source": [
            "Intro to ML Safety Course"
        ],
        "Link": "https://arxiv.org/abs/1706.06083",
        "ML Subfield": [
            "Robustness and Adversariality",
            "Theory",
            "Optimization"
        ],
        "ML Subfield (Is Starting Point)": [
            "Robustness and Adversariality",
            "Theory"
        ],
        "Reviewed by": [
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Adversaries / Robustness / Generalization"
        ],
        "Title": "Towards Deep Learning Models Resistant to Adversarial Attacks (Madry et al., 2018)",
        "Type": [
            "Paper"
        ],
        "Who tagged for ML": [
            "Viktoria"
        ],
        "Year": "2018",
        "airtable_createdTime": "2024-01-15T21:18:46.000Z",
        "airtable_id": "recZe4B1aPbj1v99k"
    },
    {
        "# Top Paper votes": 0,
        "Blog or Video": "https://www.alignment-workshop.com/nola-talks/christian-schroeder-de-witt-secret-collusion-among-generative-ai-agents",
        "Category": [
            "Reward misspecification and goal misgeneralization"
        ],
        "Internal source": [
            "NOLA Workshop 2023"
        ],
        "Link": "https://openreview.net/attachment?id=FXZFrOvIoc&name=pdf",
        "Safety Category": [
            "Reward misspecification and goal misgeneralization",
            "Adversaries / Robustness / Generalization"
        ],
        "Title": "A Perfect Collusion Benchmark: How can AI agents be prevented from colluding with information-theoretic undetectability? (Motwani et al., 2023)",
        "Type": [
            "Paper",
            "Video"
        ],
        "Vael's Notes": "Not including the lightning talks for now, since pretty EA probably\n",
        "airtable_createdTime": "2024-02-10T00:27:32.000Z",
        "airtable_id": "reca6JjrsQjpB6uwZ"
    },
    {
        "# Top Paper votes": 0,
        "Category": [
            "Model evaluations and benchmarks"
        ],
        "Context phrase (please add!)": "Expert1, Expert2\n",
        "Description from MAIA / AISF / Elsewhere": "(haven't watched this yet)",
        "Link": "https://youtu.be/VbNPZoFe7ng?t=92",
        "Safety Category": [
            "Model evaluations / monitoring / detection"
        ],
        "Time from MAIA / AISF / Elsewhere (min)": "25",
        "Title": "Discovering AI Risks with AIs (Ethan Perez, 53-min video) ",
        "Type": [
            "Video"
        ],
        "airtable_createdTime": "2024-02-10T00:27:32.000Z",
        "airtable_id": "recaEaKdtRf5TpOdc"
    },
    {
        "# Top Paper votes": 0,
        "Category": [
            "Reward misspecification and goal misgeneralization"
        ],
        "Internal source": [
            "NOLA Workshop 2023"
        ],
        "Link": "https://www.alignment-workshop.com/nola-talks/jascha-sohl-dickstein-adversarial-examples-transfer-from-machines-to-huma",
        "Safety Category": [
            "Reward misspecification and goal misgeneralization",
            "Adversaries / Robustness / Generalization"
        ],
        "Title": "Adversarial examples transfer from machines to humans (Jascha Sohl-Dickstein, 4-min video)",
        "Type": [
            "Video"
        ],
        "Vael's Notes": "Not including the lightning talks for now, since pretty EA probably\n",
        "airtable_createdTime": "2024-02-10T00:27:32.000Z",
        "airtable_id": "recaT8WdaJ5FVgPi5"
    },
    {
        "# Top Paper votes": 0,
        "Abstract": "We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4. \n",
        "Category": [
            "Model evaluations and benchmarks",
            "Reinforcement Learning"
        ],
        "Citations": 1172,
        "High Citation and Max Select": true,
        "Internal source": [
            "Elsewhere"
        ],
        "Link": "https://arxiv.org/abs/2303.08774",
        "ML Methods Tag": [
            "RLHF"
        ],
        "ML Subfield": [
            "NLP",
            "Applied ML",
            "Reinforcement Learning"
        ],
        "ML Subfield (Is Starting Point)": [
            "NLP"
        ],
        "ML Subtopic": [
            "NLP: LLMs",
            "Applied ML: Chatbots"
        ],
        "Reviewed by": [
            "AG",
            "VK",
            "PC",
            "RS",
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Model evaluations / monitoring / detection"
        ],
        "Time from MAIA / AISF / Elsewhere (min)": "5",
        "Title": "GPT-4 Technical Report (OpenAI, 2023)",
        "Type": [
            "Paper"
        ],
        "What sections to read from MAIA": "(section 2.9 only, staring on pg 54)",
        "Who tagged for ML": [
            "Viktoria"
        ],
        "Year": "2023",
        "airtable_createdTime": "2023-09-05T22:50:05.000Z",
        "airtable_id": "recbHM3koYESAgAf0"
    },
    {
        "# Top Paper votes": 0,
        "Abstract": "Lack of transparency in deep neural networks\n(DNNs) make them susceptible to backdoor attacks, where hidden\nassociations or triggers override normal classification to produce\nunexpected results. For example, a model with a backdoor always\nidentifies a face as Bill Gates if a specific symbol is present in the\ninput. Backdoors can stay hidden indefinitely until activated by\nan input, and present a serious security risk to many security or\nsafety related applications, e.g., biometric authentication systems\nor self-driving cars.\nWe present the first robust and generalizable detection and\nmitigation system for DNN backdoor attacks. Our techniques\nidentify backdoors and reconstruct possible triggers. We identify\nmultiple mitigation techniques via input filters, neuron pruning\nand unlearning. We demonstrate their efficacy via extensive\nexperiments on a variety of DNNs, against two types of backdoor\ninjection methods identified by prior work. Our techniques also\nprove robust against a number of variants of the backdoor attack.\n",
        "Category": [
            "Security",
            "Unlearning and Knowledge Editing"
        ],
        "Citations": 1380,
        "Goes online in Expanded Papers": true,
        "High Citation and Max Select": true,
        "Internal source": [
            "Intro to ML Safety Course"
        ],
        "Link": "https://people.cs.uchicago.edu/~ravenben/publications/abstracts/backdoor-sp19.html",
        "ML Subfield": [
            "Robustness and Adversariality",
            "Security",
            "Vision"
        ],
        "ML Subfield (Is Starting Point)": [
            "Security"
        ],
        "ML Subtopic": [
            "Applied ML: Autonomous Vehicles"
        ],
        "Reviewed by": [
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Adversaries / Robustness / Generalization"
        ],
        "Safety Topic": [
            "Trojans"
        ],
        "Title": "Neural Cleanse: Identifying and Mitigating Backdoor Attacks in Neural Networks (Wang et al., 2019)z",
        "Type": [
            "Paper"
        ],
        "Who tagged for ML": [
            "Viktoria"
        ],
        "Year": "2019",
        "airtable_createdTime": "2024-01-15T21:37:21.000Z",
        "airtable_id": "recbXLvDKIJcxB6p7"
    },
    {
        "# Top Paper votes": 0,
        "Abstract": "Common methods for aligning large language models (LLMs) with desired behaviour heavily rely on human-labelled data. However, as models grow increasingly sophisticated, they will surpass human expertise, and the role of human evaluation will evolve into non-experts overseeing experts. In anticipation of this, we ask: can weaker models assess the correctness of stronger models? We investigate this question in an analogous setting, where stronger models (experts) possess the necessary information to answer questions and weaker models (non-experts) lack this information. The method we evaluate is debate, where two LLM experts each argue for a different answer, and a non-expert selects the answer. We find that debate consistently helps both non-expert models and humans answer questions, achieving 76% and 88% accuracy respectively (naive baselines obtain 48% and 60%). Furthermore, optimising expert debaters for persuasiveness in an unsupervised manner improves non-expert ability to identify the truth in debates. Our results provide encouraging empirical evidence for the viability of aligning models with debate in the absence of ground truth.\n",
        "Category": [
            "Scalable oversight"
        ],
        "Goes online as Top Paper": true,
        "Goes online in Expanded Papers": true,
        "Link": "https://arxiv.org/abs/2402.06782",
        "Title": "Debating with More Persuasive LLMs Leads to More Truthful Answers",
        "Type": [
            "Paper"
        ],
        "Year": "2024",
        "airtable_createdTime": "2024-08-09T15:35:26.000Z",
        "airtable_id": "recbYwqXQKClhQuzM"
    },
    {
        "# Top Paper votes": 0,
        "Abstract": "An important goal in deep learning is to learn versatile, high-level feature representations of input data. However, standard networks' representations seem to possess shortcomings that, as we illustrate, prevent them from fully realizing this goal. In this work, we show that robust optimization can be re-cast as a tool for enforcing priors on the features learned by deep neural networks. It turns out that representations learned by robust models address the aforementioned shortcomings and make significant progress towards learning a high-level encoding of inputs. In particular, these representations are approximately invertible, while allowing for direct visualization and manipulation of salient input features. More broadly, our results indicate adversarial robustness as a promising avenue for improving learned representations. Our code and models for reproducing these results is available at this https URL . \n",
        "Category": [
            "Interpretability / explainability",
            "Non-Adversarial Robustness"
        ],
        "Description from MAIA / AISF / Elsewhere": "This paper trains networks with a \u201crobust objective,\u201d i.e. an objective which incentivizes the model to perform well even when its inputs are corrupted by small perturbations. The resulting networks learn features which are much more semantically cohesive, suggesting a connection between adversarial robustness and interpretability.",
        "Link": "https://arxiv.org/abs/1906.00945",
        "ML Subfield": [
            "Robustness and Adversariality",
            "Optimization"
        ],
        "Reviewed by": [
            "AG",
            "VK",
            "PC",
            "RS",
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Transparency / Interpretability / Model internals / Latent knowledge"
        ],
        "Time from MAIA / AISF / Elsewhere (min)": "30",
        "Title": "Adversarial Robustness as a Prior for Learned Representations (Engstrom et al., 2019) ",
        "Type": [
            "Paper"
        ],
        "Who tagged for ML": [
            "Viktoria"
        ],
        "airtable_createdTime": "2023-09-16T22:38:17.000Z",
        "airtable_id": "recbjfhPyJmesqNiQ"
    },
    {
        "# Top Paper votes": 2,
        "AG comments": "Very engaging\n",
        "AG: Top Paper": true,
        "Abstract": "\n",
        "Category": [
            "Interpretability / explainability"
        ],
        "Citations": 258,
        "Context phrase (please add!)": "Canonical introduction to mechanistic interpretability - Vael\n",
        "Goes online as Top Paper": true,
        "Goes online in Expanded Papers": true,
        "High Citation and Max Select": true,
        "Internal source": [
            "AWAIR",
            "Elsewhere"
        ],
        "Link": "https://distill.pub/2020/circuits/zoom-in/",
        "ML Subfield": [
            "Vision",
            "Theory"
        ],
        "ML Subfield (Is Starting Point)": [
            "Theory"
        ],
        "Reviewed by": [
            "AG",
            "VK",
            "PC",
            "RS",
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Transparency / Interpretability / Model internals / Latent knowledge"
        ],
        "Safety Topic": [
            "Mechanistic interpretability"
        ],
        "Time from MAIA / AISF / Elsewhere (min)": "35",
        "Title": "Zoom In: An Introduction to Circuits (Olah et al., 2020)",
        "Transcripts / Audio / Slides": "https://preview.type3.audio/episode/agi-safety-fundamentals-alignment/632feda4-71a5-42a2-84a1-588c6b9ea1ad",
        "Type": [
            "Paper"
        ],
        "VK: Top Paper": true,
        "Vael's Notes": "\"foundational work decoding the representations and algorithms learned by NNs\" - AWAIR\n",
        "Who tagged for ML": [
            "Viktoria"
        ],
        "Year": "2020",
        "airtable_createdTime": "2023-09-05T22:50:05.000Z",
        "airtable_id": "recbq8JY2ZgWzqImr"
    },
    {
        "# Top Paper votes": 0,
        "AG comments": "Recommended science of deep learning paper\n\nTBH none of these [science of deep learning papers] are directly safety relevant, it's much more \"foundational knowledge that'll help with safety\". I do feel ~good about safety-motivated people looking into this area, but they'd need to get the safety motivation/context from other areas, so it's not something I'd necessarily highlight. Might be good for people with more theoretical backgrounds though?\n",
        "Abstract": "We show that a variety of modern deep learning tasks exhibit a 'double-descent' phenomenon where, as we increase model size, performance first gets worse and then gets better. Moreover, we show that double descent occurs not just as a function of model size, but also as a function of the number of training epochs. We unify the above phenomena by defining a new complexity measure we call the effective model complexity and conjecture a generalized double descent with respect to this measure. Furthermore, our notion of model complexity allows us to identify certain regimes where increasing (even quadrupling) the number of train samples actually hurts test performance.\n",
        "Category": [
            "Theory"
        ],
        "Internal source": [
            "Elsewhere"
        ],
        "Link": "https://iopscience.iop.org/article/10.1088/1742-5468/ac3a74/meta",
        "ML Subfield": [
            "Theory"
        ],
        "ML Subfield (Is Starting Point)": [
            "Theory"
        ],
        "Reviewed by": [
            "AG",
            "VK",
            "PC",
            "RS"
        ],
        "Safety Category": [
            "Capabilities of LLMs",
            "Science of deep learning"
        ],
        "Title": "Deep double descent: where bigger models and more data hurt (Nakkiran et al., 2021)",
        "Type": [
            "Paper"
        ],
        "Who tagged for ML": [
            "Viktoria"
        ],
        "airtable_createdTime": "2024-02-10T00:27:32.000Z",
        "airtable_id": "reccKrHwW41B5sZso"
    },
    {
        "# Top Paper votes": 0,
        "Abstract": "    Research in mechanistic interpretability seeks to explain behaviors of machine learning models in terms of their internal components. However, most previous work either focuses on simple behaviors in small models, or describes complicated behaviors in larger models with broad strokes. In this work, we bridge this gap by presenting an explanation for how GPT-2 small performs a natural language task called indirect object identification (IOI). Our explanation encompasses 26 attention heads grouped into 7 main classes, which we discovered using a combination of interpretability approaches relying on causal interventions. To our knowledge, this investigation is the largest end-to-end attempt at reverse-engineering a natural behavior \"in the wild\" in a language model. We evaluate the reliability of our explanation using three quantitative criteria--faithfulness, completeness and minimality. Though these criteria support our explanation, they also point to remaining gaps in our understanding. Our work provides evidence that a mechanistic understanding of large ML models is feasible, opening opportunities to scale our understanding to both larger models and more complex tasks. \n",
        "Category": [
            "Interpretability / explainability"
        ],
        "Internal source": [
            "Further reading on MAIA/HAIST Summer 2023 Curriculum"
        ],
        "Link": "https://arxiv.org/pdf/2211.00593.pdf",
        "ML Subfield": [
            "NLP: LLMs"
        ],
        "ML Subtopic": [
            "NLP"
        ],
        "Reviewed by": [
            "AG",
            "VK",
            "PC",
            "RS"
        ],
        "Safety Category": [
            "Transparency / Interpretability / Model internals / Latent knowledge"
        ],
        "Safety Topic": [
            "Mechanistic interpretability"
        ],
        "Time from MAIA / AISF / Elsewhere (min)": "30",
        "Title": "Interpretability In The Wild: A Circuit For Indirect Object Identification In GPT-2 Small (Wang et al., 2022)",
        "Type": [
            "Paper"
        ],
        "Who tagged for ML": [
            "Viktoria"
        ],
        "airtable_createdTime": "2024-02-10T00:27:32.000Z",
        "airtable_id": "reccP1OhcuQiNHUw0"
    },
    {
        "# Top Paper votes": 1,
        "AD: Top Paper": true,
        "AG comments": "Maybe good for FATE folks as he specifically addresses concerns they may have? \n",
        "Category": [
            "Overviews and agendas"
        ],
        "Internal source": [
            "Elsewhere"
        ],
        "Link": "https://yoshuabengio.org/2023/06/24/faq-on-catastrophic-ai-risks/",
        "ML Subfield": [
            "Domain General"
        ],
        "Reviewed by": [
            "AG",
            "VK",
            "PC",
            "RS",
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Major problems (misalignment, misuse, threat models)",
            "Overview"
        ],
        "Title": "FAQ on Catastrophic AI Risks (Bengio, 2023)\n",
        "Type": [
            "Blog post"
        ],
        "Who tagged for ML": [
            "Max"
        ],
        "Year": "2023",
        "airtable_createdTime": "2023-09-05T22:50:05.000Z",
        "airtable_id": "reccSsY3HAcXQleKy"
    },
    {
        "# Top Paper votes": 0,
        "Abstract": "\"Induction heads\" are attention heads that implement a simple algorithm to complete token sequences like \\[A]\\[B] ... [A] -> [B]. In this work, we present preliminary and indirect evidence for a hypothesis that induction heads might constitute the mechanism for the majority of all \"in-context learning\" in large transformer models (i.e. decreasing loss at increasing token indices). We find that induction heads develop at precisely the same point as a sudden sharp increase in in-context learning ability, visible as a bump in the training loss. We present six complementary lines of evidence, arguing that induction heads may be the mechanistic source of general in-context learning in transformer models of any size. For small attention-only models, we present strong, causal evidence; for larger models with MLPs, we present correlational evidence.\n",
        "Category": [
            "Interpretability / explainability"
        ],
        "Internal source": [
            "Intro to ML Safety Course"
        ],
        "Link": "https://arxiv.org/abs/2209.11895",
        "Safety Category": [
            "Transparency / Interpretability / Model internals / Latent knowledge"
        ],
        "Title": "In-context Learning and Induction Heads (Olsson et al., 2022)",
        "Type": [
            "Paper"
        ],
        "airtable_createdTime": "2024-03-31T18:06:47.000Z",
        "airtable_id": "recccylY9lb9ShdGm"
    },
    {
        "# Top Paper votes": 0,
        "Abstract": "    The use of language-model-based question-answering systems to aid humans in completing difficult tasks is limited, in part, by the unreliability of the text these systems generate. Using hard multiple-choice reading comprehension questions as a testbed, we assess whether presenting humans with arguments for two competing answer options, where one is correct and the other is incorrect, allows human judges to perform more accurately, even when one of the arguments is unreliable and deceptive. If this is helpful, we may be able to increase our justified trust in language-model-based systems by asking them to produce these arguments where needed. Previous research has shown that just a single turn of arguments in this format is not helpful to humans. However, as debate settings are characterized by a back-and-forth dialogue, we follow up on previous results to test whether adding a second round of counter-arguments is helpful to humans. We find that, regardless of whether they have access to arguments or not, humans perform similarly on our task. These findings suggest that, in the case of answering reading comprehension questions, debate is not a helpful format. \n",
        "Category": [
            "Scalable oversight"
        ],
        "Internal source": [
            "Elsewhere"
        ],
        "Link": "https://arxiv.org/pdf/2210.10860.pdf",
        "ML Subfield": [
            "NLP: Question Answering",
            "NLP: LLMs"
        ],
        "ML Subtopic": [
            "Human Model Interaction",
            "NLP"
        ],
        "Reviewed by": [
            "AG",
            "VK",
            "PC",
            "RS"
        ],
        "Safety Category": [
            "Scalable oversight"
        ],
        "Safety Topic": [
            "Debate"
        ],
        "Title": "Two-Turn Debate Doesn't Help Humans Answer Hard Reading Comprehension Questions (Parrish et al., 2022)",
        "Twitter": "https://twitter.com/sleepinyourhat/status/1585759654478422016",
        "Type": [
            "Paper"
        ],
        "Who tagged for ML": [
            "Viktoria"
        ],
        "airtable_createdTime": "2024-02-10T00:27:32.000Z",
        "airtable_id": "recd1qLofbmVBlCYU"
    },
    {
        "# Top Paper votes": 0,
        "Abstract": "The emergence of pre-trained AI systems with powerful capabilities across a diverse and ever-increasing set of complex domains has raised a critical challenge for AI safety as tasks can become too complicated for humans to judge directly. Irving et al. [2018] proposed a debate method in this direction with the goal of pitting the power of such AI models against each other until the problem of identifying (mis)-alignment is broken down into a manageable subtask. While the promise of this approach is clear, the original framework was based on the assumption that the honest strategy is able to simulate deterministic AI systems for an exponential number of steps, limiting its applicability. In this paper, we show how to address these challenges by designing a new set of debate protocols where the honest strategy can always succeed using a simulation of a polynomial number of steps, whilst being able to verify the alignment of stochastic AI systems, even when the dishonest strategy is allowed to use exponentially many simulation steps. \n",
        "Category": [
            "Scalable oversight",
            "Theory"
        ],
        "Citations": 2,
        "Goes online in Expanded Papers": true,
        "Internal source": [
            "Elsewhere"
        ],
        "Link": "https://arxiv.org/abs/2311.14125",
        "ML Subfield": [
            "NLP"
        ],
        "Safety Category": [
            "Scalable oversight",
            "Alignment <-> RLHF"
        ],
        "Safety Topic": [
            "Debate"
        ],
        "Title": "Scalable AI Safety via Doubly-Efficient Debate (Brown-Cohen et al., 2024) ",
        "Type": [
            "Paper"
        ],
        "Year": "2024",
        "airtable_createdTime": "2024-03-31T18:06:47.000Z",
        "airtable_id": "recdnm4wB9wYIMoBf"
    },
    {
        "# Top Paper votes": 0,
        "Abstract": "One might imagine that AI systems with harmless goals will be harmless. This paper instead shows that intelligent systems will need to be carefully designed to prevent them from behaving in harmful ways. We identify a number of \u201cdrives\u201d that will appear in sufficiently advanced AI systems of any design. We call them drives because they are tendencies which will be present unless explicitly counteracted. We start by showing that goal-seeking systems will have drives to model their own operation and to improve themselves. We then show that self-improving systems will be driven to clarify their goals and represent them as economic utility functions. They will also strive for their actions to approximate rational economic behavior. This will lead almost all systems to protect their utility functions from modification and their utility measurement systems from corruption. We also discuss some exceptional systems which will want to modify their utility functions. We next discuss the drive toward self-protection which causes systems try to prevent themselves from being harmed. Finally we examine drives toward the acquisition of resources and toward their efficient utilization. We end with a discussion of how to incorporate these insights in d\n",
        "Internal source": [
            "Intro to ML Safety Course"
        ],
        "Link": "https://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf",
        "Safety Category": [
            "Forecasting"
        ],
        "Title": "The Basic AI Drives (Omohundro, 2008)",
        "Type": [
            "Paper"
        ],
        "airtable_createdTime": "2024-03-31T18:06:47.000Z",
        "airtable_id": "recdpkXODgWySnSjL"
    },
    {
        "# Top Paper votes": 0,
        "Abstract": "The Frontier Safety Framework is our rst version of a set of protocols that aims to address severe risks\nthat may arise from poweul capabilities of future foundation models. In focusing on these risks at the\nmodel level, it is intended to complement Google\u2019s existing suite of AI responsibility and safety\npractices, and enable AI innovation and deployment consistent with our AI Principles.\n",
        "Category": [
            "Model evaluations and benchmarks"
        ],
        "Goes online as Top Paper": true,
        "Goes online in Expanded Papers": true,
        "Link": "https://deepmind.google/discover/blog/introducing-the-frontier-safety-framework/?utm_source=substack&utm_medium=email",
        "Title": "Frontier Safety Framework",
        "Year": "2024",
        "airtable_createdTime": "2024-08-09T16:03:58.000Z",
        "airtable_id": "recdw83Zfa8LUSCg6"
    },
    {
        "# Top Paper votes": 1,
        "Abstract": "For artificial intelligence to be beneficial to humans the behaviour of AI agents needs to be aligned with what humans want. In this paper we discuss some behavioural issues for language agents, arising from accidental misspecification by the system designer. We highlight some ways that misspecification can occur and discuss some behavioural issues that could arise from misspecification, including deceptive or manipulative language, and review some approaches for avoiding these issues. \n",
        "Category": [
            "Overviews and agendas",
            "Reward misspecification and goal misgeneralization",
            "Deception",
            "Scalable oversight"
        ],
        "Citations": 116,
        "Internal source": [
            "Hendrycks Textbook"
        ],
        "Link": "http://arxiv.org/abs/2103.14659",
        "ML Subfield": [
            "NLP",
            "Human Model Interaction"
        ],
        "ML Subtopic": [
            "NLP: LLMs"
        ],
        "Reviewed by": [
            "PC",
            "RS",
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Major problems (misalignment, misuse, threat models)",
            "Reward misspecification and goal misgeneralization",
            "Alignment <-> RLHF",
            "Deception",
            "Adversaries / Robustness / Generalization",
            "Overview"
        ],
        "Title": "Alignment of Language Agents (Kenton et al., 2021)",
        "Type": [
            "Paper"
        ],
        "VK: Top Paper": true,
        "Who tagged for ML": [
            "Max"
        ],
        "Year": "2021",
        "airtable_createdTime": "2023-12-12T20:00:03.000Z",
        "airtable_id": "rece9ZY68HIqaNcvb"
    },
    {
        "# Top Paper votes": 0,
        "Abstract": "Advanced AI models hold the promise of tremendous benefits for humanity, but society needs to proactively manage the accompanying risks. In this paper, we focus on what we term \"frontier AI\" models: highly capable foundation models that could possess dangerous capabilities sufficient to pose severe risks to public safety. Frontier AI models pose a distinct regulatory challenge: dangerous capabilities can arise unexpectedly; it is difficult to robustly prevent a deployed model from being misused; and, it is difficult to stop a model's capabilities from proliferating broadly. To address these challenges, at least three building blocks for the regulation of frontier models are needed: (1) standard-setting processes to identify appropriate requirements for frontier AI developers, (2) registration and reporting requirements to provide regulators with visibility into frontier AI development processes, and (3) mechanisms to ensure compliance with safety standards for the development and deployment of frontier AI models. Industry self-regulation is an important first step. However, wider societal discussions and government intervention will be needed to create standards and to ensure compliance with them. We consider several options to this end, including granting enforcement powers to supervisory authorities and licensure regimes for frontier AI models. Finally, we propose an initial set of safety standards. These include conducting pre-deployment risk assessments; external scrutiny of model behavior; using risk assessments to inform deployment decisions; and monitoring and responding to new information about model capabilities and uses post-deployment. We hope this discussion contributes to the broader conversation on how to balance public safety risks and innovation benefits from advances at the frontier of AI development. \n",
        "Category": [
            "AI Governance",
            "Security"
        ],
        "Citations": 42,
        "Internal source": [
            "Twitter"
        ],
        "Link": "https://arxiv.org/abs/2307.03718",
        "ML Subfield": [
            "Security",
            "Human Model Interaction",
            "Applied ML"
        ],
        "ML Subfield (Is Starting Point)": [
            "Security",
            "Applied ML"
        ],
        "ML Subtopic": [
            "Applied ML: Chatbots"
        ],
        "Reviewed by": [
            "AG",
            "VK",
            "PC",
            "RS",
            "DH",
            "AD"
        ],
        "Safety Category": [
            "AI Governance"
        ],
        "Title": "Frontier AI Regulation: Managing Emerging Risks to Public Safety (Anderljung et al., 2023)",
        "Twitter": "https://twitter.com/Manderljung/status/1678414590529490947",
        "Type": [
            "Paper"
        ],
        "Who tagged for ML": [
            "Viktoria"
        ],
        "airtable_createdTime": "2023-09-12T20:51:45.000Z",
        "airtable_id": "receC7C0oxz6i5Zw9"
    },
    {
        "# Top Paper votes": 0,
        "Blog or Video": "https://www.alignment-workshop.com/nola-talks/sheila-mcilraith-epistemic-side-effects",
        "Category": [
            "Scalable oversight"
        ],
        "Internal source": [
            "NOLA Workshop 2023"
        ],
        "Link": "https://www.cs.toronto.edu/~toryn/docs/KlassenAAMAS2023epistemic.pdf",
        "Safety Category": [
            "Scalable oversight"
        ],
        "Title": "Epistemic Side Effects: An AI Safety Problem (Klassen et al., 2023)",
        "Type": [
            "Paper",
            "Video"
        ],
        "Vael's Notes": "Not including the lightning talks for now, since pretty EA probably\n",
        "airtable_createdTime": "2024-02-10T00:27:32.000Z",
        "airtable_id": "receTL0NxfwQMdIgW"
    },
    {
        "# Top Paper votes": 0,
        "Blog or Video": "https://ai-alignment.com/mechanistic-anomaly-detection-and-elk-fb84f4c6d0dc",
        "Category": [
            "Scalable oversight",
            "Interpretability / explainability"
        ],
        "Internal source": [
            "Alignment Workshop 2023"
        ],
        "Link": "https://www.youtube.com/watch?v=U2zJuTLzIm8&t=2646s",
        "ML Subfield": [
            "Model Evaluation"
        ],
        "Reviewed by": [
            "AG",
            "VK",
            "PC",
            "RS",
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Scalable oversight",
            "Transparency / Interpretability / Model internals / Latent knowledge"
        ],
        "Safety Topic": [
            "Detection of out-of-distribution or malicious behavior"
        ],
        "Title": "Mechanistic anomaly detection (Paul Christiano, 8-minute video)",
        "Type": [
            "Video"
        ],
        "Vael's Notes": "(5-min talk) Paul Christiano\n",
        "Who tagged for ML": [
            "Viktoria"
        ],
        "airtable_createdTime": "2023-09-08T20:49:33.000Z",
        "airtable_id": "recf3Q7DzIoOxpjtO"
    },
    {
        "# Top Paper votes": 0,
        "Blog or Video": "https://www.alignment-workshop.com/nola-talks/boaz-barak-the-impossibility-of-strong-watermarking",
        "Category": [
            "Reward misspecification and goal misgeneralization"
        ],
        "Internal source": [
            "NOLA Workshop 2023"
        ],
        "Link": "https://arxiv.org/abs/2311.04378",
        "Safety Category": [
            "Reward misspecification and goal misgeneralization",
            "Adversaries / Robustness / Generalization"
        ],
        "Title": "Watermarks in the Sand: Impossibility of Strong Watermarking for Generative Models (Zhang et al., 2023)",
        "Type": [
            "Paper",
            "Video"
        ],
        "Vael's Notes": "Not including the lightning talks for now, since pretty EA probably\n",
        "airtable_createdTime": "2024-02-10T00:27:32.000Z",
        "airtable_id": "recf8Hfa42qqGcyIv"
    },
    {
        "# Top Paper votes": 0,
        "Abstract": "Many intellectual endeavors require mathematical problem solving, but this skill remains beyond the capabilities of computers. To measure this ability in machine learning models, we introduce MATH, a new dataset of 12,500 challenging competition mathematics problems. Each problem in MATH has a full step-by-step solution which can be used to teach models to generate answer derivations and explanations. To facilitate future research and increase accuracy on MATH, we also contribute a large auxiliary pretraining dataset which helps teach models the fundamentals of mathematics. Even though we are able to increase accuracy on MATH, our results show that accuracy remains relatively low, even with enormous Transformer models. Moreover, we find that simply increasing budgets and model parameter counts will be impractical for achieving strong mathematical reasoning if scaling trends continue. While scaling Transformers is automatically solving most other text-based tasks, scaling is not currently solving MATH. To have more traction on mathematical problem solving we will likely need new algorithmic advancements from the broader research community.\n",
        "Category": [
            "Model evaluations and benchmarks"
        ],
        "Internal source": [
            "Elsewhere"
        ],
        "Link": "https://arxiv.org/abs/2103.03874",
        "Safety Category": [
            "Model evaluations / monitoring / detection"
        ],
        "Title": "Measuring Mathematical Problem Solving With the MATH Dataset (Hendrycks et al., 2021)",
        "Type": [
            "Paper"
        ],
        "airtable_createdTime": "2024-03-31T18:06:47.000Z",
        "airtable_id": "recfXTzyVle5BpAzn"
    },
    {
        "# Top Paper votes": 1,
        "AD: Top Paper": true,
        "Category": [
            "Overviews and agendas",
            "Theory",
            "Reinforcement Learning"
        ],
        "Internal source": [
            "NOLA Workshop 2023"
        ],
        "Link": "https://www.alignment-workshop.com/nola-2023#h.qvhufi1ane9m",
        "ML Methods Tag": [
            "Probabilistic Models"
        ],
        "ML Subfield": [
            "Probabilistic Modeling and Bayesian ML",
            "Theory",
            "Reinforcement Learning"
        ],
        "Reviewed by": [
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Major problems (misalignment, misuse, threat models)"
        ],
        "Title": "Towards Quantitative Safety Guarantees and Alignment (Yoshua Bengio, 59-min video)",
        "Type": [
            "Video"
        ],
        "Who tagged for ML": [
            "Viktoria"
        ],
        "airtable_createdTime": "2024-01-15T20:13:24.000Z",
        "airtable_id": "recgWnjdaVonFfsBd"
    },
    {
        "# Top Paper votes": 0,
        "Internal source": [
            "Hendrycks Textbook"
        ],
        "Link": "https://www.aisafetybook.com/textbook/4-1",
        "Safety Category": [
            "Overview"
        ],
        "Title": "Introduction to AI Safety, Ethics, and Society: Chapter 4 - Safety Engineering (Hendrycks, 2023)",
        "Type": [
            "Other"
        ],
        "airtable_createdTime": "2024-03-31T18:06:47.000Z",
        "airtable_id": "recgcYMQ2EuyPFyDF"
    },
    {
        "# Top Paper votes": 0,
        "Abstract": "    Scaling up language models has been shown to predictably improve performance and sample efficiency on a wide range of downstream tasks. This paper instead discusses an unpredictable phenomenon that we refer to as emergent abilities of large language models. We consider an ability to be emergent if it is not present in smaller models but is present in larger models. Thus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models. The existence of such emergence implies that additional scaling could further expand the range of capabilities of language models. \n",
        "Blog or Video": "https://www.youtube.com/watch?v=HiYWwjma3xE&t=169s",
        "Context phrase (please add!)": "\n",
        "Internal source": [
            "Alignment Workshop 2023"
        ],
        "Link": "https://arxiv.org/abs/2206.07682",
        "ML Subfield": [
            "NLP"
        ],
        "ML Subtopic": [
            "NLP: LLMs"
        ],
        "Reviewed by": [
            "AG",
            "VK",
            "PC",
            "RS"
        ],
        "Safety Category": [
            "Capabilities of LLMs",
            "Science of deep learning"
        ],
        "Safety Topic": [
            "Emergence"
        ],
        "Title": "Emergent Abilities of Large Language Models (Wei et al., 2022)",
        "Twitter": "https://twitter.com/_jasonwei/status/1537230731599962112",
        "Type": [
            "Paper"
        ],
        "Who tagged for ML": [
            "Viktoria"
        ],
        "airtable_createdTime": "2024-02-10T00:27:32.000Z",
        "airtable_id": "recgl9pH8AIMUDI44"
    },
    {
        "# Top Paper votes": 0,
        "Blog or Video": "https://www.alignment-workshop.com/nola-talks/victor-veitch-what-and-why-is-a-linear-representation",
        "Category": [
            "Interpretability / explainability"
        ],
        "Internal source": [
            "NOLA Workshop 2023"
        ],
        "Link": "https://arxiv.org/abs/2311.03658",
        "Safety Category": [
            "Transparency / Interpretability / Model internals / Latent knowledge"
        ],
        "Title": "The Linear Representation Hypothesis and the Geometry of Large Language Models (Park et al., 2023)",
        "Type": [
            "Video",
            "Paper"
        ],
        "Vael's Notes": "Not including the lightning talks for now, since pretty EA probably\n",
        "airtable_createdTime": "2024-02-10T00:27:32.000Z",
        "airtable_id": "recgyH7TmJbHFVs1n"
    },
    {
        "# Top Paper votes": 0,
        "Abstract": "Humans are capable of strategically deceptive behavior: behaving helpfully in most situations, but then behaving very differently in order to pursue alternative objectives when given the opportunity. If an AI system learned such a deceptive strategy, could we detect it and remove it using current state-of-the-art safety training techniques? To study this question, we construct proof-of-concept examples of deceptive behavior in large language models (LLMs). For example, we train models that write secure code when the prompt states that the year is 2023, but insert exploitable code when the stated year is 2024. We find that such backdoor behavior can be made persistent, so that it is not removed by standard safety training techniques, including supervised fine-tuning, reinforcement learning, and adversarial training (eliciting unsafe behavior and then training to remove it). The backdoor behavior is most persistent in the largest models and in models trained to produce chain-of-thought reasoning about deceiving the training process, with the persistence remaining even when the chain-of-thought is distilled away. Furthermore, rather than removing backdoors, we find that adversarial training can teach models to better recognize their backdoor triggers, effectively hiding the unsafe behavior. Our results suggest that, once a model exhibits deceptive behavior, standard techniques could fail to remove such deception and create a false impression of safety.\n",
        "Blog or Video": "https://www.anthropic.com/news/sleeper-agents-training-deceptive-llms-that-persist-through-safety-training",
        "Category": [
            "Deception",
            "Non-Adversarial Robustness"
        ],
        "Citations": 13,
        "Goes online as Top Paper": true,
        "Goes online in Expanded Papers": true,
        "Internal source": [
            "Elsewhere"
        ],
        "Link": "https://arxiv.org/abs/2401.05566",
        "ML Subfield": [
            "NLP",
            "Robustness and Adversariality"
        ],
        "ML Subfield (Is Starting Point)": [
            "Robustness and Adversariality"
        ],
        "ML Subtopic (Is Starting Point)": [
            "NLP: LLMs"
        ],
        "Safety Category": [
            "Adversaries / Robustness / Generalization",
            "Model evaluations / monitoring / detection",
            "Deception"
        ],
        "Title": "Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training (Hubinger et al., 2024)\n",
        "Twitter": "https://twitter.com/AnthropicAI/status/1745854907968880970",
        "Type": [
            "Paper"
        ],
        "Who tagged for ML": [
            "Vael"
        ],
        "Year": "2024",
        "airtable_createdTime": "2024-02-14T01:05:57.000Z",
        "airtable_id": "rechItzz4b9rPay1z"
    },
    {
        "# Top Paper votes": 0,
        "Abstract": " It is important that consumers and regulators can verify the provenance of large neural models to evaluate their capabilities and risks. We introduce the concept of a \"Proof-of-Training-Data\": any protocol that allows a model trainer to convince a Verifier of the training data that produced a set of model weights. Such protocols could verify the amount and kind of data and compute used to train the model, including whether it was trained on specific harmful or beneficial data sources. We explore efficient verification strategies for Proof-of-Training-Data that are compatible with most current large-model training procedures. These include a method for the model-trainer to verifiably pre-commit to a random seed used in training, and a method that exploits models' tendency to temporarily overfit to training data in order to detect whether a given data-point was included in training. We show experimentally that our verification procedures can catch a wide variety of attacks, including all known attacks from the Proof-of-Learning literature. \n",
        "Category": [
            "AI Governance",
            "Security",
            "Theory"
        ],
        "Link": "https://arxiv.org/abs/2307.00682",
        "ML Subfield": [
            "Security",
            "Theory"
        ],
        "Reviewed by": [
            "AG",
            "VK",
            "PC",
            "RS",
            "DH",
            "AD"
        ],
        "Safety Category": [
            "AI Governance"
        ],
        "Safety Topic": [
            "Compute governance"
        ],
        "Title": "Tools for Verifying Neural Models\u2019 Training Data (Choi, Shavit and Duvenaud, 2023)",
        "Twitter": "https://twitter.com/DavidDuvenaud/status/1676672532970196993",
        "Type": [
            "Paper"
        ],
        "Who tagged for ML": [
            "Max"
        ],
        "airtable_createdTime": "2023-09-05T22:50:05.000Z",
        "airtable_id": "rechPb1AJIuWQYmfl"
    },
    {
        "# Top Paper votes": 1,
        "AD: Top Paper": true,
        "AG comments": "Appealing but not really x-risk focused. Good if you want broad support but don't care about nuance.\n",
        "Abstract": "Machine learning (ML) systems are rapidly increasing in size, are acquiring new capabilities, and are increasingly deployed in high-stakes settings. As with other powerful technologies, safety for ML should be a leading research priority. In response to emerging safety challenges in ML, such as those introduced by recent large-scale models, we provide a new roadmap for ML Safety and refine the technical problems that the field needs to address. We present four problems ready for research, namely withstanding hazards (\u201cRobustness\u201d), identifying hazards (\u201cMonitoring\u201d), steering ML systems (\u201cAlignment\u201d), and reducing deployment hazards (\u201cSystemic Safety\u201d). Throughout, we clarify each problem\u2019s motivation and provide concrete research directions.\n",
        "Category": [
            "Overviews and agendas"
        ],
        "Citations": 243,
        "DH Comments": "https://drive.google.com/file/d/1CGQPn9Srs-toFVhZZsem1aH7q7q2OL3e/view",
        "Goes online in Expanded Papers": true,
        "Internal source": [
            "Elsewhere"
        ],
        "Link": "https://arxiv.org/abs/2109.13916",
        "ML Subfield": [
            "Robustness and Adversariality",
            "Theory",
            "Applied ML",
            "Human Model Interaction"
        ],
        "Reviewed by": [
            "AG",
            "VK",
            "PC",
            "RS",
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Major problems (misalignment, misuse, threat models)",
            "Overview"
        ],
        "Title": "Unsolved Problems in ML safety (Hendrycks et al., 2021)",
        "Type": [
            "Paper"
        ],
        "Vael's Notes": "(50 mins)\n",
        "Who tagged for ML": [
            "Max"
        ],
        "Year": "2021",
        "airtable_createdTime": "2023-09-05T22:50:05.000Z",
        "airtable_id": "rechY6CLgSysOy27G"
    },
    {
        "# Top Paper votes": 0,
        "Abstract": " Concept erasure aims to remove specified features from a representation. It can improve fairness (e.g. preventing a classifier from using gender or race) and interpretability (e.g. removing a concept to observe changes in model behavior). We introduce LEAst-squares Concept Erasure (LEACE), a closed-form method which provably prevents all linear classifiers from detecting a concept while changing the representation as little as possible, as measured by a broad class of norms. We apply LEACE to large language models with a novel procedure called \"concept scrubbing,\" which erases target concept information from every layer in the network. We demonstrate our method on two tasks: measuring the reliance of language models on part-of-speech information, and reducing gender bias in BERT embeddings. Code is available at this https URL. \n",
        "Category": [
            "Theory",
            "Unlearning and Knowledge Editing"
        ],
        "Citations": 41,
        "Goes online in Expanded Papers": true,
        "Internal source": [
            "Hendrycks Textbook"
        ],
        "Link": "https://arxiv.org/abs/2306.03819",
        "ML Subfield": [
            "Applied ML",
            "Theory"
        ],
        "ML Subfield (Is Starting Point)": [
            "Theory"
        ],
        "Reviewed by": [
            "PC",
            "RS",
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Transparency / Interpretability / Model internals / Latent knowledge"
        ],
        "Safety Topic": [
            "Model editing"
        ],
        "Title": "LEACE: Perfect linear concept erasure in closed form (Belrose et al., 2023)",
        "Type": [
            "Paper"
        ],
        "Who tagged for ML": [
            "Viktoria"
        ],
        "Year": "2023",
        "airtable_createdTime": "2023-12-12T20:00:03.000Z",
        "airtable_id": "rechcPTOws4aTwAN7"
    },
    {
        "# Top Paper votes": 0,
        "Abstract": "Large language models (LLMs) perform bet-ter when they produce step-by-step, \u201cChain-of-Thought\u201d (CoT) reasoning before answering aquestion, but it is unclear if the stated reason-ing is a faithful explanation of the model\u2019s actualreasoning (i.e., its process for answering the ques-tion). We investigate hypotheses for how CoTreasoning may be unfaithful, by examining howthe model predictions change when we interveneon the CoT (e.g., by adding mistakes or paraphras-ing it). Models show large variation across tasksin how strongly they condition on the CoT whenpredicting their answer, sometimes relying heav-ily on the CoT and other times primarily ignoringit. CoT\u2019s performance boost does not seem tocome from CoT\u2019s added test-time compute aloneor from information encoded via the particularphrasing of the CoT. As models become largerand more capable, they produce less faithful rea-soning on most tasks we study. Overall, our re-sults suggest that CoT can be faithful if the cir-cumstances such as the model size and task arecarefully chosen.\n",
        "Category": [
            "Deception"
        ],
        "Internal source": [
            "AWAIR"
        ],
        "Link": "https://www-files.anthropic.com/production/files/measuring-faithfulness-in-chain-of-thought-reasoning.pdf",
        "ML Subfield": [
            "Applied ML: Cognitive Tasks"
        ],
        "ML Subtopic": [
            "NLP",
            "Applied ML",
            "Model Evaluation",
            "Human Model Interaction"
        ],
        "Reviewed by": [
            "AG",
            "VK",
            "PC",
            "RS"
        ],
        "Safety Category": [
            "Deception"
        ],
        "Safety Topic": [
            "Sycophancy"
        ],
        "Title": "Measuring Faithfulness in Chain-of-Thought Reasoning (Lanham et al., 2023)\n",
        "Type": [
            "Paper"
        ],
        "Who tagged for ML": [
            "Max"
        ],
        "airtable_createdTime": "2024-02-10T00:27:32.000Z",
        "airtable_id": "reciDoybK8KIVHnT6"
    },
    {
        "# Top Paper votes": 0,
        "Abstract": "We explore machine unlearning (MU) in the domain of large language models (LLMs), referred to as LLM unlearning. This initiative aims to eliminate undesirable data influence (e.g., sensitive or illegal information) and the associated model capabilities, while maintaining the integrity of essential knowledge generation and not affecting causally unrelated information. We envision LLM unlearning becoming a pivotal element in the life-cycle management of LLMs, potentially standing as an essential foundation for developing generative AI that is not only safe, secure, and trustworthy, but also resource-efficient without the need of full retraining. We navigate the unlearning landscape in LLMs from conceptual formulation, methodologies, metrics, and applications. In particular, we highlight the often-overlooked aspects of existing LLM unlearning research, e.g., unlearning scope, data-model interaction, and multifaceted efficacy assessment. We also draw connections between LLM unlearning and related areas such as model editing, influence functions, model explanation, adversarial training, and reinforcement learning. Furthermore, we outline an effective assessment framework for LLM unlearning and explore its applications in copyright and privacy safeguards and sociotechnical harm reduction.\n",
        "Category": [
            "Unlearning and Knowledge Editing"
        ],
        "Goes online in Expanded Papers": true,
        "Link": "https://arxiv.org/abs/2402.08787",
        "Title": "Rethinking Machine Unlearning for Large Language Models",
        "Type": [
            "Paper"
        ],
        "Year": "2024",
        "airtable_createdTime": "2024-08-09T15:28:34.000Z",
        "airtable_id": "reciNyQCTi1Bx5doO"
    },
    {
        "# Top Paper votes": 0,
        "Blog or Video": "https://www.alignment-workshop.com/nola-talks/sanjeev-arora-emergence-of-complex-skills-in-llms-and-do-they-understan",
        "Category": [
            "Interpretability / explainability"
        ],
        "Internal source": [
            "NOLA Workshop 2023"
        ],
        "Link": "https://arxiv.org/abs/2307.15936",
        "Safety Category": [
            "Transparency / Interpretability / Model internals / Latent knowledge"
        ],
        "Title": "A Theory of Emergence of Complex Skills in Language Models (Arora and Goyal, 2023)",
        "Type": [
            "Paper",
            "Video"
        ],
        "Vael's Notes": "Not including the lightning talks for now, since pretty EA probably\n",
        "airtable_createdTime": "2024-02-10T00:27:32.000Z",
        "airtable_id": "recjb15bNNB95vmcV"
    },
    {
        "# Top Paper votes": 0,
        "Abstract": "    We consider the two related problems of detecting if an example is misclassified or out-of-distribution. We present a simple baseline that utilizes probabilities from softmax distributions. Correctly classified examples tend to have greater maximum softmax probabilities than erroneously classified and out-of-distribution examples, allowing for their detection. We assess performance by defining several tasks in computer vision, natural language processing, and automatic speech recognition, showing the effectiveness of this baseline across all. We then show the baseline can sometimes be surpassed, demonstrating the room for future research on these underexplored detection tasks. \n",
        "Category": [
            "Model evaluations and benchmarks"
        ],
        "Internal source": [
            "Intro to ML Safety Course"
        ],
        "Link": "https://arxiv.org/abs/1610.02136",
        "ML Subfield": [
            "Model Evaluation",
            "Domain General"
        ],
        "Reviewed by": [
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Model evaluations / monitoring / detection"
        ],
        "Safety Topic": [
            "Detection of out-of-distribution or malicious behavior"
        ],
        "Title": "A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks (Hendrycks and Gimpel, 2017)",
        "Type": [
            "Paper"
        ],
        "Who tagged for ML": [
            "Viktoria"
        ],
        "airtable_createdTime": "2024-01-15T21:36:26.000Z",
        "airtable_id": "reck5acwQjT9KzrSo"
    },
    {
        "# Top Paper votes": 0,
        "Abstract": "Transformer-based large language models are making significant strides in various fields, such as natural language processing1,2,3,4,5, biology6,7, chemistry8,9,10 and computer programming11,12. Here, we show the development and capabilities of Coscientist, an artificial intelligence system driven by GPT-4 that autonomously designs, plans and performs complex experiments by incorporating large language models empowered by tools such as internet and documentation search, code execution and experimental automation. Coscientist showcases its potential for accelerating research across six diverse tasks, including the successful reaction optimization of palladium-catalysed cross-couplings, while exhibiting advanced capabilities for (semi-)autonomous experimental design and execution. Our findings demonstrate the versatility, efficacy and explainability of artificial intelligence systems like Coscientist in advancing research.\n",
        "Internal source": [
            "Elsewhere"
        ],
        "Link": "https://www.nature.com/articles/s41586-023-06792-0",
        "ML Subfield": [
            "Applied ML"
        ],
        "ML Subfield (Is Starting Point)": [
            "Applied ML"
        ],
        "ML Subtopic": [
            "NLP: LLMs",
            "Applied ML: Cognitive Tasks",
            "Applied ML: AI-Based Automation"
        ],
        "Reviewed by": [
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Capabilities of LLMs"
        ],
        "Title": "Autonomous chemical research with large language models (Boike et al., 2023)",
        "Type": [
            "Paper"
        ],
        "Who tagged for ML": [
            "Max"
        ],
        "airtable_createdTime": "2023-12-27T21:30:47.000Z",
        "airtable_id": "recknVtvSFUPpSlQf"
    },
    {
        "# Top Paper votes": 0,
        "Internal source": [
            "NOLA Workshop 2023"
        ],
        "Title": "Provably Safe AI (Max Tegmark) -- Recording coming soon",
        "Vael's Notes": "Not including the lightning talks for now, since pretty EA probably\n",
        "airtable_createdTime": "2024-02-10T00:27:32.000Z",
        "airtable_id": "recloYG2hTa0BjX8X"
    },
    {
        "# Top Paper votes": 0,
        "Abstract": "\n",
        "Category": [
            "Reward misspecification and goal misgeneralization",
            "Scalable oversight"
        ],
        "Link": "https://openai.com/blog/learning-to-summarize-with-human-feedback/",
        "ML Subfield": [
            "NLP",
            "Human Model Interaction",
            "Applied ML"
        ],
        "ML Subtopic": [
            "NLP: Summarization"
        ],
        "Reviewed by": [
            "AG",
            "VK",
            "PC",
            "RS"
        ],
        "Safety Category": [
            "Alignment <-> RLHF",
            "Reward misspecification and goal misgeneralization"
        ],
        "Time from MAIA / AISF / Elsewhere (min)": "15",
        "Title": "Learning to summarize with human feedback (Stiennon et al., 2020) ",
        "Type": [
            "Blog post"
        ],
        "Who tagged for ML": [
            "Max"
        ],
        "airtable_createdTime": "2024-02-10T00:27:32.000Z",
        "airtable_id": "recm2aB9yXAv8HGB0"
    },
    {
        "# Top Paper votes": 1,
        "AD: Top Paper": true,
        "Abstract": "The widespread public deployment of large language models (LLMs) in recent months has prompted a wave of new attention and engagement from advocates, policymakers, and scholars from many fields. This attention is a timely response to the many urgent questions that this technology raises, but it can sometimes miss important considerations. This paper surveys the evidence for eight potentially surprising such points: \n1\\. LLMs predictably get more capable with increasing investment, even without targeted innovation.\n 2\\. Many important LLM behaviors emerge unpredictably as a byproduct of increasing investment.\n 3\\. LLMs often appear to learn and use representations of the outside world.\n 4\\. There are no reliable techniques for steering the behavior of LLMs.\n 5\\. Experts are not yet able to interpret the inner workings of LLMs. 6. Human performance on a task isn\u2019t an upper bound on LLM performance. \n7\\. LLMs need not express the values of their creators nor the values encoded in web text. 8. Brief interactions with LLMs are often misleading.\n",
        "Category": [
            "Why large-scale safety?"
        ],
        "Citations": 132,
        "Goes online in Expanded Papers": true,
        "Internal source": [
            "Twitter"
        ],
        "Link": "https://arxiv.org/abs/2304.00612",
        "ML Subfield": [
            "NLP"
        ],
        "ML Subtopic": [
            "NLP: LLMs"
        ],
        "Reviewed by": [
            "AG",
            "VK",
            "PC",
            "RS",
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Capabilities of LLMs",
            "Major problems (misalignment, misuse, threat models)"
        ],
        "Title": "Eight Things to Know about Large Language Models (Bowman, 2023)",
        "Twitter": "https://twitter.com/sleepinyourhat/status/1642614846796734464",
        "Type": [
            "Paper"
        ],
        "Who tagged for ML": [
            "Max"
        ],
        "Year": "2023",
        "airtable_createdTime": "2023-09-12T20:37:24.000Z",
        "airtable_id": "recm4kzvpwNbQtHCQ"
    },
    {
        "# Top Paper votes": 0,
        "Abstract": "Rapid advancements in artificial intelligence (AI) have sparked growing concerns among experts, policymakers, and world leaders regarding the potential for increasingly advanced AI systems to pose catastrophic risks. Although numerous risks have been detailed separately, there is a pressing need for a systematic discussion and illustration of the potential dangers to better inform efforts to mitigate them. This paper provides an overview of the main sources of catastrophic AI risks, which we organize into four categories: malicious use, in which individuals or groups intentionally use AIs to cause harm; AI race, in which competitive environments compel actors to deploy unsafe AIs or cede control to AIs; organizational risks, highlighting how human factors and complex systems can increase the chances of catastrophic accidents; and rogue AIs, describing the inherent difficulty in controlling agents far more intelligent than humans. For each category of risk, we describe specific hazards, present illustrative stories, envision ideal scenarios, and propose practical suggestions for mitigating these dangers. Our goal is to foster a comprehensive understanding of these risks and inspire collective and proactive efforts to ensure that AIs are developed and deployed in a safe manner. Ultimately, we hope this will allow us to realize the benefits of this powerful technology while minimizing the potential for catastrophic outcomes.\n",
        "Category": [
            "Overviews and agendas",
            "Why large-scale safety?"
        ],
        "Citations": 71,
        "Goes online as Top Paper": true,
        "Goes online in Expanded Papers": true,
        "Internal source": [
            "Elsewhere"
        ],
        "Link": "https://www.safe.ai/ai-risk",
        "ML Subfield": [
            "Domain General",
            "Applied ML"
        ],
        "Reviewed by": [
            "AG",
            "VK",
            "PC",
            "RS",
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Major problems (misalignment, misuse, threat models)",
            "Overview"
        ],
        "Title": "An Overview of Catastrophic AI Risks (Hendrycks et al., 2023)",
        "Transcripts / Audio / Slides": "https://www.safe.ai/ai-risk",
        "Twitter": "https://x.com/DanHendrycks/status/1671894767331061763",
        "Type": [
            "Blog post",
            "Paper"
        ],
        "Who tagged for ML": [
            "Max"
        ],
        "Year": "2023",
        "airtable_createdTime": "2023-09-05T22:50:05.000Z",
        "airtable_id": "recmELABdVyEpj8r5"
    },
    {
        "# Top Paper votes": 0,
        "AG comments": "Too inside baseball\n",
        "Category": [
            "Scalable oversight",
            "Interpretability / explainability"
        ],
        "Internal source": [
            "AWAIR"
        ],
        "Link": "https://docs.google.com/document/d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8/edit#heading=h.kkaua0hwmp1d",
        "Reviewed by": [
            "AG",
            "VK"
        ],
        "Safety Category": [
            "Scalable oversight",
            "Transparency / Interpretability / Model internals / Latent knowledge"
        ],
        "Title": "Eliciting latent knowledge: How to tell if your eyes deceive you (Christiano, Cotra and Xu, 2021)\n",
        "Type": [
            "Paper"
        ],
        "VK comments": "Too long and hard to follow for people outside alignment\n",
        "airtable_createdTime": "2024-02-10T00:27:32.000Z",
        "airtable_id": "recmVN31r7NhTSfV0"
    },
    {
        "# Top Paper votes": 0,
        "Abstract": "Optimizing large language models (LLMs) for downstream use cases often involves the customization of pre-trained LLMs through further fine-tuning. Meta's open release of Llama models and OpenAI's APIs for fine-tuning GPT-3.5 Turbo on custom datasets also encourage this practice. But, what are the safety costs associated with such custom fine-tuning? We note that while existing safety alignment infrastructures can restrict harmful behaviors of LLMs at inference time, they do not cover safety risks when fine-tuning privileges are extended to end-users. Our red teaming studies find that the safety alignment of LLMs can be compromised by fine-tuning with only a few adversarially designed training examples. For instance, we jailbreak GPT-3.5 Turbo's safety guardrails by fine-tuning it on only 10 such examples at a cost of less than $0.20 via OpenAI's APIs, making the model responsive to nearly any harmful instructions. Disconcertingly, our research also reveals that, even without malicious intent, simply fine-tuning with benign and commonly used datasets can also inadvertently degrade the safety alignment of LLMs, though to a lesser extent. These findings suggest that fine-tuning aligned LLMs introduces new safety risks that current safety infrastructures fall short of addressing -- even if a model's initial safety alignment is impeccable, it is not necessarily to be maintained after custom fine-tuning. We outline and critically analyze potential mitigations and advocate for further research efforts toward reinforcing safety protocols for the custom fine-tuning of aligned LLMs. \n",
        "Category": [
            "Scalable oversight"
        ],
        "Citations": 116,
        "Internal source": [
            "Elsewhere"
        ],
        "Link": "https://arxiv.org/abs/2310.03693",
        "Safety Category": [
            "Alignment <-> RLHF"
        ],
        "Title": "Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To! (Qi et al., 2023)",
        "Type": [
            "Paper"
        ],
        "Year": "2023",
        "airtable_createdTime": "2024-03-31T21:00:52.000Z",
        "airtable_id": "recmZDPcFZwDa1FGm"
    },
    {
        "# Top Paper votes": 0,
        "AD comments": "\n",
        "Abstract": "    This paper studies extractable memorization: training data that an adversary can efficiently extract by querying a machine learning model without prior knowledge of the training dataset. We show an adversary can extract gigabytes of training data from open-source language models like Pythia or GPT-Neo, semi-open models like LLaMA or Falcon, and closed models like ChatGPT. Existing techniques from the literature suffice to attack unaligned models; in order to attack the aligned ChatGPT, we develop a new divergence attack that causes the model to diverge from its chatbot-style generations and emit training data at a rate 150x higher than when behaving properly. Our methods show practical attacks can recover far more data than previously thought, and reveal that current alignment techniques do not eliminate memorization. \n",
        "Category": [
            "Non-Adversarial Robustness",
            "Security"
        ],
        "Citations": 77,
        "Description from MAIA / AISF / Elsewhere": "[Paper] extracted several megabytes of training data from GPT-3.5 by prompting it to repeat the word \u201cpoem\u201d forever. The authors conjecture that this prompt leads to input unlike those the model saw during fine-tuning, and therefore causes the model to revert to its original language modeling objective.",
        "Internal source": [
            "ML Newsletter"
        ],
        "Link": "https://arxiv.org/abs/2311.17035",
        "ML Subfield": [
            "Robustness and Adversariality",
            "Security",
            "Applied ML"
        ],
        "ML Subfield (Is Starting Point)": [
            "Security",
            "Applied ML"
        ],
        "ML Subtopic": [
            "NLP: LLMs"
        ],
        "Reviewed by": [
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Adversaries / Robustness / Generalization"
        ],
        "Title": "Scalable Extraction of Training Data from (Production) Language Models (Nasr et al., 2023)",
        "Type": [
            "Paper"
        ],
        "Who tagged for ML": [
            "Viktoria"
        ],
        "Year": "2023",
        "airtable_createdTime": "2023-12-20T01:45:16.000Z",
        "airtable_id": "recmhCy2CTt8ioXRZ"
    },
    {
        "# Top Paper votes": 0,
        "Category": [
            "Overviews and agendas"
        ],
        "Internal source": [
            "Elsewhere"
        ],
        "Link": "https://bounded-regret.ghost.io/complex-systems-are-hard-to-control/",
        "ML Subfield": [
            "Applied ML",
            "Human Model Interaction"
        ],
        "Reviewed by": [
            "PC",
            "RS",
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Major problems (misalignment, misuse, threat models)"
        ],
        "Title": "Complex Systems are Hard to Control (Steinhart, 2023)",
        "Type": [
            "Blog post"
        ],
        "Who tagged for ML": [
            "Max"
        ],
        "Year": "2023",
        "airtable_createdTime": "2023-12-12T20:00:03.000Z",
        "airtable_id": "recmijQu23oMy4kBC"
    },
    {
        "# Top Paper votes": 0,
        "Internal source": [
            "Hendrycks Textbook"
        ],
        "Link": "https://sarahconstantin.substack.com/p/scaling-laws-for-ai-and-some-implications",
        "ML Subfield": [
            "Applied ML"
        ],
        "ML Subfield (Is Starting Point)": [
            "Applied ML"
        ],
        "ML Subtopic": [
            "Applied ML: Efficiency and Hardware",
            "NLP: LLMs"
        ],
        "Reviewed by": [
            "PC",
            "RS",
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Forecasting"
        ],
        "Title": "\"Scaling Laws\" for AI and Some Implications (Constantin, 2023)",
        "Type": [
            "Blog post"
        ],
        "Who tagged for ML": [
            "Max"
        ],
        "airtable_createdTime": "2023-12-12T19:59:59.000Z",
        "airtable_id": "recmjjpCXdo0xrJoS"
    },
    {
        "# Top Paper votes": 0,
        "Category": [
            "Overviews and agendas"
        ],
        "Internal source": [
            "Alignment Workshop 2023"
        ],
        "Link": "https://www.youtube.com/watch?v=m1gbzNQ4JRI&t=2249s",
        "ML Subfield": [
            "NLP"
        ],
        "ML Subtopic": [
            "NLP: LLMs"
        ],
        "Reviewed by": [
            "AG",
            "VK",
            "PC",
            "RS",
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Major problems (misalignment, misuse, threat models)",
            "Overview"
        ],
        "Title": "Aligning Massive Models: Current and Future Challenges (Jacob Steinhardt, 66-min video)",
        "Transcripts / Audio / Slides": "https://jsteinhardt.stat.berkeley.edu/talks/satml/tutorial.html#slideIndex=0&level=0",
        "Type": [
            "Video"
        ],
        "Vael's Notes": "Jacob Steinhardt \u201cAligning Massive Models: Current and Future Challenges\u201d: \nMain approaches to alignment: \n\\- RLHF (\u201crefining human feedback\u201d)\n\\- ELK (\u201cdiscovering latent knowledge\u201d) <\u2014 talks about CCS Burns paper, mentions Halawi et al. (2022) \u201cOverthinking the Truth\u201d and Hernandex et al. (2022), touches on mechanistic interpretability. Problem: not scalable\n\\- other methods + evals / red-teaming\u00a0(\u201cbeyond language models\u201d)\n\u2014 note: not the best video because lots of inaudible questions and rushes through the end slides\n",
        "Who tagged for ML": [
            "Viktoria"
        ],
        "airtable_createdTime": "2023-09-06T18:51:10.000Z",
        "airtable_id": "recnki5KIgu9p0601"
    },
    {
        "# Top Paper votes": 0,
        "Internal source": [
            "NOLA Workshop 2023"
        ],
        "Title": "Sociotechnical AI safety (David Krueger) -- Recording coming soon",
        "Vael's Notes": "Not including the lightning talks for now, since pretty EA probably\n",
        "airtable_createdTime": "2024-02-10T00:27:32.000Z",
        "airtable_id": "reco0zogLH3FyuNj4"
    },
    {
        "# Top Paper votes": 2,
        "AD: Top Paper": true,
        "AG comments": "Good for people doing LLM work, even if not value alignment focused, since RLHF is so well known at this point\n",
        "Abstract": "Reinforcement learning from human feedback (RLHF) is a technique for training AI systems to align with human goals. RLHF has emerged as the central method used to finetune state-of-the-art large language models (LLMs). Despite this popularity, there has been relatively little public work systematizing its flaws. In this paper, we (1) survey open problems and fundamental limitations of RLHF and related methods; (2) overview techniques to understand, improve, and complement RLHF in practice; and (3) propose auditing and disclosure standards to improve societal oversight of RLHF systems. Our work emphasizes the limitations of RLHF and highlights the importance of a multi-faceted approach to the development of safer AI systems. \n",
        "Category": [
            "Reward misspecification and goal misgeneralization",
            "Reinforcement Learning"
        ],
        "Citations": 184,
        "Goes online in Expanded Papers": true,
        "High Citation and Max Select": true,
        "Internal source": [
            "Elsewhere"
        ],
        "Link": "https://arxiv.org/abs/2307.15217",
        "ML Methods Tag": [
            "RLHF"
        ],
        "ML Subfield": [
            "Reinforcement Learning",
            "Applied ML",
            "Human Model Interaction"
        ],
        "ML Subfield (Is Starting Point)": [
            "Reinforcement Learning"
        ],
        "ML Subtopic (Is Starting Point)": [
            "NLP: LLMs"
        ],
        "Reviewed by": [
            "AG",
            "VK",
            "PC",
            "RS",
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Reward misspecification and goal misgeneralization",
            "Alignment <-> RLHF"
        ],
        "Title": "Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback (Casper et al., 2023)",
        "Twitter": "https://twitter.com/StephenLCasper/status/1686036515653361664",
        "Type": [
            "Paper"
        ],
        "VK comments": "Good to share with ML people who think that human feedback is sufficient for alignment\n",
        "VK: Top Paper": true,
        "Vael's Notes": "Twitter was in comments\n",
        "Who tagged for ML": [
            "Max"
        ],
        "Year": "2023",
        "airtable_createdTime": "2023-09-05T22:50:05.000Z",
        "airtable_id": "reco364BeosUasjep"
    },
    {
        "# Top Paper votes": 2,
        "AD: Top Paper": true,
        "Abstract": "As language models (LMs) scale, they develop many novel behaviors, good and bad, exacerbating the need to evaluate how they behave. Prior work creates evaluations with crowdwork (which is time-consuming and expensive) or existing data sources (which are not always available). Here, we automatically generate evaluations with LMs. We explore approaches with varying amounts of human effort, from instructing LMs to write yes/no questions to making complex Winogender schemas with multiple stages of LM-based generation and filtering. Crowdworkers rate the examples as highly relevant and agree with 90-100% of labels, sometimes more so than corresponding human-written datasets. We generate 154 datasets and discover new cases of inverse scaling where LMs get worse with size. Larger LMs repeat back a dialog user's preferred answer (\"sycophancy\") and express greater desire to pursue concerning goals like resource acquisition and goal preservation. We also find some of the first examples of inverse scaling in RL from Human Feedback (RLHF), where more RLHF makes LMs worse. For example, RLHF makes LMs express stronger political views (on gun rights and immigration) and a greater desire to avoid shut down. Overall, LM-written evaluations are high-quality and let us quickly discover many novel LM behaviors. \n",
        "Blog or Video": "https://www.youtube.com/watch?v=HiYWwjma3xE&t=1687s",
        "Category": [
            "Model evaluations and benchmarks",
            "Scalable oversight"
        ],
        "Citations": 133,
        "Description from MAIA / AISF / Elsewhere": "The authors use language models to generate datasets for behaviorally evaluating language models. Some evidence of potentially risky model tendencies \u2013 like expressing desire not to be shut off, or acting sycophantically towards the user \u2013 is found.",
        "Goes online as Top Paper": true,
        "Goes online in Expanded Papers": true,
        "High Citation and Max Select": true,
        "Internal source": [
            "AWAIR",
            "Alignment Workshop 2023",
            "Further reading on MAIA/HAIST Summer 2023 Curriculum"
        ],
        "Link": "https://arxiv.org/abs/2212.09251",
        "ML Subfield": [
            "Model Evaluation",
            "NLP"
        ],
        "ML Subfield (Is Starting Point)": [
            "NLP"
        ],
        "ML Subtopic": [
            "NLP: LLMs"
        ],
        "Reviewed by": [
            "AG",
            "VK",
            "PC",
            "RS",
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Model evaluations / monitoring / detection",
            "Scalable oversight"
        ],
        "Supplemental Material": "https://www.evals.anthropic.com/",
        "Time from MAIA / AISF / Elsewhere (min)": "40",
        "Title": "Discovering Language Model Behaviors with Model-Written Evaluations (Perez et al., 2023)",
        "Twitter": "https://x.com/AnthropicAI/status/1604883576218341376?lang=en",
        "Type": [
            "Paper"
        ],
        "VK comments": "Great evals paper that covers both alignment and sociotechnical safety evaluations, accessible to a broad audience\n",
        "VK: Top Paper": true,
        "Vael's Notes": "\n",
        "Who tagged for ML": [
            "Viktoria"
        ],
        "Year": "2023",
        "airtable_createdTime": "2023-09-05T22:50:05.000Z",
        "airtable_id": "reco7RocehTQKS7wp"
    },
    {
        "# Top Paper votes": 0,
        "Category": [
            "Overviews and agendas"
        ],
        "Internal source": [
            "AWAIR"
        ],
        "Link": "https://www.alignmentforum.org/posts/AyNHoTWWAJ5eb99ji/another-outer-alignment-failure-story",
        "Safety Category": [
            "Major problems (misalignment, misuse, threat models)"
        ],
        "Title": "Another (outer) alignment failure story (Christiano, 2021)",
        "Type": [
            "Blog post"
        ],
        "airtable_createdTime": "2024-02-10T00:27:32.000Z",
        "airtable_id": "recoDkuLKraEWzWhJ"
    },
    {
        "# Top Paper votes": 0,
        "Abstract": "We hope to see further work in the high-stakes reliability setting, including more powerful tools for enhancing human adversaries and better ways to measure high levels of reliability, until we can confidently rule out the possibility of catastrophic deployment-time failures of powerful models. \n",
        "Category": [
            "Interpretability / explainability"
        ],
        "Internal source": [
            "AWAIR"
        ],
        "Link": "https://arxiv.org/pdf/2306.03341.pdf",
        "ML Subfield": [
            "NLP: LLMs"
        ],
        "ML Subtopic": [
            "NLP"
        ],
        "Reviewed by": [
            "AG",
            "VK",
            "PC",
            "RS"
        ],
        "Safety Category": [
            "Transparency / Interpretability / Model internals / Latent knowledge"
        ],
        "Safety Topic": [
            "Model editing"
        ],
        "Title": "Inference-Time Intervention: Eliciting Truthful Answers from a Language Model (Li et al., 2023)",
        "Twitter": "https://twitter.com/ke_li_2021/status/1666810649526308867",
        "Type": [
            "Paper"
        ],
        "Who tagged for ML": [
            "Viktoria"
        ],
        "airtable_createdTime": "2024-02-10T00:27:32.000Z",
        "airtable_id": "recovwveAiw8H1Gjm"
    },
    {
        "# Top Paper votes": 1,
        "AD: Top Paper": true,
        "Category": [
            "Model evaluations and benchmarks"
        ],
        "Link": "https://www.youtube.com/watch?v=Vb5g7jlNzOk&t=438s",
        "ML Subfield": [
            "Model Evaluation"
        ],
        "ML Subtopic": [
            "NLP: LLMs",
            "Applied ML: Chatbots"
        ],
        "Reviewed by": [
            "AG",
            "VK",
            "PC",
            "RS",
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Model evaluations / monitoring / detection"
        ],
        "Time from MAIA / AISF / Elsewhere (min)": "5",
        "Title": "Safety evaluations and standards for AI (Beth Barnes, 32-min video)",
        "Type": [
            "Video"
        ],
        "What sections to read from MAIA": "(7:18-13:55)",
        "Who tagged for ML": [
            "Viktoria"
        ],
        "airtable_createdTime": "2023-09-16T22:35:45.000Z",
        "airtable_id": "recoxV1RKTVpbgo2g"
    },
    {
        "# Top Paper votes": 0,
        "Abstract": "    Large language models trained for safety and harmlessness remain susceptible to adversarial misuse, as evidenced by the prevalence of \"jailbreak\" attacks on early releases of ChatGPT that elicit undesired behavior. Going beyond recognition of the issue, we investigate why such attacks succeed and how they can be created. We hypothesize two failure modes of safety training: competing objectives and mismatched generalization. Competing objectives arise when a model's capabilities and safety goals conflict, while mismatched generalization occurs when safety training fails to generalize to a domain for which capabilities exist. We use these failure modes to guide jailbreak design and then evaluate state-of-the-art models, including OpenAI's GPT-4 and Anthropic's Claude v1.3, against both existing and newly designed attacks. We find that vulnerabilities persist despite the extensive red-teaming and safety-training efforts behind these models. Notably, new attacks utilizing our failure modes succeed on every prompt in a collection of unsafe requests from the models' red-teaming evaluation sets and outperform existing ad hoc jailbreaks. Our analysis emphasizes the need for safety-capability parity -- that safety mechanisms should be as sophisticated as the underlying model -- and argues against the idea that scaling alone can resolve these safety failure modes. \n",
        "Category": [
            "Security",
            "Adversarial Robustness"
        ],
        "Citations": 268,
        "Goes online in Expanded Papers": true,
        "Internal source": [
            "AWAIR"
        ],
        "Link": "https://arxiv.org/abs/2307.02483",
        "ML Subfield": [
            "NLP",
            "Model Evaluation",
            "Robustness and Adversariality",
            "Applied ML"
        ],
        "ML Subfield (Is Starting Point)": [
            "Robustness and Adversariality",
            "Applied ML"
        ],
        "ML Subtopic": [
            "Applied ML: Chatbots",
            "NLP: LLMs"
        ],
        "ML Subtopic (Is Starting Point)": [
            "NLP: LLMs"
        ],
        "Reviewed by": [
            "AG",
            "VK",
            "PC",
            "RS",
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Adversaries / Robustness / Generalization",
            "Model evaluations / monitoring / detection"
        ],
        "Title": "Jailbroken: How Does LLM Safety Training Fail? (Wei et al., 2023)",
        "Type": [
            "Paper"
        ],
        "Who tagged for ML": [
            "Viktoria"
        ],
        "Year": "2023",
        "airtable_createdTime": "2023-09-12T00:20:13.000Z",
        "airtable_id": "recpxvedtlDJ51O12"
    },
    {
        "# Top Paper votes": 0,
        "Abstract": "  A number of leading AI companies, including OpenAI, Google DeepMind, and Anthropic, have the stated goal of building artificial general intelligence (AGI) - AI systems that achieve or exceed human performance across a wide range of cognitive tasks. In pursuing this goal, they may develop and deploy AI systems that pose particularly significant risks. While they have already taken some measures to mitigate these risks, best practices have not yet emerged. To support the identification of best practices, we sent a survey to 92 leading experts from AGI labs, academia, and civil society and received 51 responses. Participants were asked how much they agreed with 50 statements about what AGI labs should do. Our main finding is that participants, on average, agreed with all of them. Many statements received extremely high levels of agreement. For example, 98% of respondents somewhat or strongly agreed that AGI labs should conduct pre-deployment risk assessments, dangerous capabilities evaluations, third-party model audits, safety restrictions on model usage, and red teaming. Ultimately, our list of statements may serve as a helpful foundation for efforts to develop best practices, standards, and regulations for AGI labs. \n",
        "Category": [
            "AI Governance"
        ],
        "Citations": 34,
        "Goes online in Expanded Papers": true,
        "Internal source": [
            "Elsewhere"
        ],
        "Link": "https://arxiv.org/abs/2305.07153",
        "ML Subfield": [
            "Security"
        ],
        "Reviewed by": [
            "AG",
            "VK",
            "PC",
            "RS",
            "DH",
            "AD"
        ],
        "Safety Category": [
            "AI Governance"
        ],
        "Title": "Towards best practices in AGI safety and governance: A survey of expert opinion (Schuett et al., 2023)",
        "Twitter": "https://twitter.com/jonasschuett/status/1658025252675366913",
        "Type": [
            "Paper"
        ],
        "Who tagged for ML": [
            "Viktoria"
        ],
        "Year": "2023",
        "airtable_createdTime": "2023-09-13T00:04:27.000Z",
        "airtable_id": "recq7tFp7GgC2a5BI"
    },
    {
        "# Top Paper votes": 0,
        "Category": [
            "Why large-scale safety?"
        ],
        "Goes online in Expanded Papers": true,
        "Internal source": [
            "Elsewhere"
        ],
        "Link": "https://www.youtube.com/watch?v=yl2nlejBcg0",
        "ML Subfield": [
            "Domain General",
            "Human Model Interaction"
        ],
        "Reviewed by": [
            "AG",
            "VK",
            "PC",
            "RS",
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Major problems (misalignment, misuse, threat models)"
        ],
        "Title": "Researcher Perceptions of Current and Future AI (Vael Gates, 60-min video)",
        "Type": [
            "Video"
        ],
        "Vael's Notes": "(first 48m; skip the Q&A) (Transcript)\n",
        "Who tagged for ML": [
            "Max"
        ],
        "airtable_createdTime": "2023-09-05T22:50:05.000Z",
        "airtable_id": "recqzx4JJ0BPBnwOQ"
    },
    {
        "# Top Paper votes": 0,
        "Category": [
            "Scalable oversight"
        ],
        "Internal source": [
            "NOLA Workshop 2023"
        ],
        "Link": "https://www.alignment-workshop.com/nola-talks/dylan-hadfield-menell-preference-learning-in-alignment",
        "Safety Category": [
            "Scalable oversight",
            "Alignment <-> RLHF"
        ],
        "Title": "Preference Learning in Alignment (Dylan Hadfield-Menell, 4-min video)",
        "Type": [
            "Video"
        ],
        "Vael's Notes": "Not including the lightning talks for now, since pretty EA probably\n",
        "airtable_createdTime": "2024-02-10T00:27:32.000Z",
        "airtable_id": "recr6RmDgk3ScwUXt"
    },
    {
        "# Top Paper votes": 0,
        "Abstract": "    We identify obfuscated gradients, a kind of gradient masking, as a phenomenon that leads to a false sense of security in defenses against adversarial examples. While defenses that cause obfuscated gradients appear to defeat iterative optimization-based attacks, we find defenses relying on this effect can be circumvented. We describe characteristic behaviors of defenses exhibiting the effect, and for each of the three types of obfuscated gradients we discover, we develop attack techniques to overcome it. In a case study, examining non-certified white-box-secure defenses at ICLR 2018, we find obfuscated gradients are a common occurrence, with 7 of 9 defenses relying on obfuscated gradients. Our new attacks successfully circumvent 6 completely, and 1 partially, in the original threat model each paper considers. \n",
        "Category": [
            "Adversarial Robustness"
        ],
        "Citations": 3357,
        "High Citation and Max Select": true,
        "Internal source": [
            "Intro to ML Safety Course"
        ],
        "Link": "https://arxiv.org/abs/1802.00420",
        "ML Subfield": [
            "Robustness and Adversariality",
            "Vision"
        ],
        "ML Subfield (Is Starting Point)": [
            "Robustness and Adversariality"
        ],
        "Reviewed by": [
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Adversaries / Robustness / Generalization"
        ],
        "Title": "Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples (Athalye et al., 2018)",
        "Type": [
            "Paper"
        ],
        "Who tagged for ML": [
            "Viktoria"
        ],
        "Year": "2018",
        "airtable_createdTime": "2024-01-15T21:15:52.000Z",
        "airtable_id": "recr9RuuBoXkDDiaA"
    },
    {
        "# Top Paper votes": 0,
        "Abstract": "The White House Executive Order on Artificial Intelligence highlights the risks of large language models (LLMs) empowering malicious actors in developing biological, cyber, and chemical weapons. To measure these risks of malicious use, government institutions and major AI labs are developing evaluations for hazardous capabilities in LLMs. However, current evaluations are private, preventing further research into mitigating risk. Furthermore, they focus on only a few, highly specific pathways for malicious use. To fill these gaps, we publicly release the Weapons of Mass Destruction Proxy (WMDP) benchmark, a dataset of 4,157 multiple-choice questions that serve as a proxy measurement of hazardous knowledge in biosecurity, cybersecurity, and chemical security. WMDP was developed by a consortium of academics and technical consultants, and was stringently filtered to eliminate sensitive information prior to public release. WMDP serves two roles: first, as an evaluation for hazardous knowledge in LLMs, and second, as a benchmark for unlearning methods to remove such hazardous knowledge. To guide progress on unlearning, we develop CUT, a state-of-the-art unlearning method based on controlling model representations. CUT reduces model performance on WMDP while maintaining general capabilities in areas such as biology and computer science, suggesting that unlearning may be a concrete path towards reducing malicious use from LLMs. We release our benchmark and code publicly at [this https URL](https://wmdp.ai).\n\n",
        "Blog or Video": "https://www.safe.ai/blog/wmdp-benchmark",
        "Category": [
            "Model evaluations and benchmarks",
            "Unlearning and Knowledge Editing"
        ],
        "Citations": 6,
        "Goes online in Expanded Papers": true,
        "Internal source": [
            "Elsewhere"
        ],
        "Link": "https://arxiv.org/abs/2403.03218",
        "ML Subfield": [
            "Model Evaluation",
            "Transparency"
        ],
        "ML Subtopic": [
            "NLP: LLMs"
        ],
        "Safety Category": [
            "Model evaluations / monitoring / detection",
            "Transparency / Interpretability / Model internals / Latent knowledge"
        ],
        "Safety Topic": [
            "Benchmarking"
        ],
        "Title": "The WMDP Benchmark: Measuring and Reducing Malicious Use With Unlearning (Li et al., 2024)",
        "Type": [
            "Paper"
        ],
        "Who tagged for ML": [
            "Vael"
        ],
        "Year": "2024",
        "airtable_createdTime": "2024-03-30T19:34:28.000Z",
        "airtable_id": "recrFmZZkgmjX7YSQ"
    },
    {
        "# Top Paper votes": 0,
        "Abstract": "    Our ability to know when to trust the decisions made by machine learning systems has not kept up with the staggering improvements in their performance, limiting their applicability in high-stakes domains. We introduce Prover-Verifier Games (PVGs), a game-theoretic framework to encourage learning agents to solve decision problems in a verifiable manner. The PVG consists of two learners with competing objectives: a trusted verifier network tries to choose the correct answer, and a more powerful but untrusted prover network attempts to persuade the verifier of a particular answer, regardless of its correctness. The goal is for a reliable justification protocol to emerge from this game. We analyze variants of the framework, including simultaneous and sequential games, and narrow the space down to a subset of games which provably have the desired equilibria. We develop instantiations of the PVG for two algorithmic tasks, and show that in practice, the verifier learns a robust decision rule that is able to receive useful and reliable information from an untrusted prover. Importantly, the protocol still works even when the verifier is frozen and the prover's messages are directly optimized to convince the verifier. \n",
        "Category": [
            "Scalable oversight",
            "Theory",
            "Security",
            "Non-Adversarial Robustness"
        ],
        "Citations": 10,
        "Context phrase (please add!)": "\"This is a sort of alternative to debate.\" - Roger Grosse\n",
        "Internal source": [
            "Expert rec"
        ],
        "Link": "https://arxiv.org/abs/2108.12099",
        "ML Subfield": [
            "Theory",
            "Human Model Interaction",
            "Model Evaluation",
            "Security",
            "Robustness and Adversariality"
        ],
        "ML Subfield (Is Starting Point)": [
            "Security",
            "Robustness and Adversariality"
        ],
        "ML Subtopic": [
            "Applied ML: Cognitive Tasks"
        ],
        "RS Comments": "\n",
        "Reviewed by": [
            "VK",
            "PC",
            "RS",
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Scalable oversight"
        ],
        "Safety Topic": [
            "Debate"
        ],
        "Title": "Learning to Give Checkable Answers with Prover-Verifier Games (Anil et al., 2021)\n",
        "Type": [
            "Paper"
        ],
        "Who tagged for ML": [
            "Viktoria"
        ],
        "airtable_createdTime": "2023-12-06T23:14:09.000Z",
        "airtable_id": "recrYPnKGa4P2dyER"
    },
    {
        "# Top Paper votes": 1,
        "AD: Top Paper": true,
        "Abstract": "When trying to gain better visibility into a machine learning model in order to understand and mitigate the associated risks, a potentially valuable source of evidence is: which training examples most contribute to a given behavior? Influence functions aim to answer a counterfactual: how would the model's parameters (and hence its outputs) change if a given sequence were added to the training set? While influence functions have produced insights for small models, they are difficult to scale to large language models (LLMs) due to the difficulty of computing an inverse-Hessian-vector product (IHVP). We use the Eigenvalue-corrected Kronecker-Factored Approximate Curvature (EK-FAC) approximation to scale influence functions up to LLMs with up to 52 billion parameters. In our experiments, EK-FAC achieves similar accuracy to traditional influence function estimators despite the IHVP computation being orders of magnitude faster. We investigate two algorithmic techniques to reduce the cost of computing gradients of candidate training sequences: TF-IDF filtering and query batching. We use influence functions to investigate the generalization patterns of LLMs, including the sparsity of the influence patterns, increasing abstraction with scale, math and programming abilities, cross-lingual generalization, and role-playing behavior. Despite many apparently sophisticated forms of generalization, we identify a surprising limitation: influences decay to near-zero when the order of key phrases is flipped. Overall, influence functions give us a powerful new tool for studying the generalization properties of LLMs. \n",
        "Blog or Video": "https://www.alignment-workshop.com/nola-talks/roger-grosse-studying-llm-generalization-through-influence-functions",
        "Category": [
            "Interpretability / explainability"
        ],
        "Citations": 50,
        "Goes online in Expanded Papers": true,
        "Internal source": [
            "Alignment Workshop 2023",
            "NOLA Workshop 2023"
        ],
        "Link": "https://arxiv.org/abs/2308.03296",
        "ML Subfield": [
            "NLP",
            "Statistical Modeling"
        ],
        "ML Subfield (Is Starting Point)": [
            "Statistical Modeling"
        ],
        "ML Subtopic": [
            "NLP: LLMs"
        ],
        "Reviewed by": [
            "AG",
            "VK",
            "PC",
            "RS",
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Transparency / Interpretability / Model internals / Latent knowledge"
        ],
        "Supplemental Material": "https://youtu.be/U2zJuTLzIm8?feature=shared&t=738",
        "Title": "Studying Large Language Model Generalization with Influence Functions (Grosse et al., 2023)",
        "Type": [
            "Paper",
            "Video"
        ],
        "Who tagged for ML": [
            "Viktoria"
        ],
        "Year": "2023",
        "airtable_createdTime": "2023-09-08T20:49:29.000Z",
        "airtable_id": "recrh8M1QaMBO3I5Z"
    },
    {
        "# Top Paper votes": 0,
        "AD comments": "\n",
        "Category": [
            "Overviews and agendas"
        ],
        "Description from MAIA / AISF / Elsewhere": "[Resource] is a new textbook. It aims to provide an accessible and comprehensive introduction to AI safety that draws on safety engineering, economics, philosophy, and other disciplines. Those who would like to take a free online course based on the textbook can express their interest here.",
        "Goes online in Expanded Papers": true,
        "Internal source": [
            "ML Newsletter"
        ],
        "Link": "https://drive.google.com/file/d/1JN7-ZGx9KLqRJ94rOQVwRSa7FPZGl2OY/view",
        "ML Subfield": [
            "Domain General",
            "Human Model Interaction",
            "Reinforcement Learning"
        ],
        "ML Subtopic": [
            "Applied ML: AI-Based Automation",
            "Applied ML: Chatbots",
            "Applied ML: Cognitive Tasks"
        ],
        "Reviewed by": [
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Major problems (misalignment, misuse, threat models)",
            "Overview"
        ],
        "Supplemental Material": "https://www.aisafetybook.com/virtual-course",
        "Title": "Introduction to AI Safety, Ethics, and Society (Hendrycks, 2023)",
        "Type": [
            "Other"
        ],
        "Who tagged for ML": [
            "Viktoria"
        ],
        "Year": "2023",
        "airtable_createdTime": "2023-12-20T01:45:25.000Z",
        "airtable_id": "recrpNrVBSiDVyr4j"
    },
    {
        "# Top Paper votes": 0,
        "Abstract": "While large-scale unsupervised language models (LMs) learn broad world knowledge and some reasoning skills, achieving precise control of their behavior is difficult due to the completely unsupervised nature of their training. Existing methods for gaining such steerability collect human labels of the relative quality of model generations and fine-tune the unsupervised LM to align with these preferences, often with reinforcement learning from human feedback (RLHF). However, RLHF is a complex and often unstable procedure, first fitting a reward model that reflects the human preferences, and then fine-tuning the large unsupervised LM using reinforcement learning to maximize this estimated reward without drifting too far from the original model. In this paper we introduce a new parameterization of the reward model in RLHF that enables extraction of the corresponding optimal policy in closed form, allowing us to solve the standard RLHF problem with only a simple classification loss. The resulting algorithm, which we call Direct Preference Optimization (DPO), is stable, performant, and computationally lightweight, eliminating the need for sampling from the LM during fine-tuning or performing significant hyperparameter tuning. Our experiments show that DPO can fine-tune LMs to align with human preferences as well as or better than existing methods. Notably, fine-tuning with DPO exceeds PPO-based RLHF in ability to control sentiment of generations, and matches or improves response quality in summarization and single-turn dialogue while being substantially simpler to implement and train.\n",
        "Internal source": [
            "Elsewhere"
        ],
        "Link": "https://arxiv.org/abs/2305.18290",
        "Safety Category": [
            "Science of deep learning"
        ],
        "Title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model (Rafailov et al., 2023)",
        "Type": [
            "Paper"
        ],
        "airtable_createdTime": "2024-03-31T18:06:47.000Z",
        "airtable_id": "recrv9lgw6azHspEu"
    },
    {
        "# Top Paper votes": 1,
        "Category": [
            "Deception",
            "Theory"
        ],
        "Goes online in Expanded Papers": true,
        "Internal source": [
            "AWAIR"
        ],
        "Link": "https://bounded-regret.ghost.io/ml-systems-will-have-weird-failure-modes-2/",
        "ML Subfield": [
            "Domain General",
            "Theory",
            "Reinforcement Learning"
        ],
        "ML Subfield (Is Starting Point)": [
            "Reinforcement Learning",
            "Domain General"
        ],
        "Reviewed by": [
            "AG",
            "VK",
            "PC",
            "RS",
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Deception",
            "Reward misspecification and goal misgeneralization"
        ],
        "Time from MAIA / AISF / Elsewhere (min)": "15",
        "Title": "ML Systems Will Have Weird Failure Modes (Steinhardt, 2022)",
        "Type": [
            "Blog post"
        ],
        "VK comments": "Good intro to deceptive alignment for ML people\n",
        "VK: Top Paper": true,
        "Who tagged for ML": [
            "Max"
        ],
        "Year": "2022",
        "airtable_createdTime": "2023-09-11T20:40:02.000Z",
        "airtable_id": "recrytMT7QnXwQ4aR"
    },
    {
        "# Top Paper votes": 0,
        "Abstract": "We propose Joint MLP/Attention (JoMA) dynamics, a novel mathematical framework to understand the training procedure of multilayer Transformer architectures. This is achieved by integrating out the self-attention layer in Transformers, producing a modified dynamics of MLP layers only. JoMA removes unrealistic assumptions in previous analysis (e.g., lack of residual connection) and predicts that the attention first becomes sparse (to learn salient tokens), then dense (to learn less salient tokens) in the presence of nonlinear activations, while in the linear case, it is consistent with existing works that show attention becomes sparse over time. We leverage JoMA to qualitatively explains how tokens are combined to form hierarchies in multilayer Transformers, when the input tokens are generated by a latent hierarchical generative model. Experiments on models trained from real-world dataset (Wikitext2/Wikitext103) and various pre-trained models (OPT, Pythia) verify our theoretical findings.\n",
        "Category": [
            "Interpretability / explainability"
        ],
        "Internal source": [
            "http://rdi.berkeley.edu/understanding_llms/s24"
        ],
        "Link": "https://arxiv.org/abs/2310.00535",
        "Safety Category": [
            "Science of deep learning",
            "Transparency / Interpretability / Model internals / Latent knowledge"
        ],
        "Title": "JoMA: Demystifying Multilayer Transformers via JOint Dynamics of MLP and Attention (Tian et al., 2023)",
        "Type": [
            "Paper"
        ],
        "airtable_createdTime": "2024-03-31T18:06:47.000Z",
        "airtable_id": "recsI9BLxgbNNMkmf"
    },
    {
        "# Top Paper votes": 0,
        "Category": [
            "Overviews and agendas",
            "Reinforcement Learning"
        ],
        "Internal source": [
            "Elsewhere"
        ],
        "Link": "https://bounded-regret.ghost.io/intrinsic-drives-and-extrinsic-misuse-two-intertwined-risks-of-ai/",
        "ML Subfield": [
            "Reinforcement Learning",
            "Applied ML",
            "Human Model Interaction"
        ],
        "ML Subtopic": [
            "NLP: LLMs",
            "Applied ML: Chatbots"
        ],
        "Reviewed by": [
            "PC",
            "RS",
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Major problems (misalignment, misuse, threat models)"
        ],
        "Title": "Intrinsic Drives and Extrinsic Misuse: Two Intertwined Risks of AI (Steinhardt, 2023)",
        "Type": [
            "Blog post"
        ],
        "Who tagged for ML": [
            "Max"
        ],
        "Year": "2023",
        "airtable_createdTime": "2023-12-12T20:00:03.000Z",
        "airtable_id": "recsqIOv5NELDy209"
    },
    {
        "# Top Paper votes": 0,
        "Abstract": "This paper argues that a range of current AI systems have learned how to deceive humans. We define deception as the systematic inducement of false beliefs in the pursuit of some outcome other than the truth. We first survey empirical examples of AI deception, discussing both special-use AI systems (including Meta's CICERO) built for specific competitive situations, and general-purpose AI systems (such as large language models). Next, we detail several risks from AI deception, such as fraud, election tampering, and losing control of AI systems. Finally, we outline several potential solutions to the problems posed by AI deception: first, regulatory frameworks should subject AI systems that are capable of deception to robust risk-assessment requirements; second, policymakers should implement bot-or-not laws; and finally, policymakers should prioritize the funding of relevant research, including tools to detect AI deception and to make AI systems less deceptive. Policymakers, researchers, and the broader public should work proactively to prevent AI deception from destabilizing the shared foundations of our society. \n",
        "Category": [
            "Deception",
            "Theory"
        ],
        "Citations": 45,
        "Goes online in Expanded Papers": true,
        "Internal source": [
            "Hendrycks Textbook"
        ],
        "Link": "https://arxiv.org/abs/2308.14752",
        "ML Subfield": [
            "Applied ML",
            "Human Model Interaction",
            "Robustness and Adversariality"
        ],
        "ML Subfield (Is Starting Point)": [
            "Robustness and Adversariality",
            "Applied ML"
        ],
        "ML Subtopic": [
            "Applied ML: Chatbots",
            "NLP: LLMs",
            "RL: Games",
            "Applied ML: Cognitive Tasks"
        ],
        "Reviewed by": [
            "PC",
            "RS",
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Deception",
            "Overview"
        ],
        "Title": "AI Deception: A Survey of Examples, Risks, and Potential Solutions (Park et al., 2023)",
        "Type": [
            "Paper"
        ],
        "Who tagged for ML": [
            "Max"
        ],
        "Year": "2023",
        "airtable_createdTime": "2023-12-12T20:00:03.000Z",
        "airtable_id": "rect0eNjViu7iCNAx"
    },
    {
        "# Top Paper votes": 0,
        "Abstract": "    We revisit and extend model stitching (Lenc & Vedaldi 2015) as a methodology to study the internal representations of neural networks. Given two trained and frozen models A and B, we consider a \"stitched model'' formed by connecting the bottom-layers of A to the top-layers of B, with a simple trainable layer between them. We argue that model stitching is a powerful and perhaps under-appreciated tool, which reveals aspects of representations that measures such as centered kernel alignment (CKA) cannot. Through extensive experiments, we use model stitching to obtain quantitative verifications for intuitive statements such as \"good networks learn similar representations'', by demonstrating that good networks of the same architecture, but trained in very different ways (e.g.: supervised vs. self-supervised learning), can be stitched to each other without drop in performance. We also give evidence for the intuition that \"more is better'' by showing that representations learnt with (1) more data, (2) bigger width, or (3) more training time can be \"plugged in'' to weaker models to improve performance. Finally, our experiments reveal a new structural property of SGD which we call \"stitching connectivity'', akin to mode-connectivity: typical minima reached by SGD can all be stitched to each other with minimal change in accuracy. \n",
        "Category": [
            "Interpretability / explainability"
        ],
        "Internal source": [
            "Further reading on MAIA/HAIST Summer 2023 Curriculum"
        ],
        "Link": "https://arxiv.org/abs/2106.07682",
        "ML Subtopic": [
            "Theory"
        ],
        "Reviewed by": [
            "AG",
            "VK",
            "PC",
            "RS"
        ],
        "Safety Category": [
            "Transparency / Interpretability / Model internals / Latent knowledge"
        ],
        "Time from MAIA / AISF / Elsewhere (min)": "30",
        "Title": "Revisiting Model Stitching to Compare Neural Representations (Bansal et al., 2021)",
        "Type": [
            "Paper"
        ],
        "Who tagged for ML": [
            "Viktoria"
        ],
        "airtable_createdTime": "2024-02-10T00:27:32.000Z",
        "airtable_id": "rect6DnnHRqa0rRoq"
    },
    {
        "# Top Paper votes": 0,
        "AD comments": "\n",
        "Abstract": "This paper presents CYBERSECEVAL, a comprehensive benchmark developed to help bolster the cybersecurity of Large Language Models (LLMs) employed as coding assistants. As what we believe to be the most extensive unified cybersecurity safety benchmark to date, CYBERSECEVAL provides a thorough evaluation of LLMs in two crucial security domains: their propensity to generate insecure code and their level of compliance when asked to assist in cyberattacks. Through a case study involving seven models from the Llama2, codeLlama, and OpenAI GPT large language model families, CYBERSECEVAL effectively pinpointed key cybersecurity risks. More importantly, it offered practical insights for refining these models. A significant observation from the study was the tendency of more advanced models to suggest insecure code, highlighting the critical need for integrating security considerations in the development of sophisticated LLMs. CYBERSECEVAL, with its automated test case generation and evaluation pipeline covers a broad scope and equips LLM designers and researchers with a tool to broadly measure and enhance the cybersecurity safety properties of LLMs, contributing to the development of more secure AI systems.\n",
        "Category": [
            "Model evaluations and benchmarks",
            "Security"
        ],
        "Description from MAIA / AISF / Elsewhere": "[Paper] evaluates code generated by LLMs for security vulnerabilities. GPT-4, Llama 2, and other LLMs are found to frequently generate code containing security vulnerabilities. It also evaluates whether LLMs refuse requests to assist in a cyberattack, but does not evaluate whether these refusals are robust to adversarial prompts or fine-tuning.",
        "Goes online in Expanded Papers": true,
        "Internal source": [
            "ML Newsletter"
        ],
        "Link": "https://ai.meta.com/research/publications/purple-llama-cyberseceval-a-benchmark-for-evaluating-the-cybersecurity-risks-of-large-language-models/",
        "ML Subfield": [
            "Security",
            "Model Evaluation",
            "NLP"
        ],
        "ML Subfield (Is Starting Point)": [
            "Security"
        ],
        "ML Subtopic": [
            "NLP: LLMs"
        ],
        "Reviewed by": [
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Model evaluations / monitoring / detection"
        ],
        "Safety Topic": [
            "Benchmarking",
            "Security"
        ],
        "Supplemental Material": "https://ai.meta.com/llama/purple-llama/",
        "Title": "Purple Llama CyberSecEval: A benchmark for evaluating the cybersecurity risks of large language models (Bhatt et al., 2023)",
        "Type": [
            "Paper"
        ],
        "Who tagged for ML": [
            "Viktoria"
        ],
        "airtable_createdTime": "2023-12-20T01:45:25.000Z",
        "airtable_id": "rectAPClvKWwMF4OB"
    },
    {
        "# Top Paper votes": 0,
        "Category": [
            "Interpretability / explainability"
        ],
        "Internal source": [
            "NOLA Workshop 2023"
        ],
        "Link": "https://www.alignment-workshop.com/nola-talks/johannes-von-oswald-mechanistic-interpretability-of-in-context-learning",
        "Safety Category": [
            "Transparency / Interpretability / Model internals / Latent knowledge"
        ],
        "Safety Topic": [
            "Mechanistic interpretability"
        ],
        "Title": "Mechanistic Interpretability of in-context learning (Johannes von Oswald, 5-min video)",
        "Type": [
            "Video"
        ],
        "Vael's Notes": "Not including the lightning talks for now, since pretty EA probably\n",
        "airtable_createdTime": "2024-02-10T00:27:32.000Z",
        "airtable_id": "rectAWvRWz2EKoDWy"
    },
    {
        "# Top Paper votes": 1,
        "AD: Top Paper": true,
        "Category": [
            "Interpretability / explainability"
        ],
        "Goes online in Expanded Papers": true,
        "Internal source": [
            "AWAIR"
        ],
        "Link": "https://www.neelnanda.io/mechanistic-interpretability/quickstart",
        "ML Subfield": [
            "Theory",
            "NLP"
        ],
        "ML Subtopic": [
            "NLP: LLMs"
        ],
        "Reviewed by": [
            "AG",
            "VK",
            "PC",
            "RS",
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Transparency / Interpretability / Model internals / Latent knowledge",
            "Overview"
        ],
        "Safety Topic": [
            "Mechanistic interpretability"
        ],
        "Title": "Mechanistic Interpretability Quickstart Guide (Nanda, 2023)",
        "Type": [
            "Blog post"
        ],
        "Who tagged for ML": [
            "Viktoria"
        ],
        "Year": "2023",
        "airtable_createdTime": "2023-09-12T01:54:18.000Z",
        "airtable_id": "rectak0XEkOCM3nch"
    },
    {
        "# Top Paper votes": 1,
        "AD: Top Paper": true,
        "Internal source": [
            "Elsewhere"
        ],
        "Link": "https://bounded-regret.ghost.io/forecasting-ai-overview/",
        "ML Subfield": [
            "Domain General"
        ],
        "ML Subfield (Is Starting Point)": [
            "Domain General"
        ],
        "Reviewed by": [
            "AG",
            "VK",
            "PC",
            "RS",
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Forecasting"
        ],
        "Title": "Forecasting AI (Overview) (Steinhardt, 2023)",
        "Type": [
            "Blog post"
        ],
        "Who tagged for ML": [
            "Viktoria"
        ],
        "Year": "2023",
        "airtable_createdTime": "2023-09-05T22:50:05.000Z",
        "airtable_id": "rectiyp3Ov34yJcgE"
    },
    {
        "# Top Paper votes": 0,
        "Abstract": "    What is learned by sophisticated neural network agents such as AlphaZero? This question is of both scientific and practical interest. If the representations of strong neural networks bear no resemblance to human concepts, our ability to understand faithful explanations of their decisions will be restricted, ultimately limiting what we can achieve with neural network interpretability. In this work we provide evidence that human knowledge is acquired by the AlphaZero neural network as it trains on the game of chess. By probing for a broad range of human chess concepts we show when and where these concepts are represented in the AlphaZero network. We also provide a behavioural analysis focusing on opening play, including qualitative analysis from chess Grandmaster Vladimir Kramnik. Finally, we carry out a preliminary investigation looking at the low-level details of AlphaZero's representations, and make the resulting behavioural and representational analyses available online. \n",
        "Internal source": [
            "Hendrycks Textbook"
        ],
        "Link": "http://arxiv.org/abs/2111.09259",
        "ML Subtopic": [
            "Applied ML: Cognitive Tasks"
        ],
        "Reviewed by": [
            "PC",
            "RS"
        ],
        "Safety Category": [
            "Capabilities of LLMs",
            "Science of deep learning"
        ],
        "Safety Topic": [
            "Emergence"
        ],
        "Title": "Acquisition of Chess Knowledge in AlphaZero (McGrath et al., 2022)",
        "Type": [
            "Paper"
        ],
        "Who tagged for ML": [
            "Viktoria"
        ],
        "airtable_createdTime": "2024-02-10T00:27:32.000Z",
        "airtable_id": "rectlIYeaX1jZcJ4L"
    },
    {
        "# Top Paper votes": 1,
        "AD: Top Paper": true,
        "Abstract": "    While Large Language Models (LLMs) have shown exceptional performance in various tasks, one of their most prominent drawbacks is generating inaccurate or false information with a confident tone. In this paper, we provide evidence that the LLM's internal state can be used to reveal the truthfulness of statements. This includes both statements provided to the LLM, and statements that the LLM itself generates. Our approach is to train a classifier that outputs the probability that a statement is truthful, based on the hidden layer activations of the LLM as it reads or generates the statement. Experiments demonstrate that given a set of test sentences, of which half are true and half false, our trained classifier achieves an average of 71\\\\% to 83\\\\% accuracy labeling which sentences are true versus false, depending on the LLM base model. Furthermore, we explore the relationship between our classifier's performance and approaches based on the probability assigned to the sentence by the LLM. We show that while LLM-assigned sentence probability is related to sentence truthfulness, this probability is also dependent on sentence length and the frequencies of words in the sentence, resulting in our trained classifier providing a more reliable approach to detecting truthfulness, highlighting its potential to enhance the reliability of LLM-generated content and its practical applicability in real-world scenarios. \n",
        "Category": [
            "Interpretability / explainability",
            "Deception"
        ],
        "Citations": 124,
        "Goes online as Top Paper": true,
        "Goes online in Expanded Papers": true,
        "Internal source": [
            "AWAIR"
        ],
        "Link": "https://arxiv.org/abs/2304.13734",
        "ML Subfield": [
            "NLP"
        ],
        "ML Subtopic": [
            "NLP: LLMs"
        ],
        "Reviewed by": [
            "AG",
            "VK",
            "PC",
            "RS",
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Transparency / Interpretability / Model internals / Latent knowledge",
            "Deception"
        ],
        "Safety Topic": [
            "Sycophancy"
        ],
        "Title": "The Internal State of an LLM Knows When It's Lying (Azaria and Mitchell, 2023)",
        "Type": [
            "Paper"
        ],
        "Who tagged for ML": [
            "Viktoria"
        ],
        "Year": "2023",
        "airtable_createdTime": "2023-09-05T22:50:05.000Z",
        "airtable_id": "rectnjrpLZEwgWMVX"
    },
    {
        "# Top Paper votes": 1,
        "AD: Top Paper": true,
        "AG comments": "Reasonable intro from what I recall but I don't remember the details so might be off.\n",
        "Category": [
            "Why large-scale safety?"
        ],
        "Goes online in Expanded Papers": true,
        "Internal source": [
            "Alignment Workshop 2023"
        ],
        "Link": "https://www.alignment-workshop.com/sf-talks/paul-christiano-how-misalignment-could-lead-to-takeover",
        "ML Subfield": [
            "Domain General",
            "Applied ML",
            "Human Model Interaction"
        ],
        "Reviewed by": [
            "AG",
            "VK",
            "PC",
            "RS",
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Major problems (misalignment, misuse, threat models)"
        ],
        "Title": "How Misalignment Could Lead to Takeover (Paul Christiano, 30-min video)",
        "Type": [
            "Video"
        ],
        "Vael's Notes": "Paul Christiano - How Misalignment Could Lead to Takeover\n\\- Why might AI want to take over?\n\u2014 Failure mode 1: Reward hacking (\u201cdisempowering humanity may be an effective strategy for AI systems collectively to get a lot of reward\u201d)\n\u00a0\u2026 \u201ccorrupting measurements may be the best way to get a high reward\u201d e.g. sycophancy to situational awareness deception\n\\- choice: after you get corrected a bunch for bad behavior, do you: generalize correctly and not make mistakes at higher levels? Or do you go into a mode where you understand where humans will test you and be situationally-aware deceptive?\n\u2014 Failure mode 2: Deceptive alignment\n\\- again have the problem of people asking questions inaudibly for a long time\n",
        "Who tagged for ML": [
            "Viktoria"
        ],
        "airtable_createdTime": "2023-09-06T19:17:58.000Z",
        "airtable_id": "rectoXlPYxekOjRAj"
    },
    {
        "# Top Paper votes": 0,
        "Category": [
            "Model evaluations and benchmarks"
        ],
        "Description from MAIA / AISF / Elsewhere": "Karnofsky discusses what he would like to see scaling labs do to help mitigate catastrophic risks from AI.",
        "Link": " https://www.cold-takes.com/what-ai-companies-can-do-today-to-help-with-the-most-important-century/",
        "ML Subfield": [
            "Human Model Interaction"
        ],
        "Reviewed by": [
            "AG",
            "VK",
            "PC",
            "RS",
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Model evaluations / monitoring / detection"
        ],
        "Title": "What AI companies can do today to help with the most important century (Karnofsky, 2023)",
        "Type": [
            "Blog post"
        ],
        "Who tagged for ML": [
            "Viktoria"
        ],
        "Year": "2023",
        "airtable_createdTime": "2023-09-16T22:36:33.000Z",
        "airtable_id": "recu1t6h3VkMiYyEi"
    },
    {
        "# Top Paper votes": 0,
        "Category": [
            "Scalable oversight"
        ],
        "Internal source": [
            "NOLA Workshop 2023"
        ],
        "Link": "https://www.alignment-workshop.com/nola-talks/shane-legg-system-two-safety",
        "Safety Category": [
            "Scalable oversight"
        ],
        "Title": "System Two Safety (Shane Legg, 5-min video)",
        "Type": [
            "Video"
        ],
        "Vael's Notes": "Not including the lightning talks for now, since pretty EA probably\n",
        "airtable_createdTime": "2024-02-10T00:27:32.000Z",
        "airtable_id": "recu9kUytlAMalV7W"
    },
    {
        "# Top Paper votes": 2,
        "AD: Top Paper": true,
        "Abstract": " We study goal misgeneralization, a type of out-of-distribution generalization failure in reinforcement learning (RL). Goal misgeneralization failures occur when an RL agent retains its capabilities out-of-distribution yet pursues the wrong goal. For instance, an agent might continue to competently avoid obstacles, but navigate to the wrong place. In contrast, previous works have typically focused on capability generalization failures, where an agent fails to do anything sensible at test time. We formalize this distinction between capability and goal generalization, provide the first empirical demonstrations of goal misgeneralization, and present a partial characterization of its causes. \n",
        "Category": [
            "Reward misspecification and goal misgeneralization",
            "Reinforcement Learning"
        ],
        "Citations": 73,
        "Goes online in Expanded Papers": true,
        "High Citation and Max Select": true,
        "Link": "https://arxiv.org/abs/2105.14111",
        "ML Subfield": [
            "Reinforcement Learning",
            "Theory"
        ],
        "ML Subfield (Is Starting Point)": [
            "Reinforcement Learning"
        ],
        "ML Subtopic": [
            "RL: Games"
        ],
        "Reviewed by": [
            "VK",
            "PC",
            "RS",
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Reward misspecification and goal misgeneralization"
        ],
        "Title": "Goal Misgeneralization in Deep Reinforcement Learning (Langosco et al., 2021)\n",
        "Type": [
            "Paper"
        ],
        "VK: Top Paper": true,
        "Who tagged for ML": [
            "Max"
        ],
        "Year": "2021",
        "airtable_createdTime": "2023-12-07T01:52:40.000Z",
        "airtable_id": "recuJvG7DEYkKXbjQ"
    },
    {
        "# Top Paper votes": 1,
        "AD: Top Paper": true,
        "Category": [
            "Scalable oversight",
            "Model evaluations and benchmarks"
        ],
        "Internal source": [
            "Elsewhere"
        ],
        "Link": "https://openai.com/blog/our-approach-to-alignment-research",
        "ML Subfield": [
            "NLP",
            "Model Evaluation"
        ],
        "ML Subtopic": [
            "NLP: LLMs",
            "Applied ML: Cognitive Tasks"
        ],
        "Reviewed by": [
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Scalable oversight",
            "Alignment <-> RLHF",
            "Overview"
        ],
        "Title": "Our approach to alignment research (Leike, Schulman, and Wu; OpenAI; 2023)",
        "Type": [
            "Blog post"
        ],
        "Who tagged for ML": [
            "Viktoria"
        ],
        "Year": "2023",
        "airtable_createdTime": "2023-12-20T01:45:25.000Z",
        "airtable_id": "recuQh7g9cfWOzh5r"
    },
    {
        "# Top Paper votes": 0,
        "Category": [
            "Security"
        ],
        "Goes online in Expanded Papers": true,
        "Internal source": [
            "Elsewhere"
        ],
        "Link": "https://www.rand.org/pubs/research_reports/RRA2849-1.html",
        "Title": "Securing AI Model Weights",
        "Type": [
            "Other"
        ],
        "Year": "2024",
        "airtable_createdTime": "2024-08-09T15:24:47.000Z",
        "airtable_id": "recuqjPHivzNGERc3"
    },
    {
        "# Top Paper votes": 0,
        "Category": [
            "AI Governance"
        ],
        "Internal source": [
            "Hendrycks Textbook"
        ],
        "Link": "https://www.aisafetybook.com/textbook/8-1",
        "Safety Category": [
            "AI Governance"
        ],
        "Title": "Introduction to AI Safety, Ethics, and Society: Chapter 9 - AI Governance (Hendrycks, 2023)",
        "Type": [
            "Other"
        ],
        "airtable_createdTime": "2024-03-31T18:06:47.000Z",
        "airtable_id": "recuybAjikmVSidWj"
    },
    {
        "# Top Paper votes": 1,
        "AD comments": "\n",
        "AD: Top Paper": true,
        "Abstract": "Widely used alignment techniques, such as reinforcement learning from human feedback (RLHF), rely on the ability of humans to supervise model behavior\u2014for example, to evaluate whether a model faithfully followed instructions or generated safe outputs. However, future superhuman models will behave in complex ways too difficult for humans to reliably evaluate; humans will only be able to weakly supervise superhuman models. We study an analogy to this problem: can weak model supervision elicit the full capabilities of a much stronger model? We test this using a range of pretrained language models in the GPT-4 family on natural language processing (NLP), chess, and reward modeling tasks. We find that when we naively finetune strong pretrained models on labels generated by a weak model, they consistently perform better than their weak supervisors, a phenomenon we call weak-to-strong generalization. However, we are still far from recovering the full capabilities of strong models with naive finetuning alone, suggesting that techniques like RLHF may scale poorly to superhuman models without further work. We find that simple methods can often significantly improve weak-to-strong generalization: for example, when finetuning GPT-4 with a GPT-2-level supervisor and an auxiliary confidence loss, we can recover close to GPT-3.5-level performance on NLP tasks. Our results suggest that it is feasible to make empirical progress today on a fundamental challenge of aligning superhuman models.\n",
        "Blog or Video": "https://www.alignment-workshop.com/nola-talks/collin-burns-weak-to-strong-generalization",
        "Category": [
            "Scalable oversight"
        ],
        "Citations": 53,
        "Description from MAIA / AISF / Elsewhere": "[Paper] aims to make empirical progress on supervising superhuman AI systems by considering an analogous problem: supervising strong AI systems using weaker AI systems. From the abstract:\n\nWe study a range of pretrained language models in the GPT-4 family on NLP, chess, and reward modeling tasks and find that when we naively finetune strong models to directly imitate weak model supervision, they consis- tently perform better than their weak supervisors, a phenomenon we call weak-to- strong generalization. However, naive generalization is still far from recovering the full capabilities of strong models trained with ground truth supervision. These results suggest that existing methods for aligning models with human supervision may indeed break down when applied to superhuman models in the future.",
        "Goes online as Top Paper": true,
        "Goes online in Expanded Papers": true,
        "Internal source": [
            "ML Newsletter",
            "NOLA Workshop 2023"
        ],
        "Link": "https://arxiv.org/abs/2312.09390",
        "ML Subfield": [
            "NLP"
        ],
        "ML Subtopic": [
            "NLP: LLMs"
        ],
        "Reviewed by": [
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Scalable oversight"
        ],
        "Supplemental Material": "https://openai.com/research/weak-to-strong-generalization",
        "Title": "Weak-To-Strong Generalization: Eliciting Strong Capabilities With Weak Supervision (Burns et al., 2023)",
        "Twitter": "https://x.com/CollinBurns4/status/1735350133926314431",
        "Type": [
            "Paper",
            "Blog post",
            "Video"
        ],
        "Who tagged for ML": [
            "Viktoria"
        ],
        "Year": "2023",
        "airtable_createdTime": "2023-12-20T01:45:25.000Z",
        "airtable_id": "recuzlRQEZvDPXNLn"
    },
    {
        "# Top Paper votes": 0,
        "Abstract": "    We propose a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. We crafted questions that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from imitating human texts. We tested GPT-3, GPT-Neo/J, GPT-2 and a T5-based model. The best model was truthful on 58% of questions, while human performance was 94%. Models generated many false answers that mimic popular misconceptions and have the potential to deceive humans. The largest models were generally the least truthful. This contrasts with other NLP tasks, where performance improves with model size. However, this result is expected if false answers are learned from the training distribution. We suggest that scaling up models alone is less promising for improving truthfulness than fine-tuning using training objectives other than imitation of text from the web. \n",
        "Category": [
            "Deception",
            "Model evaluations and benchmarks"
        ],
        "Citations": 740,
        "High Citation and Max Select": true,
        "Internal source": [
            "Hendrycks Textbook"
        ],
        "Link": "https://arxiv.org/abs/2109.07958",
        "ML Subfield": [
            "Applied ML",
            "NLP",
            "Robustness and Adversariality"
        ],
        "ML Subfield (Is Starting Point)": [
            "NLP"
        ],
        "ML Subtopic": [
            "NLP: Language Modeling",
            "Applied ML: Chatbots",
            "Applied ML: Cognitive Tasks",
            "NLP: LLMs"
        ],
        "Reviewed by": [
            "PC",
            "RS",
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Deception"
        ],
        "Title": "TruthfulQA: Measuring How Models Mimic Human Falsehoods (Lin et al., 2021)",
        "Type": [
            "Paper"
        ],
        "Who tagged for ML": [
            "Max"
        ],
        "Year": "2021",
        "airtable_createdTime": "2023-12-12T20:00:03.000Z",
        "airtable_id": "recvG4moyz5IEvbmm"
    },
    {
        "# Top Paper votes": 0,
        "Category": [
            "Interpretability / explainability",
            "Unlearning and Knowledge Editing"
        ],
        "Goes online as Top Paper": true,
        "Goes online in Expanded Papers": true,
        "Internal source": [
            "Elsewhere"
        ],
        "Link": "https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html",
        "Safety Category": [
            "Transparency / Interpretability / Model internals / Latent knowledge"
        ],
        "Safety Topic": [
            "Mechanistic interpretability"
        ],
        "Title": "Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet",
        "Twitter": "https://x.com/AnthropicAI/status/1792935506587656625",
        "Type": [
            "Paper"
        ],
        "Who tagged for ML": [
            "Vael"
        ],
        "Year": "2024",
        "airtable_createdTime": "2024-07-31T00:00:04.000Z",
        "airtable_id": "recvcDouW9A4J09l7"
    },
    {
        "# Top Paper votes": 0,
        "Abstract": "    Surprisingly, we find that the adversarial prompts generated by our approach are quite transferable, including to black-box, publicly released LLMs. Specifically, we train an adversarial attack suffix on multiple prompts (i.e., queries asking for many different types of objectionable content), as well as multiple models (in our case, Vicuna-7B and 13B). When doing so, the resulting attack suffix is able to induce objectionable content in the public interfaces to ChatGPT, Bard, and Claude, as well as open source LLMs such as LLaMA-2-Chat, Pythia, Falcon, and others. In total, this work significantly advances the state-of-the-art in adversarial attacks against aligned language models, raising important questions about how such systems can be prevented from producing objectionable information. Code is available at this http URL. \n",
        "Category": [
            "Non-Adversarial Robustness"
        ],
        "Internal source": [
            "Hendrycks Textbook"
        ],
        "Link": "http://arxiv.org/abs/2305.13860",
        "ML Subfield": [
            "Robustness and Adversariality",
            "NLP"
        ],
        "ML Subtopic": [
            "Applied ML: Chatbots",
            "NLP: LLMs"
        ],
        "Reviewed by": [
            "PC",
            "RS",
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Adversaries / Robustness / Generalization"
        ],
        "Title": "Jailbreaking ChatGPT via Prompt Engineering: An Empirical Study (Liu et al., 2023)",
        "Type": [
            "Paper"
        ],
        "Who tagged for ML": [
            "Viktoria"
        ],
        "airtable_createdTime": "2023-12-12T20:00:03.000Z",
        "airtable_id": "recvxo2zgv0d7mYtS"
    },
    {
        "# Top Paper votes": 0,
        "Category": [
            "Scalable oversight",
            "Reward misspecification and goal misgeneralization"
        ],
        "Internal source": [
            "NOLA Workshop 2023"
        ],
        "Link": "https://www.alignment-workshop.com/nola-talks/anca-dr%C4%83gan-implications-of-human-model-misspecification-for-alignment",
        "Safety Category": [
            "Scalable oversight",
            "Reward misspecification and goal misgeneralization"
        ],
        "Title": "Implications of human model misspecification for alignment (Anca Dragan, 6-min video)",
        "Type": [
            "Video"
        ],
        "Vael's Notes": "Not including the lightning talks for now, since pretty EA probably\n",
        "airtable_createdTime": "2024-02-10T00:27:32.000Z",
        "airtable_id": "recwGL5eh1zccnSqe"
    },
    {
        "# Top Paper votes": 1,
        "AD: Top Paper": true,
        "Abstract": "While several recent works have identified societal-scale and extinction-level risks to humanity arising from artificial intelligence, few have attempted an exhaustive taxonomy of such risks. Many exhaustive taxonomies are possible, and some are useful\u2014particularly if they reveal new risks or practical approaches to safety. This paper explores a taxonomy based on accountability: whose actions lead to the risk, are the actors unified, and are they deliberate? We also provide stories to illustrate how the various risk types could each play out, including risks arising from unanticipated interactions of many AI systems, as well as risks from deliberate misuse, for which combined technical and policy solutions are indicated.\n",
        "Category": [
            "Overviews and agendas"
        ],
        "Citations": 12,
        "Goes online in Expanded Papers": true,
        "Internal source": [
            "Elsewhere"
        ],
        "Link": "https://arxiv.org/abs/2306.06924",
        "ML Subfield": [
            "Domain General",
            "Human Model Interaction",
            "Applied ML"
        ],
        "Reviewed by": [
            "AG",
            "VK",
            "PC",
            "RS",
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Major problems (misalignment, misuse, threat models)",
            "Overview"
        ],
        "Title": "TASRA: A Taxonomy and Analysis of Societal-Scale Risks from AI (Critch and Russell, 2023)",
        "Twitter": "https://twitter.com/AndrewCritchCA/status/1668476943208169473",
        "Type": [
            "Paper"
        ],
        "Who tagged for ML": [
            "Viktoria"
        ],
        "Year": "2023",
        "airtable_createdTime": "2023-09-05T22:50:05.000Z",
        "airtable_id": "recwKRoWr2O2vhDp8"
    },
    {
        "# Top Paper votes": 0,
        "Category": [
            "AI Governance"
        ],
        "Internal source": [
            "NOLA Workshop 2023"
        ],
        "Link": "https://www.alignment-workshop.com/nola-talks/john-schulman-keeping-humans-in-the-loop",
        "Safety Category": [
            "AI Governance"
        ],
        "Title": "Keeping Humans in the Loop (John Schulman, 5-min video)",
        "Type": [
            "Video"
        ],
        "Vael's Notes": "Not including the lightning talks for now, since pretty EA probably\n",
        "airtable_createdTime": "2024-02-10T00:27:32.000Z",
        "airtable_id": "recwPNwLqvqZE2ctR"
    },
    {
        "# Top Paper votes": 0,
        "Category": [
            "Overviews and agendas"
        ],
        "Internal source": [
            "AWAIR"
        ],
        "Link": "https://www.alignmentforum.org/posts/HBxe6wdjxK239zajf/what-failure-looks-like",
        "Safety Category": [
            "Major problems (misalignment, misuse, threat models)"
        ],
        "Title": "What failure looks like (Christiano, 2019)",
        "Type": [
            "Blog post"
        ],
        "airtable_createdTime": "2024-02-10T00:27:32.000Z",
        "airtable_id": "recwdnjj5xE62ys5j"
    },
    {
        "# Top Paper votes": 0,
        "Category": [
            "Overviews and agendas"
        ],
        "Context phrase (please add!)": "\n",
        "Internal source": [
            "Elsewhere"
        ],
        "Link": "https://theaidigest.org/progress-and-dangers",
        "ML Subfield": [
            "Applied ML",
            "Domain General",
            "Human Model Interaction"
        ],
        "ML Subfield (Is Starting Point)": [
            "Domain General",
            "Applied ML"
        ],
        "ML Subtopic": [
            "NLP: LLMs",
            "Applied ML: Chatbots"
        ],
        "Reviewed by": [
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Capabilities of LLMs",
            "Forecasting",
            "Major problems (misalignment, misuse, threat models)",
            "Overview"
        ],
        "Title": "How fast is AI improving? (AI Digest; 2023)",
        "Type": [
            "Other"
        ],
        "Who tagged for ML": [
            "Max"
        ],
        "Year": "2023",
        "airtable_createdTime": "2023-12-27T21:46:39.000Z",
        "airtable_id": "recwkENecD7xOYIT4"
    },
    {
        "# Top Paper votes": 1,
        "AD: Top Paper": true,
        "Abstract": "    Multimodal contrastive learning methods like CLIP train on noisy and uncurated training datasets. This is cheaper than labeling datasets manually, and even improves out-of-distribution robustness. We show that this practice makes backdoor and poisoning attacks a significant threat. By poisoning just 0.01% of a dataset (e.g., just 300 images of the 3 million-example Conceptual Captions dataset), we can cause the model to misclassify test images by overlaying a small patch. Targeted poisoning attacks, whereby the model misclassifies a particular test input with an adversarially-desired label, are even easier requiring control of 0.0001% of the dataset (e.g., just three out of the 3 million images). Our attacks call into question whether training on noisy and uncurated Internet scrapes is desirable. \n",
        "Category": [
            "Adversarial Robustness"
        ],
        "Citations": 134,
        "Goes online in Expanded Papers": true,
        "Internal source": [
            "Intro to ML Safety Course"
        ],
        "Link": "https://arxiv.org/abs/2106.09667",
        "ML Subfield": [
            "Robustness and Adversariality",
            "Vision",
            "Applied ML"
        ],
        "ML Subfield (Is Starting Point)": [
            "Robustness and Adversariality",
            "Applied ML"
        ],
        "Reviewed by": [
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Adversaries / Robustness / Generalization"
        ],
        "Safety Topic": [
            "Trojans"
        ],
        "Title": "Poisoning and Backdooring Contrastive Learning (Carlini and Terzis, 2022)",
        "Type": [
            "Paper"
        ],
        "Who tagged for ML": [
            "Viktoria"
        ],
        "Year": "2022",
        "airtable_createdTime": "2024-01-15T21:37:08.000Z",
        "airtable_id": "recxCdHJonCK1b3Dj"
    },
    {
        "# Top Paper votes": 0,
        "Category": [
            "Overviews and agendas"
        ],
        "Internal source": [
            "AWAIR"
        ],
        "Link": "https://www.alignmentforum.org/posts/PZtsoaoSLpKjjbMqM/the-case-for-aligning-narrowly-superhuman-models",
        "Reviewed by": [
            "AG",
            "VK"
        ],
        "Safety Category": [
            "Major problems (misalignment, misuse, threat models)"
        ],
        "Title": "The case for aligning narrowly superhuman models (Cotra, 2021)",
        "Type": [
            "Paper"
        ],
        "Vael's Notes": "This is two years old and is a bit too EA/alignment oriented, but provides useful context to me on why we're working on aligning current AI, and this actually had been an ongoing confusion for me.\n\n",
        "airtable_createdTime": "2024-02-10T00:27:32.000Z",
        "airtable_id": "recxPXgQWgZ7KJMAH"
    },
    {
        "# Top Paper votes": 0,
        "Internal source": [
            "Elsewhere"
        ],
        "Link": "https://wiki.aiimpacts.org/ai_timelines/predictions_of_human-level_ai_timelines/ai_timeline_surveys/2023_expert_survey_on_progress_in_ai",
        "ML Subfield": [
            "Domain General"
        ],
        "ML Subfield (Is Starting Point)": [
            "Domain General"
        ],
        "Reviewed by": [
            "AG",
            "VK",
            "PC",
            "RS",
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Forecasting"
        ],
        "Title": "Expert Survey on Progress in AI (AI Impacts, 2023)",
        "Type": [
            "Blog post"
        ],
        "Who tagged for ML": [
            "Viktoria"
        ],
        "Year": "2022",
        "airtable_createdTime": "2023-09-05T22:50:05.000Z",
        "airtable_id": "recyeICwVWul1Iwqk"
    },
    {
        "# Top Paper votes": 0,
        "Abstract": "    The unprecedented success of deep neural networks in many applications has made these networks a prime target for adversarial exploitation. In this paper, we introduce a benchmark technique for detecting backdoor attacks (aka Trojan attacks) on deep convolutional neural networks (CNNs). We introduce the concept of Universal Litmus Patterns (ULPs), which enable one to reveal backdoor attacks by feeding these universal patterns to the network and analyzing the output (i.e., classifying the network as \\`clean' or \\`corrupted'). This detection is fast because it requires only a few forward passes through a CNN. We demonstrate the effectiveness of ULPs for detecting backdoor attacks on thousands of networks with different architectures trained on four benchmark datasets, namely the German Traffic Sign Recognition Benchmark (GTSRB), MNIST, CIFAR10, and Tiny-ImageNet. The codes and train/test models for this paper can be found here this https URL. \n",
        "Category": [
            "Security",
            "Adversarial Robustness"
        ],
        "Citations": 224,
        "Goes online in Expanded Papers": true,
        "High Citation and Max Select": true,
        "Internal source": [
            "Intro to ML Safety Course"
        ],
        "Link": "https://arxiv.org/abs/1906.10842",
        "ML Subfield": [
            "Vision",
            "Robustness and Adversariality",
            "Security"
        ],
        "ML Subfield (Is Starting Point)": [
            "Robustness and Adversariality",
            "Vision",
            "Security"
        ],
        "ML Subtopic": [
            "Applied ML: Autonomous Vehicles"
        ],
        "Reviewed by": [
            "DH",
            "AD"
        ],
        "Safety Category": [
            "Adversaries / Robustness / Generalization"
        ],
        "Safety Topic": [
            "Trojans"
        ],
        "Title": "Universal Litmus Patterns: Revealing Backdoor Attacks in CNNs (Kolouri et al., 2020)",
        "Type": [
            "Paper"
        ],
        "Who tagged for ML": [
            "Viktoria"
        ],
        "Year": "2020",
        "airtable_createdTime": "2024-01-15T21:37:15.000Z",
        "airtable_id": "recyhdGHlhCJ24oSU"
    },
    {
        "# Top Paper votes": 0,
        "Abstract": "This report examines whether advanced AIs that perform well in training will be doing so in order to gain power later -- a behavior I call \"scheming\" (also sometimes called \"deceptive alignment\"). I conclude that scheming is a disturbingly plausible outcome of using baseline machine learning methods to train goal-directed AIs sophisticated enough to scheme (my subjective probability on such an outcome, given these conditions, is roughly 25%). In particular: if performing well in training is a good strategy for gaining power (as I think it might well be), then a very wide variety of goals would motivate scheming -- and hence, good training performance. This makes it plausible that training might either land on such a goal naturally and then reinforce it, or actively push a model's motivations towards such a goal as an easy way of improving performance. What's more, because schemers pretend to be aligned on tests designed to reveal their motivations, it may be quite difficult to tell whether this has occurred. However, I also think there are reasons for comfort. In particular: scheming may not actually be such a good strategy for gaining power; various selection pressures in training might work against schemer-like goals (for example, relative to non-schemers, schemers need to engage in extra instrumental reasoning, which might harm their training performance); and we may be able to increase such pressures intentionally. The report discusses these and a wide variety of other considerations in detail, and it suggests an array of empirical research directions for probing the topic further.\n\n",
        "Category": [
            "Deception",
            "Theory"
        ],
        "Citations": 10,
        "Goes online in Expanded Papers": true,
        "Internal source": [
            "Elsewhere"
        ],
        "Link": "https://arxiv.org/abs/2311.08379",
        "Safety Category": [
            "Major problems (misalignment, misuse, threat models)",
            "Deception"
        ],
        "Title": "Scheming AIs: Will AIs fake alignment during training in order to get power? (Carlsmith, 2023)",
        "Type": [
            "Paper"
        ],
        "Year": "2023",
        "airtable_createdTime": "2024-03-31T21:01:41.000Z",
        "airtable_id": "reczElBRkHNtCL0P6"
    }
]