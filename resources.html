---
layout: page
title: Resources
og-description: Resources for machine learning researchers, for those interested in AI governance and policy, and for a general audience.
nav-menu: true
order: 4
---

<!-- Main -->
<div id="main" class="alt">

<section>
	<div class="inner">
		<h1>Resources</h1>
		<h2 id="opportunities">Opportunities</h2>
		<!--<p>Organizations and researchers in the space, funding and job opportunities, and guides to get involved</p>-->


		<h3 id="research_opportunities">Research and Engineering Opportunities</h3>
		<ul>
			<li class="expandable">OpenAI
				<ul>
						<ul>
							<li><a href="https://openai.com/safety/safety-systems">Safety Systems Team</a> (<a href="https://openai.com/careers/search?c=safety-systems">roles</a>)</li>
							<li><a href="https://openai.com/safety/preparedness">Preparedness Team</a> (<a href="https://openai.com/careers/search?c=preparedness">roles</a>)</li>
							<li><a href="https://openai.com/blog/introducing-superalignment">Superalignment Team</a> (<a href="https://openai.com/careers/search?c=alignment">roles</a>)</li>
						  </ul>
						  
				</ul>
			</li>
			<li class="expandable">Anthropic (<a href="https://www.anthropic.com/careers#open-roles">roles</a>)
				<ul>
					<li><a href="https://www.anthropic.com/news/frontier-threats-red-teaming-for-ai-safety">Frontier Red Team</a></li>
					<li><a href="https://www.anthropic.com/news/anthropics-responsible-scaling-policy">Responsible Scaling Policy Team</a></li>
					<li><a href="https://www.alignmentforum.org/posts/EPDSdXr8YbsDkgsDG/introducing-alignment-stress-testing-at-anthropic">Alignment Stress Testing Team</a></li>
					<li>Interpretability Team</li>
					<li>Dangerous Capability Evaluations Team</li>
					<li>Assurance Team</li>
					<li>Security Team</li>
				</ul>
			</li>
			<li class="expandable"> Google DeepMind (<a href="https://deepmind.google/about/careers/#open-roles">roles</a>)
				<ul>
					<li><a href="https://deepmind.google/about/responsibility-safety/#:~:text=To%20empower%20teams%20to%20pioneer,and%20collaborations%20against%20our%20AI">Responsibility & Safety Team</a></li>
					<li>Scalable Alignment Team</li>
					<li>Frontier Model and Governance Team</li>
					<li>Mechanistic Interpretability Team</li>
				</ul>
			</li>
			<li><a href="https://far.ai/">FAR AI</a> (<a href="https://far.ai/jobs">roles</a>)</li>
			<li><a href="https://www.safe.ai/">Center for AI Safety (CAIS)</a> (<a href="https://safe.ai/careers">roles</a>)</li>
			<li><a href="https://metr.org">Model Evaluations and Threat Research (METR) (<a href="https://jobs.lever.co/alignment.org">roles</a>)</li>
			<li><a href="https://www.gov.uk/government/publications/ai-safety-institute-overview/introducing-the-ai-safety-institute">UK AI Safety Institute (AISI)</a> (<a href="https://www.linkedin.com/jobs/search/?currentJobId=3776968621&f_C=10872416&f_F=othr%2Cit%2Ceng%2Cstra&geoId=92000000&origin=JOB_SEARCH_PAGE_JOB_FILTER&sortBy=R">roles</a>)</li>
			<li><a href="https://www.apolloresearch.ai/">Apollo Research</a> (<a href="https://www.apolloresearch.ai/careers">roles</a>)</li>
			<li><a href="https://palisaderesearch.org/">Palisade Research</a> (<a href="https://palisaderesearch.org/work">roles</a>)</li>
		</ul>

		<h3 id="jobs">Job Board</h3>
		<ul>
			<li><a href="https://jobs.80000hours.org/?refinementList%5Btags_area%5D%5B0%5D=AI%20safety%20%26%20policy&refinementList%5Btags_area%5D%5B1%5D=Forecasting&refinementList%5Btags_skill%5D%5B0%5D=Data&refinementList%5Btags_skill%5D%5B1%5D=Engineering&refinementList%5Btags_skill%5D%5B2%5D=Policy&refinementList%5Btags_skill%5D%5B3%5D=Research&refinementList%5Btags_skill%5D%5B4%5D=Software%20engineering">AI Safety and Policy Job Board</a> by 80,000 Hours</li>
		</ul>


		<h3 id="alternative_opportunities">Alternative Technical Opportunities</h3>
		<ul>
			<li class="expandable">Theoretical research
				<ul>
					<li><a href="https://www.alignment.org/theory/">Alignment Research Center</a> <a href="https://www.alignment.org/hiring/">(roles)</a></li>
					<li><a href="https://intelligence.org/careers/">Machine Intelligence Research Institute (MIRI)</a> </li>
				</ul>
			<li><a href="https://jobs.80000hours.org/?refinementList%5Btags_area%5D%5B0%5D=AI%20safety%20%26%20policy&refinementList%5Btags_skill%5D%5B0%5D=Information%20security">Information Security</a></li>
			<li><a href="https://jobs.80000hours.org/?query=Forecasting&refinementList%5Btags_area%5D%5B0%5D=AI%20safety%20%26%20policy">Forecasting</a> (see especially <a href="https://epochai.org/careers">Epoch</a>)</li>
			<li><a href="https://jobs.80000hours.org/?refinementList%5Btags_area%5D%5B0%5D=AI%20safety%20%26%20policy&refinementList%5Btags_skill%5D%5B0%5D=Software%20engineering">Software engineering</a></li>
			<li class="expandable">Technical Governance (from <a href="https://web.archive.org/web/20231012161714/https://www.openphilanthropy.org/research/new-roles-on-our-gcr-team/#4-ai-governance-and-policy-aigp"> Section 4.3</a>)
				<ul>
					<li>Technical work that primarily aims to improve the efficacy of AI governance interventions, including compute governance, technical mechanisms for improving AI coordination and regulation, privacy-preserving transparency mechanisms, technical standards development, model evaluations, and information security. </li>
				</ul>
		</ul>
<!--
		<h3 id="alternative_opportunities">Example of expandable lists</h3>
		<ul>
			<li>The list item after this one has the 'expandable' class, which makes its list icon become a caret, and makes it clickable to show/hide its sublist.</li>
			<li class="expandable" data-toggle="theoretical_research">Theoretical research
				<ul id="theoretical_research">
					<li><a href="https://www.alignment.org/theory/">Alignment Research Center</a> <a href="https://www.alignment.org/hiring/">(roles)</a></li>
					<li><a href="https://intelligence.org/careers/">Machine Intelligence Research Institute (MIRI)</a></li>
				</ul>
			<li>Technical Governance (from <a href="https://web.archive.org/web/20231012161714/https://www.openphilanthropy.org/research/new-roles-on-our-gcr-team/#4-ai-governance-and-policy-aigp"> Section 4.3</a>): Technical work that primarily aims to improve the efficacy of AI governance interventions, including compute governance, technical mechanisms for improving AI coordination and regulation, privacy-preserving transparency mechanisms, technical standards development, model evaluations, and information security. </li>
		</ul>

		<h3 id="alternative_opportunities">Example of expandable lists already expanded</h3>
		<ul>
			<li>The list item after this one has the 'expandable' class, and also the 'expanded' class, so it's already open on page load.</li>
			<li class="expandable expanded" data-toggle="theoretical_research">Theoretical research
				<ul id="theoretical_research">
					<li><a href="https://www.alignment.org/theory/">Alignment Research Center</a> <a href="https://www.alignment.org/hiring/">(roles)</a></li>
					<li><a href="https://intelligence.org/careers/">Machine Intelligence Research Institute (MIRI)</a></li>
				</ul>
			<li>Technical Governance (from <a href="https://web.archive.org/web/20231012161714/https://www.openphilanthropy.org/research/new-roles-on-our-gcr-team/#4-ai-governance-and-policy-aigp"> Section 4.3</a>): Technical work that primarily aims to improve the efficacy of AI governance interventions, including compute governance, technical mechanisms for improving AI coordination and regulation, privacy-preserving transparency mechanisms, technical standards development, model evaluations, and information security. </li>
		</ul>
-->

		<h3 id="academia">Academia</h3>
		<ul>
			<li><a href="https://futureoflife.org/about-us/our-people/ai-existential-safety-community/">AI Existential Safety Community</a> from Future of Life Institute</li>
		</ul>

		<h3 id="workshops">Workshops</h3>
		<ul>
			<li><a href="https://www.alignment-workshop.com/nola-2023">New Orleans Alignment Workshop (Dec 2023)</a>, <i>recordings available</i>
				<ul> 
					<li><a href="https://airtable.com/appK578d2GvKbkbDD/pagkxO35Dx2fPrTlu/form"><i>Future events interest form</i></a></li>
				</ul>
			</li>
			<li><a href="https://www.alignment-workshop.com/sf-2023">San Francisco Alignment Workshop 2023 (Feb 2023)</a>, <i>recordings available</i>
				<ul>
					<li><a href="https://airtable.com/appK578d2GvKbkbDD/pagkxO35Dx2fPrTlu/form"><i>Future events interest form</i></a></li>
				</ul>
			</li>
			<li><a href="https://sites.google.com/mila.quebec/scaling-laws-workshop/">Neural Scaling & Alignment: Towards Maximally Beneficial AGI Workshop Series (2021-2023)</a></li>
			<li><a href="https://sites.google.com/mila.quebec/hlai-2023-boston/home">Human-Level AI: Possibilities, Challenges, and Societal Implications (June 2023)</a></li>
			<li><a href="https://futuretech.mit.edu/workshop-on-ai-scaling-and-its-implications#:~:text=The%20FutureTech%20workshop%20on%20AI,a%20range%20of%20key%20tasks%3F">Workshop on AI Scaling and its Implications (Oct 2023)</a></li>
		</ul>

		<h3 id="funding">Funding Sources</h3>
		<ul>
			<li class="expandable">OpenAI
				<ul>
					<li><a href="https://openai.com/blog/superalignment-fast-grants">Superalignment Fast Grants</a></li>
					<li><a href="https://openai.smapply.org/prog/agentic-ai-research-grants/">Research into Agentic AI Systems <i>(inactive)</i></a></li>
				</ul>
			</li>
			<li><a href="https://new.nsf.gov/funding/opportunities/safe-learning-enabled-systems">NSF</a></li>
			<li class="expandable">Open Philanthropy
				<ul>
					<li><a href="https://www.openphilanthropy.org/how-to-apply-for-funding/">How to Apply for Funding</a></li>
					<li><a href="https://www.openphilanthropy.org/rfp-llm-benchmarks/">Request for proposals: benchmarking LLM agents on consequential real-world tasks</a></li>
					<li><a href="https://www.openphilanthropy.org/rfp-llm-impacts/">Request for proposals: studying and forecasting the real-world impacts of systems built from LLMs</a></li>
				</ul>
			</li>
			<li><a href="https://www.cooperativeai.com/grants/cooperative-ai">Cooperative AI Research Grants</a></li>
			<li><a href="https://survivalandflourishing.fund/">Survival and Flourishing Fund</a></li>
			<li><a href="https://futureoflife.org/our-work/grantmaking-work/">Future of Life Institute</a></li>
			<li><a href="https://funds.effectivealtruism.org/funds/far-future">Long-Term Future Fund</a></li>
		</ul>

		<h3 id="compute">Compute Sources</h3>
		<ul>
			<li><a href="https://www.safe.ai/compute-cluster">Center for AI Safety Compute Cluster</a></li>
		</ul>

		<h3 id="china">Interested in working in China?</h3>
		<ul>
			<li>Contact <a href="https://concordia-ai.com/">Concordia AI 安远AI</a></li>
			<li>Newsletters: <a href="https://aisafetychina.substack.com/">AI Safety in China</a>, <a href="https://chinai.substack.com/about">ChinAI Newsletter</a></li>
		</ul>
	</div>
</section>

<section class="bg-gray">
	<div class="inner">
		<h2 id="learning_upskilling">Learning and Upskilling</h2>
    
    <h3 id="papers">Selection of Key Technical Safety Papers</h3>

		<div class="iframe-container">
			<iframe id="key_papers" onload="iframeLoaded('iframe_loading_spinner_key_papers')" src="https://airtable.com/embed/appOQF4xHFCwscK39/shrwwWZNAfTkrHP4h?backgroundColor=blueLight&viewControls=on" frameborder="0" onmousewheel="" width="100%" height="533" style="background: transparent; border: 1px solid #ccc;"></iframe>
			<div id="iframe_loading_spinner_key_papers" class="iframe-loading">
				{% include loading_spinner.html %}
			</div>
		</div>

    <a href="https://airtable.com/appOQF4xHFCwscK39/shrQzex73hN01B1CR" class="button button-white button-small button-right">Suggest A Resource</a>

    <h3 class="mt2" style="clear:both" id="content">Content on Risks from Advanced AI</h3>
    <ul>
      <li><a href="https://yoshuabengio.org/2023/06/24/faq-on-catastrophic-ai-risks/">FAQ on Catastrophic AI Risks</a> by Yoshua Bengio (2023)</li>
      <li><a href="https://wp.nyu.edu/arg/why-ai-safety/">Why I Think More NLP Researchers Should Engage with AI Safety Concerns</a> by Sam Bowman (2022)</li>
      <li><a href="https://arxiv.org/pdf/2209.00626.pdf">The Alignment Problem from a Deep Learning Perspective</a> (Ngo et al., 2022), <a href="https://twitter.com/RichardMCNgo/status/1603862969276051457"><i>Twitter thread</i></a></li>
      <li><a href="https://arxiv.org/pdf/2302.10329.pdf">Harms from Increasingly Agentic Algorithmic Systems</a> (Chan et al., 2023)</li>
      <li><a href="https://www.safe.ai/ai-risk">An Overview of Catastrophic AI Risks</a> (Hendrycks et al., 2023)</li>
      <li><a href="https://bounded-regret.ghost.io/more-is-different-for-ai/">More is Different for AI</a> by Jacob Steinhardt (2022)</li>
      <!--<li><a href="https://www.youtube.com/watch?v=yl2nlejBcg0">Researcher Perceptions of Current and Future AI</a> by Vael Gates (2022)</li>-->
    </ul>

    <h3 class="mt2" id="newsletters">Newsletters</h3>
    <ul>
      <li><a href="https://us13.campaign-archive.com/home/?u=67bd06787e84d73db24fb0aa5&id=6c9d98ff2c">Import AI</a> by Jack Clark, <i>for keeping up to date on AI progress</i></li>
      <li><a href="https://newsletter.mlsafety.org">ML Safety Newsletter</a> by the Center for AI Safety, <i>for recent AI safety papers</i></li>
      <li><a href="https://rohinshah.com/alignment-newsletter/">Alignment Newsletter</a> by Rohin Shah, <i>for review of AI safety papers, on hiatus</i></li>
    </ul>
		<h2 id="upskilling">Upskilling</h2>

		<h3 id="courses">Courses</h3>
		<ul>
			<li><a href="https://www.aisafetyfundamentals.com/ai-alignment-curriculum">AI Safety Fundamentals Curriculum</a> by BlueDot Impact </li>
			<li><a href="https://course.mlsafety.org/">Introduction to ML Safety</a> by the Center for AI Safety</li>
		</ul>

		<h3 id="guides">Guides</h3>
		<ul>
			<li> <b>Research</b>: <a href="https://aisafetyfundamentals.com/blog/alignment-careers-guide">Alignment Careers Guide</a></li>
			<li class="expandable"><b>Engineering</b>
				<ul>
					<li><a href="https://arena-roadmap.streamlit.app/">ARENA research engineering upskilling curriculum</a></li>
					<li><a href="https://docs.google.com/document/d/1b83_-eo9NEaKDKc9R3P5h5xkLImqMw8ADLmi__rkLo4/edit?usp=sharing">Leveling Up in AI Safety Research Engineering</a></li>
				</ul>
			
			</li>
		</ul>

		<h2 id="other">Other</h2>

		<h3 id="governance">AI Governance and Policy</h3>

		<p>AI governance is focused on developing global norms, policies, and institutions to increase the chances that advanced AI is beneficial for humanity.</p>
		<ul>
			<li><a href="https://www.agisafetyfundamentals.com/ai-governance-curriculum">AI Governance Curriculum</a> by BlueDot Impact</li>
			<li><a href="https://emergingtechpolicy.org/areas/ai-policy/">AI Policy Resources</a> by Emerging Technology Policy Careers</li>
			<li>See<a href="#alternative_opportunities"> Alternative Technical Opportunities,Technical Governance</a></li>
		<!-- 	<ul>
				<li>One highlight: <a href="https://forum.effectivealtruism.org/posts/ydpo7LcJWhrr2GJrx/the-longtermist-ai-governance-landscape-a-basic-overview">The longtermist AI governance landscape: a basic overview</a> (<a href="https://forum.effectivealtruism.org/topics/ai-governance">related posts</a>)</li>
			</ul> -->
			<li>Several organizations working in the space: Frontier AI Task Force, <a href="https://www.governance.ai/">Center for the Governance of AI (GovAI)</a>, OpenAI's Governance Team, <a href="https://www.longtermresilience.org/">Center for Long-Term Resilience (CLTR)</a>, <a href="https://www.rand.org/topics/science-technology-and-innovation-policy.html">RAND's Technology and Security Policy work</a>, <a href="https://www.horizonpublicservice.org/">Horizon Institute for Public Service</a>, <a href="https://cset.georgetown.edu/">Center for Security and Emerging Technology (CSET)</a></li>
			<!--<li>If you're interested in a career in US AI policy: <a href="https://80000hours.org/articles/us-ai-policy/">Overview</a> by 80,000 Hours and <a href="https://jobs.80000hours.org/?refinementList%5Btags_area%5D%5B0%5D=AI%20safety%20%26%20policy&refinementList%5Btags_area%5D%5B1%5D=Forecasting&refinementList%5Btags_skill%5D%5B0%5D=Policy">Job Board</a></li>
			Technical AI governance</b> outside of evaluations, which includes technical standards development (e.g. <a href="https://www.anthropic.com/index/anthropics-responsible-scaling-policy">Anthropic's Responsible Scaling Policy</a>
			<li><<a href="https://jobs.80000hours.org/?refinementList%5Btags_area%5D%5B0%5D=AI%20safety%20%26%20policy&refinementList%5Btags_skill%5D%5B0%5D=Information%20security">-->
		</ul>

		<h3 class="mt2" id="general">General Audience Readings</h3>

		<ul>
			<li><a href="https://www.planned-obsolescence.org/">Planned Obsolescence</a> by Ajeya Cotra and Kelsey Piper</li>
			<li><a href="https://www.vox.com/future-perfect/2018/12/21/18126576/ai-artificial-intelligence-machine-learning-safety-alignment"> The Case For Taking AI Seriously As A Threat to Humanity</a> by Kelsey Piper (2020)<!-- , 30m --></li>
				<li><a href="https://smile.amazon.com/Alignment-Problem-Machine-Learning-Values-ebook/dp/B085T55LGK/"> The Alignment Problem </a> by Brian Christian (2020)<!-- , book --></li>
				<li><a href="https://www.youtube.com/watch?v=UbruBnv3pZU"> Existential Risk from Power-Seeking AI</a> by Joe Carlsmith (2021)</li>
				<li><a href="https://www.cold-takes.com/why-ai-alignment-could-be-hard-with-modern-deep-learning/">Why AI Alignment Could be Hard with Modern Deep Learning</a> by Ajeya Cotra (2021)</li>
				<li><a href="https://80000hours.org/problem-profiles/artificial-intelligence/">80,000 Hours Podcast: Preventing an AI-related Catastrophe</a> (2022)<!-- , 2.5h --></li>
			<li><a href="https://www.cold-takes.com/most-important-century/">The Most Important Century</a> by Holden Karnofsky (podcasts and articles)</li>
			<li><a href="https://www.youtube.com/channel/UCLB7AzTwc6VFZrBsO2ucBMg/">AI Safety YouTube channel</a> by Robert Miles</li>
		</ul>
	</div>
</section>

<section>
	<div class="inner">
		<h2 id="ai_risk_discussions">AI Risk Discussions</h2>
		<p>Arkose's older 2022 work aimed to facilitate discussion and evaluation of potential risks from advanced AI, with a focus on soliciting and engaging with expert perspectives on the arguments and providing resources for stakeholders. Our results, based on a set of 97 interviews with AI researchers on their perspectives on current AI and the future of AI (pre-ChatGPT era), can be found below.</p>
		<br>
		<div class="row">
			<div class="6u 12u$(small)">
				<a href="interviews.html" class="button fit">Interviews</a>
				<p>One of our main goals was to facilitate conversations between those concerned about potential risks from advanced AI systems and technical experts. To that end, we conducted 97 interviews with AI researchers on their perspectives on current AI and the future of AI, with a focus on risks from advanced systems. This collection of interviews includes anonymized transcripts, quantitative analysis of the most common perspectives, and an academic talk discussing preliminary findings.</p>
			</div>
			<div class="6u$ 12u$(small)">
				<a href="{{site.baseurl}}{% link perspectives/introduction.html %}" class="button fit">Interactive Walkthrough</a>
				<p>In our interviews with AI researchers, some of the core questions focused on risks from advanced AI systems. To explore the interview questions, common responses from AI researchers, and potential counterarguments, we created an interactive walkthrough. You are encouraged to explore your own perspectives, and at the conclusion your series of agreements or disagreements will be displayed, so that you can compare your perspectives to other users' of the site.</p> 
			</div>
		</div>
		<h4>Project contributors</h4>
		<div id="about_us" class="text-smaller">
			<p>AI Risk Discussions was led by Dr. Vael Gates, with many other contributors, most prominently Lukas Trötzmüller (interactive walkthrough), Maheen Shermohammed (quantitative analysis), Zi Cheng (Sam) Huang (interview tagging), and Michael Keenan (website development).</p>
		</div>
	</div>
</section>

</div>

<script>
	function iframeLoaded(spinnerID) {
		document.getElementById(spinnerID).style.display = 'none';
	}
</script>


{% if false %}
<!-- All this commented-out section is inside an if-statement so it doesn't get sent to users.-->


<!--
{% if false %}
<section id="two" class="bg-gray">
	<div class="inner">

<div id="book_a_call"><h2>Book a call</h2>
<p>If you're interested in working in AI alignment and advanced AI safety, please book a call with <a href="https://vaelgates.com">Vael Gates</a>, who leads this project and conducted the <a href=interviews>interviews</a> as part of their postdoctoral work with Stanford University.</p>

<form id="book_call_form" method="post" action="#">
	<div class="row uniform">
		<div class="6u 12u$(xsmall)">
			<input type="text" name="name" id="name" value="" placeholder="Name" />
		</div>
		<div class="6u$ 12u$(xsmall)">
			<input type="email" name="email" id="email" value="" placeholder="Email" />
		</div>
		<div class="12u$">
			<div class="select-wrapper">
				<select name="interest" id="interest">
					<option value=""> Interested in talking about... </option>
					<option value="AI Alignment Research or Engineering">AI Alignment Research or Engineering</option>
					<option value="AI Alignment Governance">AI Alignment Governance</option>
					<option value="Other">Other (please specify below)</option>
				</select>
			</div>
		</div>
		<div class="12u$">
			<textarea name="message" id="message" placeholder="Enter your message" rows="6"></textarea>
		</div>
		<div class="12u$">
			<ul class="actions">
				<li>
          <button class="button" id="send_form_button">
            <div class="button-progress-bar"></div>
            <div class="button-text">Send Message</div>
          </button>
        </li>
			</ul>
		</div>
	</div>
</form>
</div>

</div>
</section>

{% endif %}

</div>

{% if false %}
<script src="{{ "assets/js/book_call.js" | absolute_url }}" type="module"></script>
<script>
  window.contactEmail = "{{site.email}}"
</script>
{% endif %}
-->

{% endif %}
