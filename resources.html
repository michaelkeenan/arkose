---
layout: page
title: Resources
og-description: Resources for machine learning researchers, for those interested in AI governance and policy, and for a general audience.
nav-menu: true
order: 4
---

<!-- Main -->
<div id="main" class="alt">

<section>
	<div class="inner">
		<h1>Resources</h1>
		<h2 id="opportunities">Opportunities</h2>
		<!--<p>Organizations and researchers in the space, funding and job opportunities, and guides to get involved</p>-->


		<h3 id="research_opportunities">Research and Engineering Opportunities</h3>
		<ul>
			<li class="expandable">OpenAI</li>
			<ul>
				<li><a href="https://openai.com/safety/safety-systems">Safety Systems Team</a> (<a href="https://openai.com/careers/search?c=safety-systems">roles</a>)</li>
				<li><a href="https://openai.com/safety/preparedness">Preparedness Team</a> (<a href="https://openai.com/careers/search?c=preparedness">roles</a>)</li>
				<li><a href="https://openai.com/blog/introducing-superalignment">Superalignment Team</a> (<a href="https://openai.com/careers/search?c=alignment">roles</a>)</li>
			</ul>
			<li class="expandable">Anthropic (<a href="https://www.anthropic.com/careers#open-roles">roles</a>)</li>
			<ul>
				<li><a href="https://www.anthropic.com/news/frontier-threats-red-teaming-for-ai-safety">Frontier Red Team</a></li>
				<li><a href="https://www.anthropic.com/news/anthropics-responsible-scaling-policy">Responsible Scaling Policy Team</a></li>
				<li><a href="https://www.alignmentforum.org/posts/EPDSdXr8YbsDkgsDG/introducing-alignment-stress-testing-at-anthropic">Alignment Stress Testing Team</a></li>
				<li>Interpretability Team</li>
				<li>Dangerous Capability Evaluations Team</li>
				<li>Assurance Team</li>
				<li>Security Team</li>
			</ul>
			<li class="expandable">Google DeepMind (<a href="https://deepmind.google/about/careers/#open-roles">roles</a>)</li>
			<ul>
				<li><a href="https://deepmind.google/about/responsibility-safety/#:~:text=To%20empower%20teams%20to%20pioneer,and%20collaborations%20against%20our%20AI">Responsibility & Safety Team</a></li>
				<li>Scalable Alignment Team</li>
				<li>Frontier Model and Governance Team</li>
				<li>Mechanistic Interpretability Team</li>
			</ul>
			<li><a href="https://www.gov.uk/government/publications/ai-safety-institute-overview/introducing-the-ai-safety-institute">UK AI Safety Institute (AISI)</a> (<a href="https://www.linkedin.com/jobs/search/?currentJobId=3776968621&f_C=10872416&f_F=othr%2Cit%2Ceng%2Cstra&geoId=92000000&origin=JOB_SEARCH_PAGE_JOB_FILTER&sortBy=R">roles</a>)</li>
			<li><a href="https://far.ai/">FAR AI</a> (<a href="https://far.ai/jobs">roles</a>)</li>
			<li><a href="https://metr.org">Model Evaluations and Threat Research (METR) (<a href="https://jobs.lever.co/alignment.org">roles</a>)</li>
			<li><a href="https://www.apolloresearch.ai/">Apollo Research</a> (<a href="https://www.apolloresearch.ai/careers">roles</a>)</li>
			<li><a href="https://www.safe.ai/">Center for AI Safety (CAIS)</a> (<a href="https://safe.ai/careers">roles</a>)</li>
			<li><a href="https://palisaderesearch.org/">Palisade Research</a> (<a href="https://palisaderesearch.org/work">roles</a>)</li>
		</ul>

		<h3 id="funding">Funding Sources</h3>

		<ul>
			<li>OpenAI</li>
			<ul>
				<li><a href="https://openai.com/blog/superalignment-fast-grants">Superalignment Fast Grants</a> (deadline: 2/18/24) </li>
			</ul>
			<li class="expandable expanded">Open Philanthropy</li>
			<ul>
				<li><a href="https://www.openphilanthropy.org/rfp-llm-benchmarks/"><b>Request for proposals: benchmarking LLM agents on consequential real-world tasks</a></b></li>
				<li><a href="https://www.openphilanthropy.org/rfp-llm-impacts/">Request for proposals: studying and forecasting the real-world impacts of systems built from LLMs</a></li>
				<li><a href="https://www.openphilanthropy.org/career-development-and-transition-funding/">Career development and transition funding</a></li>
				<li><a href="https://www.openphilanthropy.org/open-philanthropy-course-development-grants/">Course development grants</a></li>
				<li><a href="https://www.openphilanthropy.org/rfp-for-projects-to-grow-our-capacity-for-reducing-global-catastrophic-risks/"> Request for proposals for projects to grow our capacity for reducing global catastrophic risks</a></li>
				<!-- <li><a href="https://www.openphilanthropy.org/how-to-apply-for-funding/">How to Apply for Funding</a></li> -->
			</ul>			
			<li><a href="https://www.cooperativeai.com/grants/cooperative-ai">Cooperative AI Research Grants</a></li>
			<li><a href="https://funds.effectivealtruism.org/funds/far-future">Long-Term Future Fund</a></li>


			<li class="expandable" data-toggle="closed_funding"><i>Current Closed Funding Sources</i></li>
			<ul>
				<li>OpenAI: <a href="https://openai.smapply.org/prog/agentic-ai-research-grants/">Research into Agentic AI Systems</a></li>
				<li>NSF: <a href="https://new.nsf.gov/funding/opportunities/safe-learning-enabled-systems">Safe Learning-Enabled Systems</a> (deadline: 1/16/24)</li>
				<li>Survival and Flourishing Fund (SFF): <a href="https://survivalandflourishing.fund/">Grant Rounds</a> and <a href="https://survivalandflourishing.fund/speculation-grants">Speculation Grants</a></li>
				<ul>
					<li> Note: SFF gives <a href="https://survivalandflourishing.fund/faq#does-sff-have-an-indirect--overhead-rate-limit-for-grants-to-universities">grants to universities</a>. Alternatively, SFF requires that you have a 501c3 charity (i.e. your nonprofit has 501c3 status or you have a fiscal sponsor that has 501c3 status).</li>
				</ul>
				<li><a href="https://futureoflife.org/our-work/grantmaking-work/">Future of Life Institute</a>: <a href="https://futureoflife.org/grant-program/phd-fellowships/">PhD Fellowships</a> and <a href="https://futureoflife.org/grant-program/postdoctoral-fellowships/">Postdoctoral Fellowships</a></li>
			</ul>
		</ul>


		<h3 id="compute">Compute Sources</h3>
		<ul>
			<li><a href="https://www.safe.ai/compute-cluster">Center for AI Safety Compute Cluster</a></li>
			<li><a href="https://txt.cohere.com/c4ai-research-grants/">Cohere for AI, subsidized access to APIs</a></li>
		</ul>

		<h3 id="workshops">Workshops</h3>
		<ul>
			<li><a href="https://www.alignment-workshop.com/nola-2023">New Orleans Alignment Workshop (Dec 2023)</a>, <i>recordings available</i></li>
			<ul>
				<li><a href="https://airtable.com/appK578d2GvKbkbDD/pagkxO35Dx2fPrTlu/form"><i>Future events interest form</i></a></li>
			</ul>
			<li><a href="https://www.alignment-workshop.com/sf-2023">San Francisco Alignment Workshop 2023 (Feb 2023)</a>, <i>recordings available</i></li>
			<ul>
				<li><a href="https://airtable.com/appK578d2GvKbkbDD/pagkxO35Dx2fPrTlu/form"><i>Future events interest form</i></a></li>
			</ul>
			<li><a href="https://sites.google.com/mila.quebec/scaling-laws-workshop/">Neural Scaling & Alignment: Towards Maximally Beneficial AGI Workshop Series (2021-2023)</a></li>
			<li><a href="https://sites.google.com/mila.quebec/hlai-2023-boston/home">Human-Level AI: Possibilities, Challenges, and Societal Implications (June 2023)</a></li>
			<li><a href="https://futuretech.mit.edu/workshop-on-ai-scaling-and-its-implications#:~:text=The%20FutureTech%20workshop%20on%20AI,a%20range%20of%20key%20tasks%3F">Workshop on AI Scaling and its Implications (Oct 2023)</a></li>
		</ul>

				<h3 id="jobs">Job Board</h3>
		<ul>
			<li><a href="https://jobs.80000hours.org/?refinementList%5Btags_area%5D%5B0%5D=AI%20safety%20%26%20policy&refinementList%5Btags_area%5D%5B1%5D=Forecasting&refinementList%5Btags_exp_required%5D%5B0%5D=Mid%20%285-9%20years%20experience%29&refinementList%5Btags_exp_required%5D%5B1%5D=Multiple%20experience%20levels&refinementList%5Btags_exp_required%5D%5B2%5D=Senior%20%2810%2B%20years%20experience%29&refinementList%5Btags_skill%5D%5B0%5D=Data&refinementList%5Btags_skill%5D%5B1%5D=Engineering&refinementList%5Btags_skill%5D%5B2%5D=Policy&refinementList%5Btags_skill%5D%5B3%5D=Research&refinementList%5Btags_skill%5D%5B4%5D=Software%20engineering">AI Safety and Policy Job Board</a> by 80,000 Hours</li>
		</ul>


		<h3 id="alternative_opportunities">Alternative Technical Opportunities</h3>
		<ul>
			<li class="expandable" data-toggle="theoretical_research">Theoretical Research</li>
			<ul>
				<li><a href="https://www.alignment.org/theory/">Alignment Research Center</a> <a href="https://www.alignment.org/hiring/">(roles)</a></li>
				<li><a href="https://intelligence.org/careers/">Machine Intelligence Research Institute (MIRI)</a> </li>
			</ul>
			<li class="expandable" data-toggle="information_security">Information Security</li>
			<ul>
				<li><a href="https://jobs.80000hours.org/?refinementList%5Btags_area%5D%5B0%5D=AI%20safety%20%26%20policy&refinementList%5Btags_exp_required%5D%5B0%5D=Mid%20%285-9%20years%20experience%29&refinementList%5Btags_exp_required%5D%5B1%5D=Multiple%20experience%20levels&refinementList%5Btags_exp_required%5D%5B2%5D=Senior%20%2810%2B%20years%20experience%29&refinementList%5Btags_skill%5D%5B0%5D=Information%20security">Information Security roles</a></li>
				<li><a href="https://80000hours.org/career-reviews/information-security/">Overview from a security engineer at Google</a></li>
				<li><a href="https://www.linkedin.com/in/jason-clinton-475671159/">Jason Clinton</a>'s recommended <a href="https://www.google.com/books/edition/Building_Secure_and_Reliable_Systems/Kn7UxwEACAAJ?hl=en&kptab=getbook">upskilling book</a></li> <!--<a href="https://forum.effectivealtruism.org/posts/zxrBi4tzKwq2eNYKm/ea-infosec-skill-up-in-or-make-a-transition-to-infosec-via">EA Infosec: skill up in or make a transition to infosec via this book club</a>-->
			</ul>
			<li><a href="https://jobs.80000hours.org/?query=Forecasting&refinementList%5Btags_area%5D%5B0%5D=AI%20safety%20%26%20policy">Forecasting</a> (see especially <a href="https://epochai.org/careers">Epoch</a>)</li>
			<li><a href="https://jobs.80000hours.org/?refinementList%5Btags_area%5D%5B0%5D=AI%20safety%20%26%20policy&refinementList%5Btags_exp_required%5D%5B0%5D=Mid%20%285-9%20years%20experience%29&refinementList%5Btags_exp_required%5D%5B1%5D=Multiple%20experience%20levels&refinementList%5Btags_exp_required%5D%5B2%5D=Senior%20%2810%2B%20years%20experience%29&refinementList%5Btags_skill%5D%5B0%5D=Software%20engineering">Software Engineering</a></li>
			<li class="expandable" data-toggle="technical_goverance">Technical Governance (from <a href="https://web.archive.org/web/20231012161714/https://www.openphilanthropy.org/research/new-roles-on-our-gcr-team/#4-ai-governance-and-policy-aigp"> Section 4.3</a>)</li>
			<ul>
				<li>Technical work that primarily aims to improve the efficacy of AI governance interventions, including compute governance, technical mechanisms for improving AI coordination and regulation, privacy-preserving transparency mechanisms, technical standards development, model evaluations, and information security. </li>

			<li class="expandable" data-toggle="governance"> Compare with general AI Governance and Policy</li>
			<ul>
				<p>AI governance is focused on developing global norms, policies, and institutions to increase the chances that advanced AI is beneficial for humanity.</p>
				<li><a href="https://www.agisafetyfundamentals.com/ai-governance-curriculum">AI Governance Curriculum</a> by BlueDot Impact</li>
				<li><a href="https://emergingtechpolicy.org/areas/ai-policy/">AI Policy Resources</a> by Emerging Technology Policy Careers</li>
				<!-- <li>See<a href="#alternative_opportunities"> Alternative Technical Opportunities,Technical Governance</a></li> -->
				<li>Several organizations working in the space: Frontier AI Task Force, <a href="https://www.governance.ai/">Center for the Governance of AI (GovAI)</a>, OpenAI's Governance Team, <a href="https://www.longtermresilience.org/">Center for Long-Term Resilience (CLTR)</a>, <a href="https://www.rand.org/topics/science-technology-and-innovation-policy.html">RAND's Technology and Security Policy work</a>, <a href="https://www.horizonpublicservice.org/">Horizon Institute for Public Service</a>, <a href="https://cset.georgetown.edu/">Center for Security and Emerging Technology (CSET)</a></li>
				<!-- Technical AI governance</b> outside of evaluations, which includes technical standards development (e.g. <a href="https://www.anthropic.com/index/anthropics-responsible-scaling-policy">Anthropic's Responsible Scaling Policy</a>-->
			</ul>
		</ul>
		</ul>
<!--
		<h3>Example of expandable lists</h3>
		<ul>
			<li>The list item after this one has the 'expandable' class, which makes its list icon become a caret, and makes it clickable to show/hide its sublist.</li>
			<li class="expandable" data-toggle="theoretical_research">Theoretical research
				<ul id="theoretical_research">
					<li><a href="https://www.alignment.org/theory/">Alignment Research Center</a> <a href="https://www.alignment.org/hiring/">(roles)</a></li>
				</ul>
		</ul>

		<h3>Example of expandable lists already expanded</h3>
		<ul>
			<li>The list item after this one has the 'expandable' class, and also the 'expanded' class, so it's already open on page load.</li>
			<li class="expandable expanded" data-toggle="theoretical_research">Theoretical research
				<ul id="theoretical_research">
					<li><a href="https://www.alignment.org/theory/">Alignment Research Center</a> <a href="https://www.alignment.org/hiring/">(roles)</a></li>
				</ul>
		</ul>
-->

		<h3 id="academia">Researchers working on AI safety</h3>
		<ul>
			<li><a href="https://futureoflife.org/about-us/our-people/ai-existential-safety-community/">AI Existential Safety Community</a> from Future of Life Institute</li>
			<li>See speakers from the Alignment Workshop series (<a href="https://www.alignment-workshop.com/sf-2023">SF 2023</a>, <a href="https://www.alignment-workshop.com/nola-2023">NOLA 2023</a>)</li>
		</ul>

		<h3 id="china">Interested in working in China?</h3>
		<ul>
			<li>Contact <a href="https://concordia-ai.com/">Concordia AI 安远AI</a></li>
			<li>Newsletters: <a href="https://aisafetychina.substack.com/">AI Safety in China</a>, <a href="https://chinai.substack.com/about">ChinAI Newsletter</a></li>
		</ul>
	</div>
</section>

<section class="bg-gray">
	<div class="inner">
		<h2 id="ai_safety">AI Safety Content</h2>
    
    <h3 id="papers">Selected Papers</h3>

		<div class="iframe-container">
			<iframe id="key_papers" onload="iframeLoaded('iframe_loading_spinner_key_papers')" src="https://airtable.com/embed/appOQF4xHFCwscK39/shrwwWZNAfTkrHP4h?backgroundColor=blueLight&viewControls=on" frameborder="0" onmousewheel="" width="100%" height="625" style="background: transparent; border: 1px solid #ccc;"></iframe>
			<div id="iframe_loading_spinner_key_papers" class="iframe-loading">
				{% include loading_spinner.html %}
			</div>
		</div>

    <a href="https://airtable.com/appOQF4xHFCwscK39/shrQzex73hN01B1CR" class="button button-white button-small button-right">Suggest A Resource</a>

	<h3 style="clear:both"> Expanded List of Papers</h3>
	
	<a href="papers" class="button button-white fit">AI Safety Papers</a>

	<br>
    <h3 id="content">Blog posts</h3>
    <ul>
      <li><a href="https://yoshuabengio.org/2023/06/24/faq-on-catastrophic-ai-risks/">FAQ on Catastrophic AI Risks</a> by Yoshua Bengio (2023)</li>
      <li><a href="https://wp.nyu.edu/arg/why-ai-safety/">Why I Think More NLP Researchers Should Engage with AI Safety Concerns</a> by Sam Bowman (2022)</li>
      <li><a href="https://bounded-regret.ghost.io/more-is-different-for-ai/">More is Different for AI</a> by Jacob Steinhardt (2022)</li>
      <!--<li><a href="https://www.youtube.com/watch?v=yl2nlejBcg0">Researcher Perceptions of Current and Future AI</a> by Vael Gates (2022)</li>-->
      <li><a href="https://www.planned-obsolescence.org/">Planned Obsolescence</a> by Ajeya Cotra and Kelsey Piper (ongoing)</li>
      <!-- <li class="expandable" data-toggle="general"><i>For a general audience</i></li>
			<ul>
				<li><a href="https://www.planned-obsolescence.org/">Planned Obsolescence</a> by Ajeya Cotra and Kelsey Piper</li>
				<li><a href="https://www.vox.com/future-perfect/2018/12/21/18126576/ai-artificial-intelligence-machine-learning-safety-alignment"> The Case For Taking AI Seriously As A Threat to Humanity</a> by Kelsey Piper (2020)</li>
				<li><a href="https://smile.amazon.com/Alignment-Problem-Machine-Learning-Values-ebook/dp/B085T55LGK/"> The Alignment Problem </a> by Brian Christian (2020)</li>
				<li><a href="https://www.youtube.com/watch?v=UbruBnv3pZU"> Existential Risk from Power-Seeking AI</a> by Joe Carlsmith (2021)</li>
				<li><a href="https://www.cold-takes.com/why-ai-alignment-could-be-hard-with-modern-deep-learning/">Why AI Alignment Could be Hard with Modern Deep Learning</a> by Ajeya Cotra (2021)</li>
				<li><a href="https://80000hours.org/problem-profiles/artificial-intelligence/">80,000 Hours Podcast: Preventing an AI-related Catastrophe</a> (2022)</li>
				<li><a href="https://www.cold-takes.com/most-important-century/">The Most Important Century</a> by Holden Karnofsky (podcasts and articles)</li>
				<li><a href="https://www.youtube.com/channel/UCLB7AzTwc6VFZrBsO2ucBMg/">AI Safety YouTube channel</a> by Robert Miles</li>
			</ul> -->
		</ul>
	</ul>

    <h3 id="newsletters">Newsletters</h3>
    <ul>
      <li><a href="https://us13.campaign-archive.com/home/?u=67bd06787e84d73db24fb0aa5&id=6c9d98ff2c">Import AI</a> by Jack Clark, <i>for keeping up to date on AI progress</i></li>
      <li><a href="https://newsletter.mlsafety.org">ML Safety Newsletter</a> by the Center for AI Safety, <i>for recent AI safety papers</i></li>
      <li><a href="https://rohinshah.com/alignment-newsletter/">Alignment Newsletter</a> by Rohin Shah, <i>for review of AI safety papers, on hiatus</i></li>
    </ul>

	<h3 id="guides">Upskilling</h3>
	<ul>
		<li class="expandable" data-toggle="guides">Guides</li>
		<ul>
			<li> <b>Research</b>: <a href="https://aisafetyfundamentals.com/blog/alignment-careers-guide">Alignment Careers Guide</a></li>
			<li class="expandable expanded"><b>Engineering</b></li>
			<ul>
				<li><a href="https://arena-roadmap.streamlit.app/">ARENA research engineering upskilling curriculum</a>
					<ul>
						<li><a href="https://arena3-chapter0-fundamentals.streamlit.app/">Chapter 0: Fundamentals</a></li>
						<li><a href="https://arena3-chapter1-transformer-interp.streamlit.app/">Chapter 1: Transformer Interpretability</a></li>
						<li><a href="https://arena3-chapter2-rl.streamlit.app/">Chapter 2: Reinforcement Learning</a></li>
						<li><a href="https://arena-ch3-training-at-scale.streamlit.app/">Chapter 3: Training at Scale</a></li>
					</ul>
				</li>
				<li><a href="https://docs.google.com/document/d/1b83_-eo9NEaKDKc9R3P5h5xkLImqMw8ADLmi__rkLo4/edit?usp=sharing">Leveling Up in AI Safety Research Engineering</a></li>
			</ul>
		</ul>
		<li class="expandable" data-toggle="courses">Courses</li>
		<ul>
			<li><a href="https://www.aisafetyfundamentals.com/ai-alignment-curriculum">AI Safety Fundamentals Curriculum</a> by BlueDot Impact </li>
			<li><a href="https://course.mlsafety.org/">Introduction to ML Safety</a> by the Center for AI Safety</li>
		</ul>
	</ul>
	</div>
</section>

<section>
	<div class="inner">
		<h2 id="ai_risk_discussions">AI Risk Discussions</h2>
		<p>Arkose's older 2022 work aimed to facilitate discussion and evaluation of potential risks from advanced AI, with a focus on soliciting and engaging with expert perspectives on the arguments and providing resources for stakeholders. Our results, based on a set of 97 interviews with AI researchers on their perspectives on current AI and the future of AI (pre-ChatGPT era), can be found below.</p>
		<br>
		<div class="row">
			<div class="6u 12u$(small)">
				<a href="interviews.html" class="button fit">Interviews</a>
				<p>One of our main goals was to facilitate conversations between those concerned about potential risks from advanced AI systems and technical experts. To that end, we conducted 97 interviews with AI researchers on their perspectives on current AI and the future of AI, with a focus on risks from advanced systems. This collection of interviews includes anonymized transcripts, quantitative analysis of the most common perspectives, and an academic talk discussing preliminary findings.</p>
			</div>
			<div class="6u$ 12u$(small)">
				<a href="{{site.baseurl}}{% link perspectives/introduction.html %}" class="button fit">Interactive Walkthrough</a>
				<p>In our interviews with AI researchers, some of the core questions focused on risks from advanced AI systems. To explore the interview questions, common responses from AI researchers, and potential counterarguments, we created an interactive walkthrough. You are encouraged to explore your own perspectives, and at the conclusion your series of agreements or disagreements will be displayed, so that you can compare your perspectives to other users' of the site.</p> 
			</div>
		</div>
		<h4>Project contributors</h4>
		<div id="about_us" class="text-smaller">
			<p>AI Risk Discussions was led by Dr. Vael Gates, with many other contributors, most prominently Lukas Trötzmüller (interactive walkthrough), Maheen Shermohammed (quantitative analysis), Zi Cheng (Sam) Huang (interview tagging), and Michael Keenan (website development).</p>
		</div>
	</div>
</section>

</div>

<script>
	function iframeLoaded(spinnerID) {
		document.getElementById(spinnerID).style.display = 'none';
	}
</script>


{% if false %}
<!-- All this commented-out section is inside an if-statement so it doesn't get sent to users.-->


<!--
{% if false %}
<section id="two" class="bg-gray">
	<div class="inner">

<div id="book_a_call"><h2>Book a call</h2>
<p>If you're interested in working in AI alignment and advanced AI safety, please book a call with <a href="https://vaelgates.com">Vael Gates</a>, who leads this project and conducted the <a href=interviews>interviews</a> as part of their postdoctoral work with Stanford University.</p>

<form id="book_call_form" method="post" action="#">
	<div class="row uniform">
		<div class="6u 12u$(xsmall)">
			<input type="text" name="name" id="name" value="" placeholder="Name" />
		</div>
		<div class="6u$ 12u$(xsmall)">
			<input type="email" name="email" id="email" value="" placeholder="Email" />
		</div>
		<div class="12u$">
			<div class="select-wrapper">
				<select name="interest" id="interest">
					<option value=""> Interested in talking about... </option>
					<option value="AI Alignment Research or Engineering">AI Alignment Research or Engineering</option>
					<option value="AI Alignment Governance">AI Alignment Governance</option>
					<option value="Other">Other (please specify below)</option>
				</select>
			</div>
		</div>
		<div class="12u$">
			<textarea name="message" id="message" placeholder="Enter your message" rows="6"></textarea>
		</div>
		<div class="12u$">
			<ul class="actions">
				<li>
          <button class="button" id="send_form_button">
            <div class="button-progress-bar"></div>
            <div class="button-text">Send Message</div>
          </button>
        </li>
			</ul>
		</div>
	</div>
</form>
</div>

</div>
</section>

{% endif %}

</div>

{% if false %}
<script src="{{ "assets/js/book_call.js" | absolute_url }}" type="module"></script>
<script>
  window.contactEmail = "{{site.email}}"
</script>
{% endif %}
-->

{% endif %}
