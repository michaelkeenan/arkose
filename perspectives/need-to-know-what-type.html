---
layout: argument
title: "Need to know what type of AGI"
breadcrumbs: The Alignment Problem:the-alignment-problem,Need to know what type of AGI:need-to-know-what-type
---

<blockquote><p>
How did the AGI come about… I think it's pretty difficult. My intuition, that could be wrong, is that it's pretty difficult to reason about how to make it safe if we don't know what the AGI... If the AGI is a deep neural network or is a set of logic rules that are somehow just chained together into a more and more complicated loop that results in AGI or something. 
</p></blockquote>

<p>If AGI comes through entirely different paradigms than current AI systems, that casts doubt on how much insight we can have here and now into what makes it unsafe. However, it might be that AGI comes soon, and it might come through existing paradigms.</p>
<ul><li>Consider how much progress has been made simply by scaling up existing approaches, and how the benefits of scaling just seem to continue even into the >500 billion neuron range.</li>
<li>Also, consider the sudden emergence of new capabilities in language models as scale increases.</li>
</ul><p>Regardless of that, even if AGI does not come through existing paradigms, it seems worth pursuing alignment research now. While the specifics of the problem might be viewed in a different light in the future, we can already foresee that it may be a problem which will take considerable time and resources to solve. Since we may or may not have time and resources available between it becoming clear what sort of AGI we will develop and that AGI reaching a dangerous level of capability, we should start making those investments now.</p>
<ul><li>It is difficult to reason about a hypothetical future technology that has never existed before. For example, people in 1950 could not have foreseen the deep learning revolution.</li>
<li>However, even a small risk of a catastrophic new technology would mean it’s definitely worth paying attention to now—even if we do not know for certain if our present-day discussions on safety will significantly influence how safe future AI will be. Consider nuclear engineering and biotechnology: In both cases, there were dangerous mishaps which could have been prevented if safety measures had been researched and/or deployed more thoroughly.</li>
<li>That’s why research on AGI alignment focuses, at least some of the time, on high-level theoretical or philosophical arguments about goal-seeking behavior and agents. The hope is that this research will generalize to AI paradigms that are very different from our current one.</li>
</ul><a name='argnav'/>
<br/><br/><br/><!-- temporary fix for issue 19 -->
