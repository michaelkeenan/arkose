---
layout: argument
title: "Pursuing Safety Work"
breadcrumbs: Pursuing Safety Work:pursuing-safety-work
---
<ul><li>At the moment, very few people are working on the safety of future general AI systems (as opposed to improving ethical alignment of current narrow systems). It might be only around 300, while 10-100x as many people work on speeding up the progress towards general AI (see <a href='https://twitter.com/ben_j_todd/status/1489985966714544134?lang=en' target='_blank'>one estimate here</a>, <a href='https://80000hours.org/problem-profiles/artificial-intelligence/' target='_blank'>another one here</a>). Obviously, there is no proof that advanced AI systems will cause catastrophe. But there are arguments from multiple directions indicating that there is at least some level of risk, and the consequences might be catastrophic. For a technology with such impacts, it seems like safety research is neglected.<br/></li>
<li>While AI might come in 100 years or later, there is a non-trivial chance it will come much sooner. There is also some chance we will be surprised and suddenly faced with potentially disastrous, unaligned, AGI.</li>
<li>Even if it takes 100-200 years, it is not at all clear that the alignment we can achieve—if we assume we maintain present levels of investment—will be good enough by then.</li>
</ul>
<a href='../resources#technical' >Further Reading</a>

<a name='argnav'/>
<br/><br/><br/><!-- temporary fix for issue 19 -->
