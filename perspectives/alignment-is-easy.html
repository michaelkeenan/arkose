---
layout: argument
title: "Alignment is easy"
breadcrumbs: The Alignment Problem:the-alignment-problem,Alignment is easy:alignment-is-easy
---
<p>The alignment problem might seem easy to solve at first glance. However, as more and more people have researched it, we have uncovered a wide range of problems, including some which might not be obvious at first glance.</p>
For example: An AI system can be deceptively aligned, i.e. display safe behavior in the training environment but then do the wrong thing in the testing environment.
<ul><li>An AI employing deception may seem like a far-fetched scenario. However, forms of deception have already been observed even in relatively simple systems: In <a href='https://arxiv.org/abs/1803.03453' target='_blank'>The Surprising Creativity of Digital Evolution</a>, Ofria (see the section “Learning to Play Dumb on the Test”) recounts their experience from a 2001 simulated biology study published in Nature. In this study, evolutionary programming led to organisms being able to detect whether or not they were in the training environment, and radically change their behavior between the “normal” and “test” environments. This is an example of deception arising from training pressures, without even requiring general intelligence.</li>
<li>In <a href='https://arxiv.org/abs/2105.14111' target='_blank'>Goal Misgeneralization in Deep Reinforcement Learning</a> (ICML 2022), Langosco et al. provide the first empirical demonstrations of goal misgeneralization: In a simple toy environment, an agent learns the wrong objective due to slight distributional shifts between training and testing environment. This system is not an AGI with an intention to deceive - nevertheless, the outcome has parallels: The system displays safe behavior in training but pursues the wrong goal in the test environment. A later paper by DeepMind <a href='https://arxiv.org/pdf/2210.01790.pdf' target='_blank'>Goal Misgeneralization: Why Correct Specifications Aren’t Enough For Correct Goals</a>  provides a more general definition of goal misgeneralization and further examples in new settings.</li>
<li>It is possible that a hypothetical AGI system could form an intention to deceive. Very useful AI systems – anything close to “general intelligence” – will likely have a model of the world that includes itself and its internal processes, and will be able to track changes in its external environment and adjust its internal behavior accordingly. (Noting that in a training environment, actions the AI takes affect this local environment and are probably reversible. In a deployment scenario, actions the AI takes impact the overall structure of the world.) If an AI is pursuing an initially internalized goal, it may observe its goal structure being modified in “training” and “test” environments, and that to have any effect on reality with respect to its current goal, the AI needs to behave satisfactorily within these local environments before it will be deployed. Therefore, the AGI may try to optimize for acceptable-to-overseers behavior in these environments, even if it behaves differently once it is deployed.</li>
<li>Additional resources:</li>
<ul><a href='https://www.alignmentforum.org/posts/A9NxPTwbw6r6Awuwt/how-likely-is-deceptive-alignment' target='_blank'><li>How likely is deceptive alignment</a> – Presentation by <a href='https://www.linkedin.com/in/ehubinger/' target='_blank'>Evan Hubinger</a></li>
<li>The ICML paper mentioned above was popularized in a <a href='https://www.youtube.com/watch?v=zkbPdEHEyEI]' target='_blank'>YouTube video by AI researcher Robert Miles</a>.<br/></li></ul></ul>
<p>Obviously, much of this research is conceptual and we do not have any actual AGI systems to test our hypotheses on. That being said, a lot of these insights are independent of the particulars of AI architecture.</p>
<p>Furthermore, even if the alignment problem could be solved easily, it is not clear that the solution will be found in time:</p>
<ul><li>There is a significant resource gap between work on safety, compared to work on capabilities. At the moment, very few people are working on the safety of future general AI systems (as opposed to improving ethical alignment of current narrow systems). It might be only around 300, while 10-100x as many people work on speeding up the progress towards general AI (see <a href='https://twitter.com/ben_j_todd/status/1489985966714544134?lang=en' target='_blank'>one estimate here</a>, <a href='https://80000hours.org/problem-profiles/artificial-intelligence/' target='_blank'>another one here</a>). Obviously, there is no proof that advanced AI systems will cause catastrophe. But there are arguments from multiple directions indicating that there is at least some level of risk, and the consequences might be catastrophic. For a technology with such impacts, it seems like safety research is neglected.<br/></li>
</ul><p>And even if a solution is found in time, it might not be implemented correctly: Due to economic incentives, new technologies  are often deployed without adequate confirmation of safety, especially if people don’t think it needs to be carefully checked.</p>
<a name='argnav'/>
<br/><br/><br/><!-- temporary fix for issue 19 -->
