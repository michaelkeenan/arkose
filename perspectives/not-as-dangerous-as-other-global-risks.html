---
layout: argument
title: "Other global risks are more dangerous"
breadcrumbs: The Alignment Problem:the-alignment-problem,Other global risks are more dangerous:not-as-dangerous-as-other-global-risks
---

<blockquote><p>
That's true. Okay, so that is something. If it were an existential risk, I would be very motivated to worry about it. I'm not concerned that it is an existential risk, I am concerned about other existential risks. [...] So I think nuclear disarmament is necessary, and I'm very concerned about climate change. Those are two... Even climate change is actually probably not an existential risk, but  it could be. Extinction of mass crop plants under unforeseen negative consequences of climate change and inability to feed, leading to a large societal collapse. I don't know. It depends on what your definition of existential is, but I see the broad collapse of society, as we know it now, is basically existential. 
</p></blockquote>

<p>Certainly other global risks are important. But risks from AI are different: They are potentially bigger, qualitatively different, and more neglected than other catastrophic risks.</p>
<p>Risks from AI are graver than many other global risks:</p>
<ul><li>If we actually have a powerful, optimizing, superintelligent system that tries very hard to achieve a certain goal, it might lead to a doomsday scenario where humanity is overpowered.</li>
<li>Whereas climate change, and even a severe pandemic, would likely leave some parts of the human population alive and able to maintain some level of civilization and technology.</li>
</ul><p>These risks are more neglected than other risks:</p>
<ul><li>At the moment, very few people are working on the safety of future general AI systems (as opposed to improving ethical alignment of current narrow systems). It might be only around 300, while 10-100x as many people work on speeding up the progress towards general AI (see <a href='https://twitter.com/ben_j_todd/status/1489985966714544134?lang=en' target='_blank'>one estimate here</a>, <a href='https://80000hours.org/problem-profiles/artificial-intelligence/' target='_blank'>another one here</a>). Obviously, there is no proof that advanced AI systems will cause catastrophe. But there are arguments from multiple directions indicating that there is at least some level of risk, and the consequences might be catastrophic. For a technology with such impacts, it seems like safety research is neglected.<br/></li>
<li>Over the past two decades, there has been progress in communicating climate change. Despite clear, well-measured changes in weather patterns, many people still doubt whether anthropogenic global warming is real. AI safety might be significantly more difficult to communicate to lawmakers and the general public. This might impede progress towards regulatory safeguards.</li>
</ul><p>If we accept the arguments on AGI, then we are talking about creating a kind of entity that has never before existed in historyâ€”and which is smarter than us. In human history, even before the development of present-day technology, our ancestors used their intelligence to eradicate a lot of megafauna and dramatically reshape the world. This new entity could have much more intelligence than us, and goals that diverge from those of humanity.</p>
<a name='argnav'/>
<br/><br/><br/><!-- temporary fix for issue 19 -->
