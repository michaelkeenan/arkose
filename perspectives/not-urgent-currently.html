---
layout: argument
title: "Work not urgent currently"
breadcrumbs: Pursuing Safety Work:pursuing-safety-work,Work not urgent currently:not-urgent-currently
---

<blockquote><p>
 So... I trust the people that I work with to kind of hit the button when it really needs to be hit. Because right now, like I said, I think people don't take it seriously, partly because I don't think they really believe it needs to be taken seriously. These researchers are not saying, "Oh my God, we need to take this seriously, but I've got other stuff to do." They really are just like, I don't think we're at that stage where we need to take it seriously. I think the people that I work with are, on the most part-- mostly pretty well-intentioned people. There's some disagreement over this. [...] I know that within OpenAI there were some healthy discussions about what is the correct deployment model for things like GPT-3. All the discussions that I've had so far give me a lot of confidence that these people aren't stupid and they're not entirely negligent. I think they're possibly occasionally overconfident, but they're not arrogant, if that makes any sense. As in like, they know they're fallible. They don't always put the right error bars on their decisions, but they know they're not infallible. Ultimately, that's what it's going to come down to. There's not, like, a system for this. It's going to be a few hundreds to a thousand-- thousand people, doing the sensible thing. For the moment, it looks like that's going to happen. But... I agree that that isn't entirely confidence-inspiring. But I think that's going to be the way it goes. 
<p class='interview-attribution'>&#40;from: <a href='../interviews'>Interviews with AI Researchers</a>&#41;</p></p></blockquote>


<blockquote><p>
I believe [the problems] will be resolved in the future. 
<p class='interview-attribution'>&#40;from: <a href='../interviews'>Interviews with AI Researchers</a>&#41;</p></p></blockquote>

<p>Counter-arguments:</p>
<ul><li>It could be that alignment is a difficult technical project that requires many years to identify subproblems and grow a field to address them. If our timelines are 50+ years until AGI, it could still be that this field needs to be earnestly investigating the problem now, if it ends up being a problem that takes 50+ years. The technical difficulty of the alignment problem is currently unknown.</li>
<li>It could be that AGI is developed relatively quickly, and that the mentality, institutions, or skills needed to align AI at crunchtime need to be already in place well before the “need for alignment” is urgently recognized.</li>
<li>Governmental awareness of the problem and ability to act will likely trail far behind technical expertise, which may also require substantial technical work being done earlier.</li>
<li>It is unclear how much we would gain by waiting for different AI approaches to develop and then doing alignment. This is, however, a worry, and the more one believes the current deep learning paradigm will stop producing emergent behavior / having capability overhangs, the more present-day alignment efforts should be done with an eye to generalizability to potential new paradigms (one might lean harder on theory than empirical work, for example.)</li>
<li>Given one believes that AGI presents substantial potential risks to humanity, and that there is probably useful work to be done now, it seems like it could be worth one’s time to work on this problem now.</li>
</ul><a name='argnav'/>
<br/><br/><br/><!-- temporary fix for issue 19 -->
