---
layout: page
title: What Can I Do?
og-description: Reducing potential risk from advanced AI systems is an unsolved, difficult task. The communities that are working on this are relatively small, and the pathways for what is helpful are uncertain. However, here are some candidates for reducing risk.
nav-menu: true
order: 4
---

<!-- Main -->
<div id="main" class="alt">

<!-- One -->
<section id="one">
	<div class="inner">
		<header class="major">
			<h1>What can I do?</h1>
		</header>

<p>Reducing potential risk from advanced AI systems is an unsolved, difficult task, and the pathways for what is helpful are uncertain. However, here are some candidates for reducing risk:</p> 

<div class="row">
	<div class="6u 12u$(small)">
	<p><a href="#technical" class="button fit">AI Alignment Research & Eng.</a> Making progress on technical AI alignment (research and engineering)</p>
	</div>
	<div class="6u 12u$(small)">
	<p><a href="#governance" class="button fit">AI Governance</a> Developing global norms, policies, and institutions to increase the chances that advanced AI is beneficial</p>
	</div>
</div>
<div class="row">
	<div class="6u 12u$(small)">
	<p><a href="#" class="button fit disabled">Support</a> Providing support for others working in AI alignment (e.g. <a href="https://jobs.80000hours.org/?refinementList%5Btags_area%5D%5B0%5D=AI%20safety%20%26%20policy&refinementList%5Btags_area%5D%5B1%5D=Forecasting&refinementList%5Btags_skill%5D%5B0%5D=Management&refinementList%5Btags_skill%5D%5B1%5D=Operations">operations roles</a>)</p>
	</div>
	<div class="6u 12u$(small)">
	<p><a href="#" class="button disabled fit">Discussion</a> Engaging in discussion about these risks with colleagues</p>
	</div>
</div>

<div class="row">
	<div class="12u 12u$(small)">
	<p><a href="resources" class="button fit">Learn more about AI Alignment</a> The technical fields of AI alignment and AI governance are still in relatively formative stages, making it important to thoroughly <a href="resources">understand the theoretical and empirical problems of alignment, and current work in these areas</a>.</p>
	</div>
</div>

<br>
<div id="calltoaction" class="box box-blue special">
	<p> If the arguments for working to reduce risks from advanced AI systems feel substantive to you, the field <a href="https://80000hours.org/problem-profiles/artificial-intelligence/#what-can-you-do-concretely-to-help">needs many more thoughtful researchers, engineers, and support professionals</a>. We encourage you to investigate the resources below. Finally, if you would like guidance or connections and <b>you are interested in conducting research in AI alignment:</b></p>
	<a href="https://airtable.com/appPMHlJ6Z7XkDSEi/shrYnJIdu4kvWd2pB" class="button fit">apply for a call</a>
</div>
<br>






<div class="full-width-gray">
<br>
<div id="technical"><h2> Technical AI Alignment Research and Engineering </h2>

<h3> Overview of the space </h3>

<p> There are different subareas and research approaches within the field of AI alignment, and you may be a better fit for some than others.</p> 
<ul>
	<li>Some categories within the field:</li>
		<ul> 
			<li><b>Empirical research and engineering</b> (e.g. <a href="https://www.anthropic.com/">Anthropic</a>, <a href="far.ai">FAR AI</a>, <a href="https://deepmindsafetyresearch.medium.com/">Google DeepMind's alignment teams</a>, <a href="https://openai.com/blog/our-approach-to-alignment-research/">OpenAI's alignment teams</a>, <a href="safe.ai">Center for AI Safety (CAIS)</a>, <a href="https://www.redwoodresearch.org/">Redwood Research</a>)</li>
			<li><b>Empirical research and engineering aimed at evaluations of AI capabilities and alignment; technical AI governance work</b> (e.g. <a href="https://evals.alignment.org/">ARC Evals</a>, <a href="https://www.apolloresearch.ai/">Apollo Research</a>, <a href="https://palisaderesearch.org/">Palisade Research</a></li>
			<li><b>Technical AI governance</b> outside of evaluations, which includes technical standards development (e.g. <a href="https://www.anthropic.com/index/anthropics-responsible-scaling-policy">Anthropic's Responsible Scaling Policy</a>, <a href="https://epochai.org/">forecasting</a>, <a href="https://80000hours.org/career-reviews/information-security/">information security</a>, and <a href="https://forum.effectivealtruism.org/posts/BJtekdKrAufyKhBGw/ai-governance-needs-technical-work">other work</a>)</li>
			<li><b>Theoretical research</b> (e.g. <a href="https://alignment.org/">Alignment Research Center</a>, <a href="https://intelligence.org/">MIRI</a>, and many independent researchers.) </li>
		</ul> 
	<li><b>Academia</b> (e.g. UC Berkeley's <a href="https://humancompatible.ai/">CHAI</a>, NYU's <a href="https://wp.nyu.edu/arg/">Alignment Research Group</a>, <a href="https://jsteinhardt.stat.berkeley.edu/">Jacob Steinhardt</a>, <a href="https://www.davidscottkrueger.com/">David Krueger</a>, <a href="http://www.cs.cmu.edu/~focal/"> FOCAL</a> and <a href="https://www.cooperativeai.com/foundation">Cooperative AI Foundation</a>) often includes theoretical work and empirical work that doesn't require access to large-scale infrastructure.</li>
	<li><b>Engineering</b> aimed at AI alignment is almost always in industry, and <a href="https://www.alignmentforum.org/posts/YDF7XhMThhNfHfim9/ai-safety-needs-great-engineers">ML engineering</a> and research engineers are especially in-demand, but there's also <a href="https://jobs.80000hours.org/?refinementList%5Btags_area%5D%5B0%5D=AI%20safety%20%26%20policy&refinementList%5Btags_skill%5D%5B0%5D=Software%20engineering">a range</a> of engineering roles, especially security, and also software.</li>
</ul>

<h3> Funding sources</h3>
<ul>
	<li>Apply for funding from <a href="https://www.openphilanthropy.org/how-to-apply-for-funding/">Open Philanthropy</a>, <a href="https://survivalandflourishing.fund/">Survival and Flourishing Fund</a>, or <a href="https://funds.effectivealtruism.org/funds/far-future">Long-Term Future Fund</a></li>
</ul>

<h3> Careers </h3>
<ul>
	<li><a href="https://aisafetyfundamentals.com/opportunities-board/">Opportunities in AI Safety</a> and <a href="https://jobs.80000hours.org/?refinementList%5Btags_area%5D%5B0%5D=AI%20safety%20%26%20policy&refinementList%5Btags_area%5D%5B1%5D=Forecasting&refinementList%5Btags_skill%5D%5B0%5D=Data&refinementList%5Btags_skill%5D%5B1%5D=Engineering&refinementList%5Btags_skill%5D%5B2%5D=Policy&refinementList%5Btags_skill%5D%5B3%5D=Research&refinementList%5Btags_skill%5D%5B4%5D=Software%20engineering">Job Board</a></li>
	<li> Roles listed on individual organizations' webpages listed above </li>
</ul>

<h3> Guides to getting involved </h3>
<ul>
	<li> <b>Research</b>: <a href="https://rohinshah.com/faq-career-advice-for-ai-alignment-researchers/">FAQ: Advice for AI Alignment Researchers</a> by Rohin Shah (DeepMind) or <a href="https://forum.effectivealtruism.org/posts/7WXPkpqKGKewAymJf/how-to-pursue-a-career-in-technical-ai-alignment"> How to pursue a career in technical AI alignment</a></li>
	<li><b>Engineering</b>:  <a href="https://docs.google.com/document/d/1b83_-eo9NEaKDKc9R3P5h5xkLImqMw8ADLmi__rkLo4/edit?usp=sharing">Levelling Up in AI Safety Research Engineering</a></li>
</ul>

<div class="box">
<h3> Interested in working in China? </h3>
<ul>
	<li>If you're interested in working in technical alignment in China, please <a href="https://airtable.com/appPMHlJ6Z7XkDSEi/shrYnJIdu4kvWd2pB">apply a call</a> or get in contact with <a href="https://concordia-consulting.com/">Concordia AI 安远AI</a>.</li>
	<li>Newsletters on China's AI landscape: <a href="https://aisafetychina.substack.com/">AI Safety in China</a>, <a href="https://chinai.substack.com/about">ChinAI Newsletter</a></li>
</ul>
</div>
<br>
</div>
</div>




<br>
<div id="governance"><h2> AI Governance </h2>

<p>The Center for the Governance of AI (<a href="https://www.governance.ai/">GovAI</a>) describes the AI governance problem as "the problem of devising global norms, policies, and institutions to best ensure the beneficial development and use of advanced AI" (<a href="https://uploads-ssl.webflow.com/614b70a71b9f71c9c240c7a7/61d48553bf2faf58c3900bd2_GovAI-Research-Agenda.pdf">GovAI Research Agenda</a>). "AI governance" is distinct from "AI policy" in its primary focus on advanced AI — the hypothetical general purpose technology — rather than current AI as it exists today. The field believes that different actions, risks, and opportunities come about when focusing on advanced AI systems as compared to more contemporary issues, though <a href="https://www.allandafoe.com/opportunity">AI governance and AI policy naturally interface with each other and have overlapping domains</a>.</p>
<ul>
	<li>Read through the <a href="https://www.agisafetyfundamentals.com/ai-governance-curriculum">AI Governance Curriculum</a></li>
<!-- 	<ul>
		<li>One highlight: <a href="https://forum.effectivealtruism.org/posts/ydpo7LcJWhrr2GJrx/the-longtermist-ai-governance-landscape-a-basic-overview">The longtermist AI governance landscape: a basic overview</a> (<a href="https://forum.effectivealtruism.org/topics/ai-governance">related posts</a>)</li>
	</ul> -->
	<li>Several organizations working in the space: Frontier AI Task Force, <a href="https://www.governance.ai/">Center for the Governance of AI (GovAI)</a>, OpenAI's Governance Team, <a href="https://www.longtermresilience.org/">Center for Long-Term Resilience (CLTR)</a>, <a href="https://www.rand.org/topics/science-technology-and-innovation-policy.html">RAND's Technology and Security Policy work</a>, <a href="https://cset.georgetown.edu/">Center for Security and Emerging Technology (CSET)</a>
	<li>If you're interested in a career in US AI policy: <a href="https://80000hours.org/articles/us-ai-policy/">Overview</a> by 80,000 Hours and <a href="https://jobs.80000hours.org/?refinementList%5Btags_area%5D%5B0%5D=AI%20safety%20%26%20policy&refinementList%5Btags_area%5D%5B1%5D=Forecasting&refinementList%5Btags_skill%5D%5B0%5D=Policy">Job Board</a></li>
</ul>
</div>
<br>



</div>
</section>

<!--
{% if false %}
<section id="two" class="bg-gray">
	<div class="inner">

<div id="book_a_call"><h2>Book a call</h2>
<p>If you're interested in working in AI alignment and advanced AI safety, please book a call with <a href="https://vaelgates.com">Vael Gates</a>, who leads this project and conducted the <a href=interviews>interviews</a> as part of their postdoctoral work with Stanford University.</p>

<form id="book_call_form" method="post" action="#">
	<div class="row uniform">
		<div class="6u 12u$(xsmall)">
			<input type="text" name="name" id="name" value="" placeholder="Name" />
		</div>
		<div class="6u$ 12u$(xsmall)">
			<input type="email" name="email" id="email" value="" placeholder="Email" />
		</div>
		<div class="12u$">
			<div class="select-wrapper">
				<select name="interest" id="interest">
					<option value=""> Interested in talking about... </option>
					<option value="AI Alignment Research or Engineering">AI Alignment Research or Engineering</option>
					<option value="AI Alignment Governance">AI Alignment Governance</option>
					<option value="Other">Other (please specify below)</option>
				</select>
			</div>
		</div>
		<div class="12u$">
			<textarea name="message" id="message" placeholder="Enter your message" rows="6"></textarea>
		</div>
		<div class="12u$">
			<ul class="actions">
				<li>
          <button class="button" id="send_form_button">
            <div class="button-progress-bar"></div>
            <div class="button-text">Send Message</div>
          </button>
        </li>
			</ul>
		</div>
	</div>
</form>
</div>

</div>
</section>

{% endif %}

</div>

{% if false %}
<script src="{{ "assets/js/book_call.js" | absolute_url }}" type="module"></script>
<script>
  window.contactEmail = "{{site.email}}"
</script>
{% endif %}
-->