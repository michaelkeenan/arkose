<div class="page-data" data-page-title="No â€“ this is not as dangerous as other global risks"></div>
<blockquote>
That's true. Okay, so that is something. If it were an existential risk, I would be very motivated to worry about it. I'm not concerned that it is an existential risk, I am concerned about other existential risks. [...] So I think nuclear disarmament is necessary, and I'm very concerned about climate change. Those are two... Even climate change is actually probably not an existential risk, but  it could be. Extinction of mass crop plants under unforeseen negative consequences of climate change and inability to feed, leading to a large societal collapse. I don't know. It depends on what your definition of existential is, but I see the broad collapse of society, as we know it now, is basically existential. 
</blockquote>

<ul><li>Certainly other global risks are important. But risks from AI are qualitatively different from other risks that we face: They are both bigger, and more neglected than other catastrophic risks.</li>
<li>Risks from AI are graver than many other global risks:</li>
<ul><li>If we actually have a powerful optimizing superintelligent system that tries very hard to achieve a certain goal, it might lead to a doomsday scenario where humanity is overpowered.</li>
<li>Whereas climate change, and even a severe pandemic, would likely leave some parts of the human population unaffected.</li>
</ul><li>These risks are more neglected than other risks:</li>
<ul><li><br/><li>At the moment, very few people work on the safety of future general AI systems (as opposed to improving ethical alignment of current narrow systems). It might be only around 300, while 10-100x as many people work on speeding up the progress towards general AI (<br/>see <a href='https://twitter.com/ben_j_todd/status/1489985966714544134?lang=en' target='_blank'>one estimate</a>, <a href='https://80000hours.org/problem-profiles/artificial-intelligence/' target='_blank'>another one</a>). Obviously, there is no proof that advanced AI systems will cause catastrophe. We are not dealing with certainties here. But there are arguments from multiple directions indicating that there is at least some level of risk, and the consequences might be catastrophic. For a technology with such impacts, it seems like safety is grossly neglected. There is no reason to expect the safety community to suddenly grow by itself.<br/></li>
<li>Over the past two decades, there has been progress in communicating climate change. Despite clear measurable changes in weather patterns, many people still doubt whether anthropogenic global warming is real. AI safety might be significantly more difficult to communicate to lawmakers and the general public. This might impede progress towards regulatory safeguards.</li>
</ul><li>If we accept the arguments on AGI, then we are talking about creating a kind of entity that has never before existed in history - and is smarter than us!</li>
<li>We used our intelligence to eradicate a lot of megafauna (even before we had advanced technology) and reshape the world. This new entity could have much more intelligence, and goals that diverge from those of humanity.</li>
</ul><a name='argnav'/>
<br/><br/><br/><!-- temporary fix for issue 19 -->
