<div class="page-data" data-page-title="Human oversight"></div>
<blockquote><p>
If it's becoming too dangerous, or too uncomfortable about it, I think maybe people will just say, "Okay, just freeze it and don't use it." Just like some nuclear weapon. Everybody wants to own it, just don't use it.  
</p></blockquote>

<p>Currently, many machine learning models are effectively black boxes, meaning it is difficult to understand how they reach their decisions. There has been a lot of research on improving the interpretability of these models, and it is expected that we will make progress in this area. However, we are still a long way from having a clear understanding of large, state-of-the-art models. Therefore, if both AI capabilities and AI interpretability continue at their present rates, with far more resources devoted to capabilities than interpretability, it is likely that when transformative AI arrives, we will not have the ability to fully understand its inner workings.</p>
<p>It is also possible that a transformative AI system could understand its own code well enough to rewrite itself to be even more obscure to humans.</p>
<p>To avoid being switched off, a future AI system would have a range of options:</p>
<ul><li>Hide information that may cause human operators to be suspicious of the system’s next steps.</li>
<li>Persuade or bribe its human operator.</li>
<li>Present behavior that is acceptable to humans and follows established norms, while waiting for the moment when the system has been deployed across a wide range of geographically separated machines.</li>
<li>Gain access to servers on the internet, then upload and run copies of itself on them - a feat which even normal human hackers are able to do regularly.</li>
</ul><p>An AGI could be smarter than a human, which means it might out-think its operators: it could anticipate how it could be stopped, and design strategies to prevent that.</p>
<p>If the AI is incentivized against its operators, it’s going to be very hard to keep it under control. Therefore, the way to prevent AI from escaping our control cannot be to create it, observe what it does, then react by trying to shut it down. Instead, we must find a way to design it to be aligned with our interests.</p>
<p>The real world offers a vast range of possible actions that any AI connected to the internet can take. The bar for alignment therefore is very high, since failure only requires one unaligned AI and has the potential to be disastrous.</p>
<p>IT security is far from being a solved issue, as current human-designed IT systems often have vulnerabilities that can be discovered by other humans. It is likely that a human-designed sandbox for an AI would also have security weaknesses that could be identified and exploited by an AI that is smarter than a human.</p>
<p>Finally, even if we are able to design an AI system to be aligned with our interests, it is still important to have robust safety measures in place, as technology very often has unforeseen failure modes.</p>
<a name='argnav'/>
<br/><br/><br/><!-- temporary fix for issue 19 -->
