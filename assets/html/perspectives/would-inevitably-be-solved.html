<div class="page-data" data-page-title="This will probably be solved in the course of building these systems"></div>
<blockquote>
Maybe capabilities and alignment will advance together, and thus advanced artificial intelligence might be safe “by default”. For example, consider DALL-E. It paints these pictures based on textual descriptions. Painting accurate pictures is exactly what we want, and it wouldn’t be useful if it did not do that. Progress means making it aligned, in this case.  
</blockquote>

<ul><li>This is definitely a possibility! Maybe capabilities would be directly tied to alignment in some way. Maybe the development of advanced AI systems will go just fine.</li>
<li>However, there is the distinct possibility that things will not be fine. One reason for that is instrumental incentives : An advanced AI system, regardless of its goals, will probably have certain incentives relating to its own capabilities. It might, for example, employ strategies to avoid it from being switched off, and gather resources. We will cover this possibility in a later chapter.</li>
<li>Also, safety and capability do not always advance together by necessity.</li>
</ul>
<blockquote>
TODO: either have weak version of the argument here as a real quote, or set the context for now addressing the weaker version in some other way
</blockquote>

<li>Economic incentives mean things are often deployed without being perfectly safe, especially if people don’t think it needs to be super carefully checked.</li>
<li>Before the first test of the atomic bomb, there were some concerns that an energy release of this magnitude might cause a chain reaction within the atmosphere - turning the surface of earth into a barren wasteland. Further calculations were done and showed that this is not possible. However, the calculation was only checked by a small number of researchers and it could have been possible, though unlikely, that there was a mistake in the calculation or the underlying assumptions. https://blogs.scientificamerican.com/cross-check/bethe-teller-trinity-and-the-end-of-earth/</li>
<li>Current AI systems have significant ethical problems.</li>
<li>Some of these problems are only discovered in production.</li>
<li>Often, it is not possible to fix all these problems (for example, GPT-3 can be instructed to write toxic material despite efforts at preventing that)</li>
<li>It is possible that an AGI system would behave reasonably well in testing but then produce catastrophic results in the real world.</li>
<li><li>An AGI system, in order to have general intelligence, will probably need to be aware about its own inner workings and its place in the world. This would mean it might be able to know that it’s being trained, and it could intentionally display safe behaviour in training, hiding its true intentions until later. This sounds far-fetched, but is a natural consequence of assuming that future systems will have high amounts of intelligence and an accurate model of the world.</li>
<a name='argnav'/>
<br/><br/><br/><!-- temporary fix for issue 19 -->
<div>&#10149; <a href='the-alignment-problem#argnav'>Go back</a></div>
<div>&#9993; <a href='#feedback'>Send Feedback</a></div>
