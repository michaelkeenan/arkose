<div class="page-data" data-page-title="Be careful with reward function"></div>
<blockquote><p>
Right. That's hard. So with the specific example that we're talking about, "maximizing profit", "minimizing human exploitation", I suppose then it would depend on what the objective is. Is the system designed to maximize profit? Because current systems seem to be maximizing profit at the cost of human exploitation. So I think that's how systems are going to be designed because that's what we think as a society, what should be done or what's acceptable, so I don't think it's that the problem is that we are designing systems that is going to do exactly that, or not going to do exactly what we tell them to do, but I think we just have our goals or objectives wrong or incorrect. 
</p></blockquote>

<ul><li>If we do have systems that use a single reward function, it seems unlikely that it will capture everything we care about. For example, we might end up with a  system optimizing for profit instead of optimizing for human fulfillment.</li>
<li>At the moment, we do not understand reward functions well enough to guarantee that the system will do what we intend without producing unexpected consequences. We give systems rewards based on their outputs, but we often do not know what it actually learns.</li>
<li>As a society, we are not generally aiming towards developing technology that captures everything we care about. We don’t do that with our energy supply or the design of social media apps, and we probably won’t do that in AGI development either.</li>
<li>Even if the other problems were solved, there remains the possibility that someone makes a mistake in programming the reward function.</li>
<li>Problems might also arise if the system is allowed to self-modify and change its own reward function.</li>
</ul><p>Further Reading:</p>
<ul><a href='https://arxiv.org/abs/2105.14111' target='_blank'><li>Goal Misgeneralization in Deep Reinforcement Learning</a>, a 2022 paper exploring how an agent, placed in an unfamiliar environment, might be able to use all its skills that it previously learned while pursuing the wrong goal.</li>
</ul><a name='argnav'/>
<br/><br/><br/><!-- temporary fix for issue 19 -->
