<div class="page-data" data-page-title="Introduction"></div><em><p>Explore Your AI Risk Perspectives: An Interactive Walkthrough of Researchers' Most Frequent Interview Responses</em></p>
<p>Welcome to this interactive walkthrough, exploring perspectives about potential risks from advanced AI. The discussions in this walkthrough stem from a series of interviews with 97 AI researchers, wherein the interviewer probed researchers’ thoughts about the future of AI and arguments for potential risk from advanced systems. As you go through this guide, you’ll encounter researchers’ most common responses (along with direct quotes) alongside a range of arguments and counterarguments. At the bottom of each page, we encourage you to mark your own opinion: do you agree with the claim, and why or why not?</p>
<p>At the conclusion of the interactive walkthrough, we’ll display your set of agreements and disagreements, so that you can see what other users of the site agreed and disagreed with. If you don’t want your checkbox responses or feedback recorded, you can still navigate throughout the site by using the top navigation bar.</p>
<p>We hope this will help you explore some of the ongoing discussion on the development and deployment of highly capable AI systems. We welcome your feedback and encourage you to get in touch with us, especially if your own views are not represented here. To read more about the interviews, including analysis and transcripts, see the <a href='../interviews' >Interviews</a>.</p>
<p>…</p>
<p>Optional note about perspectives within this walkthrough . In discussing risks from AI, Jacob Steinhardt notes that there are two common approaches : the “Engineering” approach and the “Philosophy” approach. The Engineering approach “tends to be empirically-driven, drawing experience from existing or past ML systems”, while the Philosophy approach “tends to think more about the limit of very advanced systems”. He goes on to say that “in my experience, people who strongly subscribe to the Engineering worldview tend to think of Philosophy as fundamentally confused and ungrounded, while those who strongly subscribe to Philosophy think of most Engineering work as misguided and orthogonal (at best) to the long-term safety of ML.” However, he concludes that “Neither of these approaches is satisfying and we actually have no single good approach to thinking about risks from future ML systems,” and suggests ML researchers should lean on both when thinking about the future. In this walkthrough, we encourage readers to engage with both the engineering and philosophy approach, and try to supplement thought experiments with real-world examples whenever possible.</p>
<a name='argnav'/>
<br/><br/><br/><!-- temporary fix for issue 19 -->
