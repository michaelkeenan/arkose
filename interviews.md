---
layout: page
title: Interviews
og-description: Dr. Vael Gates conducted 97 interviews with AI researchers about their perceptions of AI and the future of the field, with a focus on risks from advanced AI systems. We present analysis, transcripts, and an interactive walkthrough of the researchers' perspectives.
nav-menu: true
order: 2
---

<!-- Main -->
<div id="main" class="alt">

<!-- One -->
<section id="one">
	<div class="inner">
		<header class="major">
			<h1>Interviews</h1>
		</header>

<!-- Content -->
<p> In February and March 2022, Dr. Vael Gates conducted 97 interviews with AI researchers about their perceptions of AI and the future of the field, with a focus on risks from advanced AI systems. 92 interviewees were randomly selected from NeurIPS or ICML 2021 submissions and 5 were outside recommendations. These interviews were transcribed and analyzed. </p>

<!-- <div class="row full-width">
	<div class="interviews-grid"> -->
<div class="row">
	<div class="4u 12u$(medium)">
			<h3 class="h3-smaller">Analysis</h3>
			<a href="https://hai.stanford.edu/events/hai-weekly-seminar-vael-gates" class="button special fit">Talk</a>
			<a href="https://docs.google.com/document/d/1ZwB7sD7VKY2LRCzK8Qr-L2gqEdAeXarEfhKKs0YgqcM/edit?usp=sharing" class="button small fit">Text version</a>
			<div class="box">
				<p>Preliminary results presented at Stanford.</p>
			</div>
			<!-- <a href="#main_report" class="button special fit">Report</a> -->
			<a href="analyze_transcripts_static.html" class="button special fit">Report</a>
			<a href="analyze_transcripts.html" class="button fit small">Interactive graph version</a>
			<div class="box">
				<p>Quantitative analysis of the common themes in the transcripts.</p>
			</div>
		</div>
		<div class="4u 12u$(medium)">
			<h3 class="h3-smaller">Transcripts</h3>
			<a href="https://drive.google.com/drive/folders/1qNN6GpAl6a4KswxnJcdhN4fqnMQgZ9Vg?usp=sharing" class="button special fit">Transcripts</a>
			<a href="https://docs.google.com/document/d/1q6-hUgIz-4H8AzejXqqtgX6uvw6w9EwMYtm9KRRYWro/edit?usp=sharing" class="button fit small">README</a>
			<div class="box">
				<p>83 researchers agreed to share anonymized transcripts.</p>
			</div>
			<a href="https://docs.google.com/spreadsheets/d/1FlBcctFLWTYY3NiIklgcuQtVYxuU-plDmUeQjn-2Cfk/edit?usp=sharing" class="button fit">Tagged Quotes</a>
			<div class="box">
				<p>Interviewee quotes were assigned tags; here are some sample pairs.</p>
			</div>
		</div>
		<div class="4u$ 12u$(medium)">
			<h3 class="h3-smaller">Interactive Walkthrough</h3>
			<a href="{{site.baseurl}}{% link perspectives/introduction.html %}" class="button special fit">Perspectives</a>
			<div class="box">
				<p>Explore the questions in the interviews, with discussion on potential risks from advanced AI, common responses from AI researchers, and potential counterarguments. </p>
			</div>
		</div>
</div>

</div>
</section>


<section id="report-main-findings" class="bg-gray">
	<div class="inner">
	<header class="major">
		<h2>Curious about the <a href="analyze_transcripts_static.html" class="button special xsmall">Report</a>'s main findings?</h2>
	</header>

<ul>
<li>Most participants (75%), at some point in the conversation, said that they thought humanity would achieve advanced AI (imprecisely labeled "AGI" for the rest of this summary) eventually, but their timelines to AGI varied <a href="analyze_transcripts_static.html#when-will-we-get-agi">(Source)</a>. Within this group:
	<ul>
	<li>32% thought it would happen in 0-50 years</li>
	<li>40% thought 50-200 years</li>
	<li>18% thought 200+ years</li>
	<li>and 28% were quite uncertain, reporting a very wide range.</li>
	</ul>	
	<p>(These sum to more than 100% because several people endorsed multiple timelines over the course of the conversation.)</p>
</li>
<li><p>Among participants who thought humanity would never develop AGI (22%), the most commonly cited reason was that they couldn't see AGI happening based on current progress in AI. <a href="analyze_transcripts_static.html#when-will-we-get-agi">(Source)</a></p></li>
<li>Participants were pretty split on whether they thought the alignment problem argument was valid. Some common reasons for disagreement were <a href="analyze_transcripts_static.html#alignment-problem">(Source)</a>:
</li>      
<ol>
	<li>A set of responses that included the idea that AI alignment problems would be solved over the normal course of AI development (caveat: this was a very heterogeneous tag).</li>
	<li>Pointing out that humans have alignment problems too (so the potential risk of the AI alignment problem is capped in some sense by how bad alignment problems are for humans).</li>
	<li>AI systems will be tested (and humans will catch issues and implement safeguards before systems are rolled out in the real world).</li>
	<li>The objective function will not be designed in a way that causes the alignment problem / dangerous consequences of the alignment problem to arise.</li>
	<li>Perfect alignment is not needed.</li> 
</ol>
<li><p> Participants were also pretty split on whether they thought the instrumental incentives argument was valid. The most common reasons for disagreement were that 1) the loss function of an AGI would not be designed such that instrumental incentives arise / pose a problem and 2) there would be oversight (by humans or other AI) to prevent this from happening. <a href="analyze_transcripts_static.html#instrumental-incentives">(Source)</a></p></li> 
<li><p> Some participants brought up that they were more concerned about misuse of AI than AGI misalignment (n = 17), or that potential risk from AGI was less dangerous than other large-scale risks humanity faces (n = 11). <a href="analyze_transcripts_static.html#merged-extended-discussion">(Source)</a></p></li>
<li><p> Of the 55 participants who were asked / had a response to this question, some (n = 13) were potentially interested in working on AI alignment research. (<a href="analyze_transcripts_static.html#work-on-this_about-this-variable">Caveat for bias</a>: the interviewer was less likely to ask this question if the participant believed AGI would never happen and/or the alignment/instrumental arguments were invalid, so as to reduce participant frustration. This question also tended to be asked in later interviews rather than earlier interviews.) Of those participants potentially interested in working on AI alignment research, almost all reported that they would need to learn more about the problem and/or would need to have a more specific research question to work on or incentives to do so. Those who were not interested reported feeling like it was not their problem to address (they had other research priorities, interests, skills, and positions), that they would need examples of risks from alignment problems and/or instrumental incentives within current systems to be interested in this work, or that they felt like they were not at the forefront of such research so would not be a good fit. <a href="analyze_transcripts_static.html#work-on-this">(Source)</a></p></li>
<li><p> Most participants had heard of AI safety (76%) in some capacity <a href="analyze_transcripts_static.html#heard-of-ai-safety">(Source)</a>; fewer had heard of AI alignment (41%) <a href="analyze_transcripts_static.html#heard-of-ai-alignment">(Source)</a>.</p></li> 
<li><p> When participants were followed-up with ~5-6 months after the interview, 51% reported the interview had a lasting effect on their beliefs <a href="analyze_transcripts_static.html#lasting-effects">(Source)</a>, and 15% reported the interview caused them to take new action(s) at work <a href="analyze_transcripts_static.html#new-actions">(Source)</a>.</p></li> 
<li><p> Thinking the alignment problem argument was valid, or the instrumental incentives argument was valid, both tended to correlate with thinking AGI would happen at some point. The effect wasn't symmetric: if participants thought these arguments were valid, they were quite likely to believe AGI would happen; if participants thought AGI would happen, it was still more likely that they thought these arguments were valid but the effect was less strong. <a href="analyze_transcripts_static.html#main-questions-x-main-questions">(Source)</a> </p></li>
</ul>


  </div>
</section>
</div>

